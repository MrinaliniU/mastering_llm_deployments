{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ Capstone Project: End-to-End LLM Deployment\n",
                "\n",
                "**Module 05 | Capstone**\n",
                "\n",
                "In this capstone, you'll apply everything you've learned to build a complete, production-ready NLP system.\n",
                "\n",
                "## Project Overview\n",
                "\n",
                "You will build a **Multi-Task NLP Service** that:\n",
                "1. Fine-tunes a model for your chosen task\n",
                "2. Optimizes the model for deployment\n",
                "3. Creates a REST API and interactive UI\n",
                "4. Containerizes the application\n",
                "\n",
                "---\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By completing this capstone, you will demonstrate:\n",
                "- [ ] Fine-tuning transformer models\n",
                "- [ ] Applying optimization techniques\n",
                "- [ ] Building production APIs\n",
                "- [ ] Creating user interfaces\n",
                "- [ ] Containerization skills\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets evaluate torch accelerate gradio fastapi uvicorn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "print(\"\\nðŸŽ“ Let's build your capstone project!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“‹ Project Requirements\n",
                "\n",
                "### Minimum Requirements\n",
                "\n",
                "| Component | Requirement |\n",
                "|-----------|-------------|\n",
                "| **Model** | Fine-tuned transformer model |\n",
                "| **Optimization** | Apply at least one optimization technique |\n",
                "| **API** | FastAPI with /predict endpoint |\n",
                "| **UI** | Gradio interface |\n",
                "| **Docker** | Working Dockerfile |\n",
                "\n",
                "### Suggested Tasks (Choose One)\n",
                "\n",
                "1. **Sentiment Analysis** - Movie/Product reviews\n",
                "2. **Text Classification** - News/Topic categorization\n",
                "3. **Summarization** - Article summarization\n",
                "4. **Question Answering** - Extractive QA\n",
                "5. **Custom Task** - Propose your own!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 1: Data Preparation & Fine-Tuning\n",
                "\n",
                "### Step 1.1: Load and Explore Your Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Example: Loading a sentiment analysis dataset\n",
                "# TODO: Replace with your chosen dataset\n",
                "dataset = load_dataset(\"imdb\")\n",
                "\n",
                "print(\"Dataset overview:\")\n",
                "print(f\"  Train: {len(dataset['train'])} samples\")\n",
                "print(f\"  Test: {len(dataset['test'])} samples\")\n",
                "\n",
                "# Explore\n",
                "print(\"\\nSample:\")\n",
                "print(f\"  Text: {dataset['train'][0]['text'][:200]}...\")\n",
                "print(f\"  Label: {dataset['train'][0]['label']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use subset for faster training (adjust for your resources)\n",
                "train_size = 5000\n",
                "test_size = 1000\n",
                "\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "test_data = dataset['test'].shuffle(seed=42).select(range(test_size))\n",
                "\n",
                "print(f\"Using {train_size} training and {test_size} test samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1.2: Tokenize Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "# TODO: Choose your base model\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding='max_length',\n",
                "        truncation=True,\n",
                "        max_length=256\n",
                "    )\n",
                "\n",
                "train_tokenized = train_data.map(tokenize_function, batched=True)\n",
                "test_tokenized = test_data.map(tokenize_function, batched=True)\n",
                "\n",
                "# Set format\n",
                "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "\n",
                "print(\"âœ… Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1.3: Fine-Tune the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
                "import evaluate\n",
                "import numpy as np\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    model_name,\n",
                "    num_labels=2\n",
                ")\n",
                "\n",
                "# Metrics\n",
                "accuracy_metric = evaluate.load(\"accuracy\")\n",
                "f1_metric = evaluate.load(\"f1\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    predictions = np.argmax(logits, axis=-1)\n",
                "    return {\n",
                "        'accuracy': accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'],\n",
                "        'f1': f1_metric.compute(predictions=predictions, references=labels)['f1']\n",
                "    }\n",
                "\n",
                "print(f\"Model loaded: {model_name}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./capstone_model\",\n",
                "    num_train_epochs=2,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    learning_rate=2e-5,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    logging_steps=100,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=test_tokenized,\n",
                "    compute_metrics=compute_metrics\n",
                ")\n",
                "\n",
                "print(\"Ready to train!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"ðŸƒ Starting training...\")\n",
                "trainer.train()\n",
                "\n",
                "# Evaluate\n",
                "results = trainer.evaluate()\n",
                "print(f\"\\nðŸ“Š Final Results:\")\n",
                "print(f\"  Accuracy: {results['eval_accuracy']:.2%}\")\n",
                "print(f\"  F1 Score: {results['eval_f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fine-tuned model\n",
                "model_path = \"./capstone_finetuned\"\n",
                "model.save_pretrained(model_path)\n",
                "tokenizer.save_pretrained(model_path)\n",
                "print(f\"\\nâœ… Model saved to {model_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Phase 2: Model Optimization\n",
                "\n",
                "Apply at least one optimization technique."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from copy import deepcopy\n",
                "import time\n",
                "\n",
                "# Reload the model\n",
                "model_optimized = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
                "\n",
                "print(\"Original model size:\")\n",
                "original_size = sum(p.numel() * p.element_size() for p in model_optimized.parameters()) / 1024**2\n",
                "print(f\"  {original_size:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Dynamic Quantization (INT8)\n",
                "model_quantized = torch.quantization.quantize_dynamic(\n",
                "    deepcopy(model_optimized),\n",
                "    {torch.nn.Linear},\n",
                "    dtype=torch.qint8\n",
                ")\n",
                "\n",
                "print(\"\\nâœ… Quantization applied!\")\n",
                "quantized_size = sum(p.nelement() * p.element_size() for p in model_quantized.parameters()) / 1024**2\n",
                "print(f\"Quantized model size: {quantized_size:.1f} MB\")\n",
                "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark latency\n",
                "test_text = \"This product exceeded all my expectations!\"\n",
                "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
                "\n",
                "def benchmark(model, inputs, n=50):\n",
                "    model.eval()\n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        for _ in range(5):\n",
                "            _ = model(**inputs)\n",
                "    # Benchmark\n",
                "    start = time.time()\n",
                "    for _ in range(n):\n",
                "        with torch.no_grad():\n",
                "            _ = model(**inputs)\n",
                "    return (time.time() - start) / n * 1000\n",
                "\n",
                "original_latency = benchmark(model_optimized, inputs)\n",
                "quantized_latency = benchmark(model_quantized, inputs)\n",
                "\n",
                "print(f\"\\nâ±ï¸ Latency Comparison (CPU):\")\n",
                "print(f\"  Original: {original_latency:.2f} ms\")\n",
                "print(f\"  Quantized: {quantized_latency:.2f} ms\")\n",
                "print(f\"  Speedup: {original_latency/quantized_latency:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the optimized model for deployment\n",
                "deployed_model = model_quantized  # or model_optimized if you prefer\n",
                "print(\"\\nâœ… Optimization complete! Using quantized model for deployment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Phase 3: Build the API & UI\n",
                "\n",
                "### Step 3.1: Prediction Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "# Set up prediction\n",
                "deployed_model.eval()\n",
                "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
                "\n",
                "def predict(text: str) -> dict:\n",
                "    \"\"\"Run inference on input text.\"\"\"\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=256\n",
                "    )\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = deployed_model(**inputs)\n",
                "    \n",
                "    probs = F.softmax(outputs.logits, dim=-1)[0]\n",
                "    pred_idx = probs.argmax().item()\n",
                "    \n",
                "    return {\n",
                "        \"label\": id2label[pred_idx],\n",
                "        \"confidence\": probs[pred_idx].item(),\n",
                "        \"probabilities\": {\n",
                "            id2label[i]: probs[i].item() \n",
                "            for i in range(len(probs))\n",
                "        }\n",
                "    }\n",
                "\n",
                "# Test\n",
                "print(\"Testing prediction function:\")\n",
                "print(predict(\"This movie was absolutely terrible!\"))\n",
                "print(predict(\"I loved every minute of this experience!\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3.2: FastAPI Application"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate FastAPI app code\n",
                "api_code = '''\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Dict\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "\n",
                "# Initialize app\n",
                "app = FastAPI(\n",
                "    title=\"Capstone NLP API\",\n",
                "    description=\"Production-ready sentiment analysis API\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Load model\n",
                "MODEL_PATH = \"./capstone_finetuned\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
                "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
                "model.eval()\n",
                "\n",
                "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
                "\n",
                "# Schemas\n",
                "class PredictRequest(BaseModel):\n",
                "    text: str = Field(..., min_length=1, max_length=5000)\n",
                "\n",
                "class PredictResponse(BaseModel):\n",
                "    label: str\n",
                "    confidence: float\n",
                "    probabilities: Dict[str, float]\n",
                "\n",
                "# Endpoints\n",
                "@app.get(\"/health\")\n",
                "def health():\n",
                "    return {\"status\": \"healthy\", \"model\": MODEL_PATH}\n",
                "\n",
                "@app.post(\"/predict\", response_model=PredictResponse)\n",
                "def predict(request: PredictRequest):\n",
                "    try:\n",
                "        inputs = tokenizer(request.text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "        pred_idx = probs.argmax().item()\n",
                "        return {\n",
                "            \"label\": id2label[pred_idx],\n",
                "            \"confidence\": probs[pred_idx].item(),\n",
                "            \"probabilities\": {id2label[i]: probs[i].item() for i in range(len(probs))}\n",
                "        }\n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=str(e))\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    import uvicorn\n",
                "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
                "'''\n",
                "\n",
                "with open(\"./capstone_api.py\", \"w\") as f:\n",
                "    f.write(api_code)\n",
                "\n",
                "print(\"âœ… FastAPI application saved to capstone_api.py\")\n",
                "print(\"\\nRun with: uvicorn capstone_api:app --reload\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3.3: Gradio Interface"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gradio as gr\n",
                "\n",
                "def predict_for_gradio(text):\n",
                "    if not text.strip():\n",
                "        return {\"Error\": 1.0}\n",
                "    result = predict(text)\n",
                "    return {\n",
                "        f\"{result['label']} {'ðŸ˜Š' if result['label'] == 'POSITIVE' else 'ðŸ˜ '}\": result['confidence'],\n",
                "        \"Other\": 1 - result['confidence']\n",
                "    }\n",
                "\n",
                "# Create interface\n",
                "demo = gr.Interface(\n",
                "    fn=predict_for_gradio,\n",
                "    inputs=gr.Textbox(\n",
                "        label=\"Enter text for sentiment analysis\",\n",
                "        placeholder=\"Type your review here...\",\n",
                "        lines=4\n",
                "    ),\n",
                "    outputs=gr.Label(label=\"Sentiment\", num_top_classes=2),\n",
                "    title=\"ðŸŽ“ Capstone: Sentiment Analysis\",\n",
                "    description=\"Fine-tuned and optimized DistilBERT model for sentiment classification.\",\n",
                "    examples=[\n",
                "        [\"This product is amazing! Best purchase I've ever made.\"],\n",
                "        [\"Terrible quality. Broke after one day.\"],\n",
                "        [\"It's okay. Nothing special but gets the job done.\"]\n",
                "    ],\n",
                "    theme=\"soft\"\n",
                ")\n",
                "\n",
                "demo.launch(share=False, inline=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Phase 4: Docker Packaging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Dockerfile\n",
                "dockerfile = '''FROM python:3.10-slim\n",
                "\n",
                "WORKDIR /app\n",
                "\n",
                "# Install dependencies\n",
                "COPY requirements.txt .\n",
                "RUN pip install --no-cache-dir -r requirements.txt\n",
                "\n",
                "# Copy model and code\n",
                "COPY capstone_finetuned/ ./capstone_finetuned/\n",
                "COPY capstone_api.py .\n",
                "\n",
                "# Expose port\n",
                "EXPOSE 8000\n",
                "\n",
                "# Health check\n",
                "HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\\\\n",
                "  CMD curl -f http://localhost:8000/health || exit 1\n",
                "\n",
                "# Run\n",
                "CMD [\"uvicorn\", \"capstone_api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
                "'''\n",
                "\n",
                "requirements = '''transformers==4.35.0\n",
                "torch==2.1.0\n",
                "fastapi==0.104.0\n",
                "uvicorn==0.24.0\n",
                "pydantic==2.5.0\n",
                "'''\n",
                "\n",
                "with open(\"./Dockerfile\", \"w\") as f:\n",
                "    f.write(dockerfile)\n",
                "\n",
                "with open(\"./requirements.txt\", \"w\") as f:\n",
                "    f.write(requirements)\n",
                "\n",
                "print(\"âœ… Docker files created:\")\n",
                "print(\"  - Dockerfile\")\n",
                "print(\"  - requirements.txt\")\n",
                "print(\"\\nBuild with: docker build -t capstone-nlp .\")\n",
                "print(\"Run with: docker run -p 8000:8000 capstone-nlp\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“‹ Submission Checklist\n",
                "\n",
                "Before submitting, verify:\n",
                "\n",
                "### Required\n",
                "- [ ] Fine-tuned model saved to `capstone_finetuned/`\n",
                "- [ ] Optimization applied (quantization/pruning)\n",
                "- [ ] FastAPI `capstone_api.py` works\n",
                "- [ ] Gradio interface runs\n",
                "- [ ] `Dockerfile` builds successfully\n",
                "\n",
                "### Evaluation Criteria\n",
                "\n",
                "| Criterion | Points |\n",
                "|-----------|--------|\n",
                "| Model accuracy (>85%) | 20 |\n",
                "| Optimization benefits documented | 20 |\n",
                "| API works correctly | 20 |\n",
                "| UI is functional | 15 |\n",
                "| Docker builds & runs | 15 |\n",
                "| Code quality & documentation | 10 |\n",
                "| **Total** | **100** |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Extension Challenges\n",
                "\n",
                "For extra credit:\n",
                "\n",
                "1. **Multi-model API** - Add summarization or NER endpoint\n",
                "2. **Batch processing** - Add `/predict/batch` endpoint\n",
                "3. **Caching** - Add Redis caching for repeated requests\n",
                "4. **Monitoring** - Add Prometheus metrics\n",
                "5. **Cloud deployment** - Deploy to AWS ECS or Hugging Face Spaces\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ‰ Congratulations!\n",
                "\n",
                "You've completed the LLM Deployment Course! You now have the skills to:\n",
                "\n",
                "- âœ… Fine-tune transformer models\n",
                "- âœ… Optimize for production (distillation, pruning, quantization)\n",
                "- âœ… Build REST APIs with FastAPI\n",
                "- âœ… Create interactive UIs with Gradio\n",
                "- âœ… Containerize with Docker\n",
                "- âœ… Deploy to cloud infrastructure\n",
                "\n",
                "**Next steps:**\n",
                "- Build your own NLP applications\n",
                "- Explore advanced optimization (ONNX, TensorRT)\n",
                "- Learn about MLOps and CI/CD\n",
                "- Contribute to open-source ML projects"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}