{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Quantization\n",
                "\n",
                "**Module 03 | Notebook 4 of 5**\n",
                "\n",
                "Quantization reduces the numerical precision of model weights, significantly decreasing memory and improving speed.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Understand different precision formats (FP32, FP16, INT8)\n",
                "2. Apply dynamic quantization with PyTorch\n",
                "3. Use bitsandbytes for 4-bit/8-bit loading\n",
                "4. Compare memory and speed improvements\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
                "from transformers import BitsAndBytesConfig\n",
                "import time\n",
                "import gc\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding Numerical Precision\n",
                "\n",
                "### Data Types in Deep Learning\n",
                "\n",
                "```\n",
                "FP32 (32-bit float):     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (4 bytes)\n",
                "FP16 (16-bit float):     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  (2 bytes)\n",
                "BF16 (bfloat16):         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  (2 bytes)\n",
                "INT8 (8-bit integer):    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          (1 byte)\n",
                "INT4 (4-bit integer):    ‚ñà‚ñà‚ñà‚ñà                              (0.5 bytes)\n",
                "```\n",
                "\n",
                "### Precision Comparison\n",
                "\n",
                "| Format | Bits | Range | Use Case |\n",
                "|--------|------|-------|----------|\n",
                "| **FP32** | 32 | ¬±3.4√ó10¬≥‚Å∏ | Training (default) |\n",
                "| **FP16** | 16 | ¬±65,504 | Mixed precision training |\n",
                "| **BF16** | 16 | ¬±3.4√ó10¬≥‚Å∏ | Same range as FP32, less precision |\n",
                "| **INT8** | 8 | -128 to 127 | Inference |\n",
                "| **INT4** | 4 | -8 to 7 | Aggressive compression |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize precision\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "# Memory comparison\n",
                "formats = ['FP32', 'FP16/BF16', 'INT8', 'INT4']\n",
                "bytes_per = [4, 2, 1, 0.5]\n",
                "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']\n",
                "\n",
                "axes[0].barh(formats, bytes_per, color=colors)\n",
                "axes[0].set_xlabel('Bytes per Parameter')\n",
                "axes[0].set_title('Memory Usage by Precision')\n",
                "for i, v in enumerate(bytes_per):\n",
                "    axes[0].text(v + 0.1, i, f'{v} bytes', va='center')\n",
                "\n",
                "# Model size comparison (100M param model)\n",
                "params = 100  # 100M parameters\n",
                "sizes = [params * b for b in bytes_per]\n",
                "\n",
                "axes[1].barh(formats, sizes, color=colors)\n",
                "axes[1].set_xlabel('Size (MB)')\n",
                "axes[1].set_title('Size of 100M Parameter Model')\n",
                "for i, v in enumerate(sizes):\n",
                "    axes[1].text(v + 5, i, f'{v:.0f} MB', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Dynamic Quantization (INT8)\n",
                "\n",
                "Dynamic quantization quantizes weights to INT8 while keeping activations in FP32.\n",
                "\n",
                "**Best for:** CPU inference, RNN/LSTM models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model in FP32\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "# Count parameters and size\n",
                "def get_model_size(model):\n",
                "    param_size = 0\n",
                "    for param in model.parameters():\n",
                "        param_size += param.nelement() * param.element_size()\n",
                "    return param_size / (1024**2)  # MB\n",
                "\n",
                "fp32_size = get_model_size(model_fp32)\n",
                "print(f\"FP32 Model Size: {fp32_size:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply dynamic quantization\n",
                "model_int8 = torch.quantization.quantize_dynamic(\n",
                "    model_fp32,\n",
                "    {torch.nn.Linear},  # Quantize Linear layers\n",
                "    dtype=torch.qint8\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Dynamic INT8 quantization applied!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare sizes\n",
                "int8_size = get_model_size(model_int8)\n",
                "\n",
                "print(f\"\\nSize Comparison:\")\n",
                "print(f\"  FP32: {fp32_size:.1f} MB\")\n",
                "print(f\"  INT8: {int8_size:.1f} MB\")\n",
                "print(f\"  Reduction: {(1 - int8_size/fp32_size)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark inference speed (CPU)\n",
                "test_text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
                "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
                "\n",
                "# Warmup\n",
                "with torch.no_grad():\n",
                "    _ = model_fp32(**inputs)\n",
                "    _ = model_int8(**inputs)\n",
                "\n",
                "# Benchmark\n",
                "n_runs = 50\n",
                "\n",
                "# FP32\n",
                "start = time.time()\n",
                "for _ in range(n_runs):\n",
                "    with torch.no_grad():\n",
                "        _ = model_fp32(**inputs)\n",
                "fp32_time = (time.time() - start) / n_runs * 1000\n",
                "\n",
                "# INT8\n",
                "start = time.time()\n",
                "for _ in range(n_runs):\n",
                "    with torch.no_grad():\n",
                "        _ = model_int8(**inputs)\n",
                "int8_time = (time.time() - start) / n_runs * 1000\n",
                "\n",
                "print(f\"\\nInference Latency (CPU, avg of {n_runs} runs):\")\n",
                "print(f\"  FP32: {fp32_time:.2f} ms\")\n",
                "print(f\"  INT8: {int8_time:.2f} ms\")\n",
                "print(f\"  Speedup: {fp32_time/int8_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify outputs match\n",
                "with torch.no_grad():\n",
                "    fp32_logits = model_fp32(**inputs).logits\n",
                "    int8_logits = model_int8(**inputs).logits\n",
                "\n",
                "fp32_probs = torch.softmax(fp32_logits, dim=-1)\n",
                "int8_probs = torch.softmax(int8_logits, dim=-1)\n",
                "\n",
                "print(f\"\\nOutput Comparison:\")\n",
                "print(f\"  FP32: POSITIVE={fp32_probs[0,1]:.4f}\")\n",
                "print(f\"  INT8: POSITIVE={int8_probs[0,1]:.4f}\")\n",
                "print(f\"  Max Difference: {torch.abs(fp32_probs - int8_probs).max():.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Half Precision (FP16/BF16)\n",
                "\n",
                "Converting to half precision is the simplest optimization for GPU inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if torch.cuda.is_available():\n",
                "    # Convert to FP16\n",
                "    model_fp16 = model_fp32.half().to(device)\n",
                "    \n",
                "    # Move FP32 to GPU for fair comparison\n",
                "    model_fp32_gpu = model_fp32.to(device)\n",
                "    \n",
                "    # Prepare inputs\n",
                "    inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
                "    inputs_fp16 = {k: v.to(device) for k, v in inputs.items()}\n",
                "    \n",
                "    # Benchmark\n",
                "    n_runs = 100\n",
                "    \n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        _ = model_fp32_gpu(**inputs_gpu)\n",
                "        _ = model_fp16(**inputs_fp16)\n",
                "    torch.cuda.synchronize()\n",
                "    \n",
                "    # FP32 GPU\n",
                "    start = time.time()\n",
                "    for _ in range(n_runs):\n",
                "        with torch.no_grad():\n",
                "            _ = model_fp32_gpu(**inputs_gpu)\n",
                "    torch.cuda.synchronize()\n",
                "    fp32_gpu_time = (time.time() - start) / n_runs * 1000\n",
                "    \n",
                "    # FP16 GPU\n",
                "    start = time.time()\n",
                "    for _ in range(n_runs):\n",
                "        with torch.no_grad():\n",
                "            _ = model_fp16(**inputs_fp16)\n",
                "    torch.cuda.synchronize()\n",
                "    fp16_time = (time.time() - start) / n_runs * 1000\n",
                "    \n",
                "    print(f\"GPU Inference Latency (avg of {n_runs} runs):\")\n",
                "    print(f\"  FP32: {fp32_gpu_time:.2f} ms\")\n",
                "    print(f\"  FP16: {fp16_time:.2f} ms\")\n",
                "    print(f\"  Speedup: {fp32_gpu_time/fp16_time:.2f}x\")\n",
                "    print(f\"\\nMemory saved: ~50% (half the bytes per parameter)\")\n",
                "else:\n",
                "    print(\"GPU not available. FP16 benefits are most significant on GPU.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4-bit Quantization with BitsAndBytes\n",
                "\n",
                "For large language models, 4-bit quantization enables running on consumer hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: This requires a GPU and the bitsandbytes library\n",
                "if torch.cuda.is_available():\n",
                "    # Configure 4-bit quantization\n",
                "    bnb_config_4bit = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "        bnb_4bit_use_double_quant=True  # Nested quantization\n",
                "    )\n",
                "    \n",
                "    # Configure 8-bit quantization\n",
                "    bnb_config_8bit = BitsAndBytesConfig(\n",
                "        load_in_8bit=True\n",
                "    )\n",
                "    \n",
                "    print(\"BitsAndBytes configurations ready!\")\n",
                "    print(\"\\n4-bit config:\")\n",
                "    print(f\"  Quant type: NF4 (NormalFloat4)\")\n",
                "    print(f\"  Compute dtype: FP16\")\n",
                "    print(f\"  Double quantization: Enabled\")\n",
                "else:\n",
                "    print(\"GPU required for bitsandbytes quantization\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Loading a model in 4-bit (using a small model for demo)\n",
                "if torch.cuda.is_available():\n",
                "    try:\n",
                "        # Clean up memory\n",
                "        gc.collect()\n",
                "        torch.cuda.empty_cache()\n",
                "        \n",
                "        # Load in 4-bit\n",
                "        model_4bit = AutoModelForSequenceClassification.from_pretrained(\n",
                "            model_name,\n",
                "            quantization_config=bnb_config_4bit,\n",
                "            device_map=\"auto\"\n",
                "        )\n",
                "        \n",
                "        print(\"‚úÖ Model loaded in 4-bit!\")\n",
                "        print(f\"Memory used: {torch.cuda.memory_allocated() / 1024**2:.0f} MB\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error loading 4-bit model: {e}\")\n",
                "        print(\"This may require more GPU memory or updated bitsandbytes.\")\n",
                "else:\n",
                "    print(\"Skipping 4-bit loading (GPU required)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Quantization Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Create comparison table\n",
                "comparison = pd.DataFrame({\n",
                "    'Method': ['FP32 (baseline)', 'FP16 (half precision)', 'INT8 (dynamic)', '8-bit (bitsandbytes)', '4-bit (bitsandbytes)'],\n",
                "    'Size Reduction': ['0%', '50%', '~50%', '75%', '87.5%'],\n",
                "    'Speed Improvement': ['1x', '1.5-2x', '2-3x (CPU)', '1-2x', '1x'],\n",
                "    'Accuracy Impact': ['baseline', 'negligible', '<1%', '1-2%', '2-5%'],\n",
                "    'Best For': ['Training', 'GPU inference', 'CPU inference', 'Large LLMs', 'Very large LLMs'],\n",
                "    'Requires Training': ['N/A', 'No', 'No', 'No', 'No']\n",
                "})\n",
                "\n",
                "print(\"Quantization Methods Comparison:\")\n",
                "print(\"=\" * 100)\n",
                "print(comparison.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visual comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "methods = ['FP32', 'FP16', 'INT8', '8-bit', '4-bit']\n",
                "sizes = [100, 50, 50, 25, 12.5]  # Relative sizes\n",
                "accuracy = [100, 99.9, 99.5, 98.5, 96]  # Approximate relative accuracy\n",
                "\n",
                "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db', '#9b59b6']\n",
                "\n",
                "# Size comparison\n",
                "axes[0].bar(methods, sizes, color=colors)\n",
                "axes[0].set_ylabel('Relative Size (%)')\n",
                "axes[0].set_title('Memory Usage by Quantization Method')\n",
                "for i, v in enumerate(sizes):\n",
                "    axes[0].text(i, v + 2, f'{v}%', ha='center')\n",
                "\n",
                "# Accuracy comparison\n",
                "axes[1].bar(methods, accuracy, color=colors)\n",
                "axes[1].set_ylabel('Relative Accuracy (%)')\n",
                "axes[1].set_title('Accuracy Retention by Method')\n",
                "axes[1].set_ylim(90, 102)\n",
                "for i, v in enumerate(accuracy):\n",
                "    axes[1].text(i, v + 0.5, f'{v}%', ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Saving Quantized Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the INT8 quantized model\n",
                "save_path = \"./quantized_model_int8\"\n",
                "torch.save(model_int8.state_dict(), f\"{save_path}/model.pt\")\n",
                "tokenizer.save_pretrained(save_path)\n",
                "\n",
                "print(f\"INT8 model saved to {save_path}\")\n",
                "\n",
                "# Note: For bitsandbytes models, use model.save_pretrained()\n",
                "# The quantization config is saved automatically"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "### Challenge: Quantization Accuracy Study"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Evaluate quantized models on a test set\n",
                "\n",
                "# 1. Load the SST-2 validation set\n",
                "# 2. Evaluate FP32, INT8, and FP16 models\n",
                "# 3. Create a table comparing:\n",
                "#    - Accuracy\n",
                "#    - Inference time\n",
                "#    - Memory usage\n",
                "\n",
                "# from datasets import load_dataset\n",
                "# dataset = load_dataset(\"glue\", \"sst2\")\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Quantization reduces precision** to save memory and speed up inference\n",
                "2. **FP16/BF16** is the easiest optimization with minimal accuracy loss\n",
                "3. **Dynamic INT8** works well for CPU inference\n",
                "4. **4-bit quantization** enables large models on consumer hardware\n",
                "5. **Trade-off exists** between size reduction and accuracy\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `05_benchmarking.ipynb` to compare all optimization techniques!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
