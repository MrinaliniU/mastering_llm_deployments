{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Benchmarking Optimization Techniques\n",
                "\n",
                "**Module 03 | Notebook 5 of 5**\n",
                "\n",
                "This notebook provides a comprehensive comparison of all optimization techniques covered in this module.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Set up a standardized benchmarking framework\n",
                "2. Compare latency, memory, and accuracy across techniques\n",
                "3. Visualize trade-offs effectively\n",
                "4. Choose the best optimization for your use case\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch datasets matplotlib pandas seaborn tabulate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from datasets import load_dataset\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import time\n",
                "import gc\n",
                "from copy import deepcopy\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1Ô∏è‚É£ Setup: Load Base Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "base_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "print(f\"Base model: {model_name}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "dataset = load_dataset(\"glue\", \"sst2\")\n",
                "test_data = dataset['validation'].shuffle(seed=42).select(range(100))\n",
                "\n",
                "print(f\"Test samples: {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2Ô∏è‚É£ Benchmarking Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model_size_mb(model):\n",
                "    \"\"\"Calculate model size in MB.\"\"\"\n",
                "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
                "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
                "    return (param_size + buffer_size) / (1024**2)\n",
                "\n",
                "def get_sparsity(model):\n",
                "    \"\"\"Calculate model sparsity percentage.\"\"\"\n",
                "    total = 0\n",
                "    zeros = 0\n",
                "    for p in model.parameters():\n",
                "        total += p.numel()\n",
                "        zeros += (p == 0).sum().item()\n",
                "    return zeros / total * 100\n",
                "\n",
                "def measure_latency(model, tokenizer, text, device, n_runs=50):\n",
                "    \"\"\"Measure inference latency in milliseconds.\"\"\"\n",
                "    model = model.to(device)\n",
                "    model.eval()\n",
                "    \n",
                "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
                "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "    \n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        for _ in range(5):\n",
                "            _ = model(**inputs)\n",
                "    \n",
                "    if device.type == 'cuda':\n",
                "        torch.cuda.synchronize()\n",
                "    \n",
                "    # Measure\n",
                "    times = []\n",
                "    for _ in range(n_runs):\n",
                "        start = time.time()\n",
                "        with torch.no_grad():\n",
                "            _ = model(**inputs)\n",
                "        if device.type == 'cuda':\n",
                "            torch.cuda.synchronize()\n",
                "        times.append((time.time() - start) * 1000)\n",
                "    \n",
                "    return np.mean(times), np.std(times)\n",
                "\n",
                "def evaluate_accuracy(model, tokenizer, test_data, device):\n",
                "    \"\"\"Evaluate model accuracy.\"\"\"\n",
                "    model = model.to(device)\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for example in test_data:\n",
                "            inputs = tokenizer(\n",
                "                example['sentence'],\n",
                "                return_tensors='pt',\n",
                "                truncation=True,\n",
                "                max_length=128\n",
                "            )\n",
                "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "            \n",
                "            outputs = model(**inputs)\n",
                "            pred = outputs.logits.argmax(dim=-1).item()\n",
                "            \n",
                "            if pred == example['label']:\n",
                "                correct += 1\n",
                "    \n",
                "    return correct / len(test_data) * 100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ Create Optimized Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.utils.prune as prune\n",
                "\n",
                "def apply_pruning(model, amount=0.3):\n",
                "    \"\"\"Apply L1 unstructured pruning.\"\"\"\n",
                "    model = deepcopy(model)\n",
                "    for name, module in model.named_modules():\n",
                "        if isinstance(module, nn.Linear):\n",
                "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
                "            prune.remove(module, 'weight')\n",
                "    return model\n",
                "\n",
                "def apply_quantization(model):\n",
                "    \"\"\"Apply INT8 dynamic quantization.\"\"\"\n",
                "    return torch.quantization.quantize_dynamic(\n",
                "        deepcopy(model),\n",
                "        {torch.nn.Linear},\n",
                "        dtype=torch.qint8\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create all model variants\n",
                "models = {}\n",
                "\n",
                "print(\"Creating model variants...\")\n",
                "\n",
                "# 1. Baseline (FP32)\n",
                "models['Baseline (FP32)'] = deepcopy(base_model)\n",
                "print(\"  ‚úì Baseline\")\n",
                "\n",
                "# 2. INT8 Quantized\n",
                "models['Quantized (INT8)'] = apply_quantization(base_model)\n",
                "print(\"  ‚úì INT8 Quantized\")\n",
                "\n",
                "# 3. Pruned 30%\n",
                "models['Pruned (30%)'] = apply_pruning(base_model, amount=0.3)\n",
                "print(\"  ‚úì Pruned 30%\")\n",
                "\n",
                "# 4. Pruned 50%\n",
                "models['Pruned (50%)'] = apply_pruning(base_model, amount=0.5)\n",
                "print(\"  ‚úì Pruned 50%\")\n",
                "\n",
                "# 5. Pruned + Quantized\n",
                "pruned_model = apply_pruning(base_model, amount=0.3)\n",
                "models['Pruned (30%) + INT8'] = apply_quantization(pruned_model)\n",
                "print(\"  ‚úì Pruned + Quantized\")\n",
                "\n",
                "print(f\"\\nTotal variants: {len(models)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ Run Benchmarks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark all models\n",
                "test_text = \"This movie was absolutely fantastic! Great acting and amazing story.\"\n",
                "results = []\n",
                "\n",
                "print(\"Running benchmarks...\\n\")\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Benchmarking: {name}\")\n",
                "    \n",
                "    # Size\n",
                "    size = get_model_size_mb(model)\n",
                "    \n",
                "    # Sparsity\n",
                "    sparsity = get_sparsity(model)\n",
                "    \n",
                "    # Latency (CPU only for quantized models)\n",
                "    test_device = 'cpu' if 'INT8' in name else device\n",
                "    latency_mean, latency_std = measure_latency(model, tokenizer, test_text, torch.device(test_device))\n",
                "    \n",
                "    # Accuracy\n",
                "    accuracy = evaluate_accuracy(model, tokenizer, test_data, torch.device(test_device))\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Size (MB)': size,\n",
                "        'Sparsity (%)': sparsity,\n",
                "        'Latency (ms)': latency_mean,\n",
                "        'Latency Std': latency_std,\n",
                "        'Accuracy (%)': accuracy,\n",
                "        'Device': str(test_device)\n",
                "    })\n",
                "    \n",
                "    print(f\"  Size: {size:.1f}MB | Latency: {latency_mean:.2f}ms | Accuracy: {accuracy:.1f}%\")\n",
                "    \n",
                "    # Clean up GPU memory\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\\nBenchmarking complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create results DataFrame\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "# Add relative metrics\n",
                "baseline_latency = df[df['Model'] == 'Baseline (FP32)']['Latency (ms)'].values[0]\n",
                "baseline_accuracy = df[df['Model'] == 'Baseline (FP32)']['Accuracy (%)'].values[0]\n",
                "baseline_size = df[df['Model'] == 'Baseline (FP32)']['Size (MB)'].values[0]\n",
                "\n",
                "df['Speedup'] = baseline_latency / df['Latency (ms)']\n",
                "df['Size Reduction (%)'] = (1 - df['Size (MB)'] / baseline_size) * 100\n",
                "df['Accuracy Drop (%)'] = baseline_accuracy - df['Accuracy (%)']\n",
                "\n",
                "print(\"\\n\" + \"=\" * 100)\n",
                "print(\"BENCHMARK RESULTS\")\n",
                "print(\"=\" * 100)\n",
                "print(df[['Model', 'Size (MB)', 'Latency (ms)', 'Accuracy (%)', 'Speedup', 'Accuracy Drop (%)']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive visualization\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
                "\n",
                "# 1. Latency comparison\n",
                "ax = axes[0, 0]\n",
                "bars = ax.bar(df['Model'], df['Latency (ms)'], color=colors)\n",
                "ax.set_ylabel('Latency (ms)')\n",
                "ax.set_title('Inference Latency')\n",
                "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
                "for bar, speedup in zip(bars, df['Speedup']):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
                "            f'{speedup:.2f}x', ha='center', fontsize=9)\n",
                "\n",
                "# 2. Accuracy comparison\n",
                "ax = axes[0, 1]\n",
                "bars = ax.bar(df['Model'], df['Accuracy (%)'], color=colors)\n",
                "ax.set_ylabel('Accuracy (%)')\n",
                "ax.set_title('Model Accuracy')\n",
                "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
                "ax.set_ylim(min(df['Accuracy (%)']) - 5, 100)\n",
                "ax.axhline(y=baseline_accuracy, color='red', linestyle='--', label=f'Baseline ({baseline_accuracy:.1f}%)')\n",
                "ax.legend()\n",
                "\n",
                "# 3. Size comparison\n",
                "ax = axes[1, 0]\n",
                "bars = ax.bar(df['Model'], df['Size (MB)'], color=colors)\n",
                "ax.set_ylabel('Size (MB)')\n",
                "ax.set_title('Model Size')\n",
                "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
                "for bar, reduction in zip(bars, df['Size Reduction (%)']):\n",
                "    if reduction > 0:\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
                "                f'-{reduction:.0f}%', ha='center', fontsize=9, color='green')\n",
                "\n",
                "# 4. Trade-off scatter plot\n",
                "ax = axes[1, 1]\n",
                "scatter = ax.scatter(df['Speedup'], df['Accuracy (%)'], \n",
                "                     s=df['Size (MB)'] * 5, c=colors, alpha=0.7, edgecolors='black')\n",
                "for i, row in df.iterrows():\n",
                "    ax.annotate(row['Model'].split('(')[0].strip(), \n",
                "                (row['Speedup'], row['Accuracy (%)']),\n",
                "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
                "ax.set_xlabel('Speedup')\n",
                "ax.set_ylabel('Accuracy (%)')\n",
                "ax.set_title('Trade-off: Speedup vs Accuracy\\n(bubble size = model size)')\n",
                "ax.axhline(y=baseline_accuracy - 1, color='green', linestyle='--', alpha=0.5, label='<1% accuracy drop')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6Ô∏è‚É£ Recommendation Engine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def recommend_optimization(priority='balanced', accuracy_tolerance=1.0):\n",
                "    \"\"\"\n",
                "    Recommend best optimization based on priority.\n",
                "    \n",
                "    Args:\n",
                "        priority: 'speed', 'accuracy', 'size', or 'balanced'\n",
                "        accuracy_tolerance: Maximum acceptable accuracy drop (%)\n",
                "    \"\"\"\n",
                "    # Filter by accuracy tolerance\n",
                "    candidates = df[df['Accuracy Drop (%)'] <= accuracy_tolerance].copy()\n",
                "    \n",
                "    if len(candidates) == 0:\n",
                "        print(f\"No models meet {accuracy_tolerance}% accuracy tolerance.\")\n",
                "        return None\n",
                "    \n",
                "    if priority == 'speed':\n",
                "        best = candidates.loc[candidates['Speedup'].idxmax()]\n",
                "    elif priority == 'accuracy':\n",
                "        best = candidates.loc[candidates['Accuracy (%)'].idxmax()]\n",
                "    elif priority == 'size':\n",
                "        best = candidates.loc[candidates['Size (MB)'].idxmin()]\n",
                "    else:  # balanced\n",
                "        # Score based on speedup and accuracy retention\n",
                "        candidates['Score'] = candidates['Speedup'] * (candidates['Accuracy (%)'] / 100)\n",
                "        best = candidates.loc[candidates['Score'].idxmax()]\n",
                "    \n",
                "    return best\n",
                "\n",
                "# Get recommendations\n",
                "print(\"OPTIMIZATION RECOMMENDATIONS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for priority in ['speed', 'accuracy', 'size', 'balanced']:\n",
                "    rec = recommend_optimization(priority=priority, accuracy_tolerance=2.0)\n",
                "    if rec is not None:\n",
                "        print(f\"\\nPriority: {priority.upper()}\")\n",
                "        print(f\"  Recommended: {rec['Model']}\")\n",
                "        print(f\"  Speedup: {rec['Speedup']:.2f}x\")\n",
                "        print(f\"  Accuracy: {rec['Accuracy (%)']:.1f}%\")\n",
                "        print(f\"  Size: {rec['Size (MB)']:.1f}MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7Ô∏è‚É£ Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results to CSV\n",
                "df.to_csv('./benchmark_results.csv', index=False)\n",
                "print(\"Results saved to benchmark_results.csv\")\n",
                "\n",
                "# Print final summary\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nBaseline: {baseline_accuracy:.1f}% accuracy, {baseline_latency:.2f}ms latency\")\n",
                "print(\"\\nBest optimizations within 1% accuracy drop:\")\n",
                "within_tolerance = df[df['Accuracy Drop (%)'] <= 1.0]\n",
                "if len(within_tolerance) > 1:\n",
                "    best_speedup = within_tolerance.loc[within_tolerance['Speedup'].idxmax()]\n",
                "    print(f\"  Fastest: {best_speedup['Model']} ({best_speedup['Speedup']:.2f}x speedup)\")\n",
                "    best_size = within_tolerance.loc[within_tolerance['Size (MB)'].idxmin()]\n",
                "    print(f\"  Smallest: {best_size['Model']} ({best_size['Size (MB)']:.1f}MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "### Challenge: Add More Optimization Combinations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Extend the benchmark with additional combinations:\n",
                "\n",
                "# 1. Add FP16 model (if GPU available)\n",
                "# 2. Try different pruning levels (40%, 60%, 70%)\n",
                "# 3. Create a combined \"Pruned (50%) + INT8\" variant\n",
                "# 4. Update the visualization and recommendations\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Key Takeaways\n",
                "\n",
                "1. **Benchmarking is essential** before deploying optimized models\n",
                "2. **Trade-offs vary** by use case (speed vs. accuracy vs. size)\n",
                "3. **Combining techniques** can maximize optimization benefits\n",
                "4. **Standardized testing** ensures fair comparisons\n",
                "5. **Document results** for reproducibility\n",
                "\n",
                "---\n",
                "\n",
                "## ‚û°Ô∏è Next Steps\n",
                "\n",
                "Continue to Module 04: **Deployment**\n",
                "- `04_Deployment/01_local_serving.ipynb`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}