{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Knowledge Distillation\n",
                "\n",
                "**Module 03 | Notebook 2 of 5**\n",
                "\n",
                "Knowledge distillation trains a smaller \"student\" model to mimic a larger \"teacher\" model.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Understand the teacher-student paradigm\n",
                "2. Implement KL divergence loss for distillation\n",
                "3. Train a distilled model from scratch\n",
                "4. Compare performance before and after distillation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.optim import AdamW\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSequenceClassification,\n",
                "    AutoConfig, get_scheduler\n",
                ")\n",
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "import evaluate\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## How Distillation Works\n",
                "\n",
                "### Traditional Training vs. Distillation\n",
                "\n",
                "```\n",
                "TRADITIONAL TRAINING:\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Hard Labels       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚   Input   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  Student  â”‚  Loss: CrossEntropy\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     [0, 1, 0]         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "\n",
                "DISTILLATION:\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        \n",
                "â”‚   Input   â”‚                        \n",
                "â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        \n",
                "      â”‚                              \n",
                "      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Soft Labels (probabilities)\n",
                "      â”‚            â”‚  Teacher  â”‚â”€â”€â”€â”€â†’ [0.05, 0.90, 0.05]\n",
                "      â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
                "      â”‚                                     â”‚ KL Divergence\n",
                "      â”‚                                     â–¼\n",
                "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "                   â”‚  Student  â”‚â”€â”€â”€â”€â†’ [0.10, 0.85, 0.05]\n",
                "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### Why Soft Labels?\n",
                "\n",
                "Soft labels contain **more information** than hard labels:\n",
                "\n",
                "| Label Type | Example | Information Content |\n",
                "|------------|---------|--------------------|\n",
                "| Hard | [0, 1, 0] (\"cat\") | Only the correct class |\n",
                "| Soft | [0.05, 0.90, 0.05] | \"Mostly cat, slightly similar to dog and bird\" |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Distillation Loss\n",
                "\n",
                "```\n",
                "L_total = Î± Ã— L_distill + (1-Î±) Ã— L_student\n",
                "\n",
                "Where:\n",
                "  L_distill = KL(teacher_soft_logits, student_soft_logits) Ã— TÂ²\n",
                "  L_student = CrossEntropy(student_logits, hard_labels)\n",
                "  T = Temperature (higher = softer probabilities)\n",
                "  Î± = Weighting factor (typically 0.5-0.9)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load Teacher and Student Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Teacher: A fine-tuned BERT model\n",
                "teacher_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n",
                "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_name).to(device)\n",
                "teacher_model.eval()  # Teacher is always in eval mode\n",
                "\n",
                "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
                "print(f\"Teacher: {teacher_name}\")\n",
                "print(f\"  Parameters: {teacher_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Student: A smaller model (we'll create a smaller DistilBERT)\n",
                "# For real distillation, you'd use an even smaller architecture\n",
                "# Here we'll use the same architecture but show the training process\n",
                "\n",
                "student_name = \"distilbert-base-uncased\"  # Base model, not fine-tuned\n",
                "student_tokenizer = AutoTokenizer.from_pretrained(student_name)\n",
                "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    student_name,\n",
                "    num_labels=2\n",
                ").to(device)\n",
                "\n",
                "student_params = sum(p.numel() for p in student_model.parameters())\n",
                "print(f\"\\nStudent: {student_name}\")\n",
                "print(f\"  Parameters: {student_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Prepare the Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load SST-2 dataset (same as teacher was trained on)\n",
                "dataset = load_dataset(\"glue\", \"sst2\")\n",
                "\n",
                "print(\"Dataset:\")\n",
                "print(f\"  Train: {len(dataset['train'])} examples\")\n",
                "print(f\"  Validation: {len(dataset['validation'])} examples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use subset for faster training\n",
                "train_size = 2000\n",
                "val_size = 500\n",
                "\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "val_data = dataset['validation'].shuffle(seed=42).select(range(val_size))\n",
                "\n",
                "# Tokenization\n",
                "def tokenize_function(examples):\n",
                "    return teacher_tokenizer(\n",
                "        examples['sentence'],\n",
                "        padding='max_length',\n",
                "        truncation=True,\n",
                "        max_length=128\n",
                "    )\n",
                "\n",
                "train_tokenized = train_data.map(tokenize_function, batched=True)\n",
                "val_tokenized = val_data.map(tokenize_function, batched=True)\n",
                "\n",
                "# Set format for PyTorch\n",
                "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "\n",
                "# Create dataloaders\n",
                "train_loader = DataLoader(train_tokenized, batch_size=16, shuffle=True)\n",
                "val_loader = DataLoader(val_tokenized, batch_size=32)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Implement Distillation Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DistillationLoss(nn.Module):\n",
                "    \"\"\"\n",
                "    Distillation loss combining soft label matching and hard label classification.\n",
                "    \n",
                "    L_total = Î± Ã— L_distill + (1-Î±) Ã— L_student\n",
                "    \"\"\"\n",
                "    def __init__(self, temperature=4.0, alpha=0.7):\n",
                "        super().__init__()\n",
                "        self.temperature = temperature\n",
                "        self.alpha = alpha\n",
                "        self.ce_loss = nn.CrossEntropyLoss()\n",
                "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
                "    \n",
                "    def forward(self, student_logits, teacher_logits, labels):\n",
                "        # Soft labels from teacher and student\n",
                "        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
                "        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
                "        \n",
                "        # Distillation loss (KL divergence)\n",
                "        distill_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n",
                "        \n",
                "        # Student loss (cross-entropy with hard labels)\n",
                "        student_loss = self.ce_loss(student_logits, labels)\n",
                "        \n",
                "        # Combined loss\n",
                "        total_loss = self.alpha * distill_loss + (1 - self.alpha) * student_loss\n",
                "        \n",
                "        return total_loss, distill_loss, student_loss\n",
                "\n",
                "# Initialize loss function\n",
                "distill_criterion = DistillationLoss(temperature=4.0, alpha=0.7)\n",
                "print(\"Distillation loss initialized with:\")\n",
                "print(f\"  Temperature: {distill_criterion.temperature}\")\n",
                "print(f\"  Alpha: {distill_criterion.alpha}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding Temperature\n",
                "\n",
                "Temperature controls how \"soft\" the probability distribution is:\n",
                "\n",
                "| Temperature | Effect |\n",
                "|-------------|--------|\n",
                "| T = 1 | Original distribution |\n",
                "| T = 2-5 | Smoother distribution (recommended) |\n",
                "| T > 10 | Very uniform distribution |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize temperature effect\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "logits = torch.tensor([2.0, 0.5, -1.0])\n",
                "temps = [1, 2, 4, 8]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
                "\n",
                "for ax, T in zip(axes, temps):\n",
                "    probs = F.softmax(logits / T, dim=-1).numpy()\n",
                "    ax.bar(['Class A', 'Class B', 'Class C'], probs, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
                "    ax.set_ylim(0, 1)\n",
                "    ax.set_title(f'Temperature = {T}')\n",
                "    ax.set_ylabel('Probability')\n",
                "\n",
                "plt.suptitle('Effect of Temperature on Softmax Probabilities', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Train the Student Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimizer and scheduler\n",
                "optimizer = AdamW(student_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
                "num_epochs = 3\n",
                "num_training_steps = num_epochs * len(train_loader)\n",
                "\n",
                "lr_scheduler = get_scheduler(\n",
                "    'linear',\n",
                "    optimizer=optimizer,\n",
                "    num_warmup_steps=int(0.1 * num_training_steps),\n",
                "    num_training_steps=num_training_steps\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(student_model, teacher_model, train_loader, optimizer, scheduler, criterion, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    student_model.train()\n",
                "    total_loss = 0\n",
                "    total_distill_loss = 0\n",
                "    total_student_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
                "    \n",
                "    for batch in progress_bar:\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['label'].to(device)\n",
                "        \n",
                "        # Get teacher predictions (no gradient)\n",
                "        with torch.no_grad():\n",
                "            teacher_outputs = teacher_model(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask\n",
                "            )\n",
                "            teacher_logits = teacher_outputs.logits\n",
                "        \n",
                "        # Get student predictions\n",
                "        student_outputs = student_model(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask\n",
                "        )\n",
                "        student_logits = student_outputs.logits\n",
                "        \n",
                "        # Calculate distillation loss\n",
                "        loss, distill_loss, student_loss = criterion(\n",
                "            student_logits, teacher_logits, labels\n",
                "        )\n",
                "        \n",
                "        # Backpropagation\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        scheduler.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        total_distill_loss += distill_loss.item()\n",
                "        total_student_loss += student_loss.item()\n",
                "        \n",
                "        progress_bar.set_postfix({\n",
                "            'loss': f'{loss.item():.4f}',\n",
                "            'distill': f'{distill_loss.item():.4f}'\n",
                "        })\n",
                "    \n",
                "    return {\n",
                "        'total_loss': total_loss / len(train_loader),\n",
                "        'distill_loss': total_distill_loss / len(train_loader),\n",
                "        'student_loss': total_student_loss / len(train_loader)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, val_loader, device):\n",
                "    \"\"\"Evaluate model accuracy.\"\"\"\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in val_loader:\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['label'].to(device)\n",
                "            \n",
                "            outputs = model(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask\n",
                "            )\n",
                "            \n",
                "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
                "            correct += (predictions == labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "    \n",
                "    return correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate teacher first (baseline)\n",
                "teacher_acc = evaluate_model(teacher_model, val_loader, device)\n",
                "print(f\"Teacher Accuracy: {teacher_acc:.2%}\")\n",
                "\n",
                "# Evaluate un-distilled student\n",
                "student_acc_before = evaluate_model(student_model, val_loader, device)\n",
                "print(f\"Student Accuracy (before distillation): {student_acc_before:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Starting Distillation Training\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "history = []\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
                "    print(\"-\" * 30)\n",
                "    \n",
                "    # Train\n",
                "    train_metrics = train_epoch(\n",
                "        student_model, teacher_model, train_loader,\n",
                "        optimizer, lr_scheduler, distill_criterion, device\n",
                "    )\n",
                "    \n",
                "    # Evaluate\n",
                "    student_acc = evaluate_model(student_model, val_loader, device)\n",
                "    \n",
                "    history.append({\n",
                "        'epoch': epoch + 1,\n",
                "        'loss': train_metrics['total_loss'],\n",
                "        'accuracy': student_acc\n",
                "    })\n",
                "    \n",
                "    print(f\"Loss: {train_metrics['total_loss']:.4f}\")\n",
                "    print(f\"  - Distillation Loss: {train_metrics['distill_loss']:.4f}\")\n",
                "    print(f\"  - Student Loss: {train_metrics['student_loss']:.4f}\")\n",
                "    print(f\"Validation Accuracy: {student_acc:.2%}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Distillation Complete!\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Compare Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final evaluation\n",
                "student_acc_after = evaluate_model(student_model, val_loader, device)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Results Comparison\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nTeacher Accuracy:                    {teacher_acc:.2%}\")\n",
                "print(f\"Student Accuracy (before):           {student_acc_before:.2%}\")\n",
                "print(f\"Student Accuracy (after distill):    {student_acc_after:.2%}\")\n",
                "print(f\"\\nImprovement:                         +{(student_acc_after - student_acc_before)*100:.1f}%\")\n",
                "print(f\"Gap to Teacher:                      {(teacher_acc - student_acc_after)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize training progress\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "epochs = [h['epoch'] for h in history]\n",
                "losses = [h['loss'] for h in history]\n",
                "accuracies = [h['accuracy'] for h in history]\n",
                "\n",
                "# Loss plot\n",
                "axes[0].plot(epochs, losses, 'b-o', linewidth=2, markersize=8)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training Loss')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy plot\n",
                "axes[1].plot(epochs, accuracies, 'g-o', linewidth=2, markersize=8, label='Student')\n",
                "axes[1].axhline(y=teacher_acc, color='r', linestyle='--', label=f'Teacher ({teacher_acc:.2%})')\n",
                "axes[1].axhline(y=student_acc_before, color='gray', linestyle=':', label=f'Student (before)')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].set_title('Validation Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Test the Distilled Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on new examples\n",
                "test_texts = [\n",
                "    \"This movie was absolutely wonderful!\",\n",
                "    \"Terrible waste of time.\",\n",
                "    \"The acting was superb but the plot was confusing.\"\n",
                "]\n",
                "\n",
                "print(\"Comparison on Test Examples:\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for text in test_texts:\n",
                "    inputs = teacher_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        teacher_out = teacher_model(**inputs).logits\n",
                "        student_out = student_model(**inputs).logits\n",
                "    \n",
                "    teacher_probs = F.softmax(teacher_out, dim=-1)[0]\n",
                "    student_probs = F.softmax(student_out, dim=-1)[0]\n",
                "    \n",
                "    teacher_pred = \"POSITIVE\" if teacher_probs[1] > 0.5 else \"NEGATIVE\"\n",
                "    student_pred = \"POSITIVE\" if student_probs[1] > 0.5 else \"NEGATIVE\"\n",
                "    \n",
                "    print(f\"\\nText: {text}\")\n",
                "    print(f\"  Teacher: {teacher_pred} ({teacher_probs[1]:.2%} positive)\")\n",
                "    print(f\"  Student: {student_pred} ({student_probs[1]:.2%} positive)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "### Challenge: Experiment with Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Experiment with different distillation settings:\n",
                "\n",
                "# 1. Try different temperatures (T = 1, 2, 8, 16)\n",
                "#    - Record accuracy for each\n",
                "#    - Which temperature works best?\n",
                "\n",
                "# 2. Try different alpha values (Î± = 0.3, 0.5, 0.7, 0.9)\n",
                "#    - Î± closer to 1 = more weight on distillation loss\n",
                "#    - Î± closer to 0 = more weight on hard labels\n",
                "\n",
                "# 3. What happens with more training epochs?\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Distillation** transfers knowledge from a large model to a smaller one\n",
                "2. **Soft labels** contain more information than hard labels\n",
                "3. **Temperature** controls how soft the probability distribution is\n",
                "4. **Combined loss** uses both distillation and classification objectives\n",
                "5. Student can achieve **95-99%** of teacher performance\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `03_pruning.ipynb` for weight pruning techniques!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
