{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Pruning\n",
                "\n",
                "**Module 03 | Notebook 3 of 5**\n",
                "\n",
                "Pruning removes unnecessary weights from neural networks, making them smaller and faster.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Understand different pruning strategies\n",
                "2. Apply L1 unstructured pruning with PyTorch\n",
                "3. Visualize sparsity in neural networks\n",
                "4. Analyze the accuracy vs. sparsity trade-off\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch matplotlib seaborn pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.utils.prune as prune\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from datasets import load_dataset\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from copy import deepcopy\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## What is Pruning?\n",
                "\n",
                "### The Intuition\n",
                "\n",
                "Neural networks are often **over-parameterized**. Many weights contribute little to the output.\n",
                "\n",
                "```\n",
                "BEFORE PRUNING:                AFTER PRUNING (50%):\n",
                "â—â”â”â”â”â”â”â—â”â”â”â”â”â”â—                â—â”â”â”â”â”â”â—â”â”â”â”â”â”â—\n",
                " \\    /|\\    /                  \\      |\\    /\n",
                "  \\  / | \\  /                    \\     | \\  /\n",
                "   \\/  |  \\/                      \\    |  \\/\n",
                "   /\\  |  /\\                       \\   |  /\\\n",
                "  /  \\ | /  \\                       \\  | /  \\\n",
                " /    \\|/    \\                       \\ |/    \\\n",
                "â—â”â”â”â”â”â”â—â”â”â”â”â”â”â—                â—      â—      â—\n",
                " \\    /|\\    /                        |\\    /\n",
                "  \\  / | \\  /                         | \\  /\n",
                "   \\/  |  \\/                          |  \\/\n",
                "   /\\  |  /\\                          |  /\\\n",
                "  /  \\ | /  \\                         | /  \\\n",
                " /    \\|/    \\                        |/    \\\n",
                "â—â”â”â”â”â”â”â—â”â”â”â”â”â”â—                â—      â—      â—\n",
                "\n",
                "All connections             Weak connections removed\n",
                "```\n",
                "\n",
                "### Types of Pruning\n",
                "\n",
                "| Type | Description | Pros | Cons |\n",
                "|------|-------------|------|------|\n",
                "| **Unstructured** | Remove individual weights | Most flexible, highest sparsity | Irregular memory access |\n",
                "| **Structured** | Remove entire neurons/channels | Hardware-friendly | Less flexible |\n",
                "| **Magnitude-based** | Remove smallest weights | Simple, effective | May miss important small weights |\n",
                "| **Gradient-based** | Remove by gradient importance | More accurate | Requires training data |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load and Analyze the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a pre-trained model\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "# Count parameters\n",
                "def count_parameters(model):\n",
                "    total = sum(p.numel() for p in model.parameters())\n",
                "    nonzero = sum((p != 0).sum().item() for p in model.parameters())\n",
                "    return total, nonzero\n",
                "\n",
                "total, nonzero = count_parameters(model)\n",
                "print(f\"Model: {model_name}\")\n",
                "print(f\"Total parameters: {total:,}\")\n",
                "print(f\"Non-zero parameters: {nonzero:,}\")\n",
                "print(f\"Sparsity: {(1 - nonzero/total)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize weight distribution before pruning\n",
                "def get_all_weights(model):\n",
                "    weights = []\n",
                "    for name, param in model.named_parameters():\n",
                "        if 'weight' in name and param.dim() >= 2:\n",
                "            weights.extend(param.detach().cpu().flatten().numpy())\n",
                "    return np.array(weights)\n",
                "\n",
                "weights_before = get_all_weights(model)\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(weights_before, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
                "plt.xlabel('Weight Value')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Weight Distribution (Before Pruning)')\n",
                "plt.axvline(x=0, color='red', linestyle='--', label='Zero')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.hist(np.abs(weights_before), bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
                "plt.xlabel('Absolute Weight Value')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Absolute Weight Distribution')\n",
                "plt.axvline(x=np.percentile(np.abs(weights_before), 20), color='red', linestyle='--', label='20% threshold')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nWeight statistics:\")\n",
                "print(f\"  Min: {weights_before.min():.4f}\")\n",
                "print(f\"  Max: {weights_before.max():.4f}\")\n",
                "print(f\"  Mean: {weights_before.mean():.4f}\")\n",
                "print(f\"  Std: {weights_before.std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Apply Unstructured Pruning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_pruning(model, amount=0.3, prune_type='l1'):\n",
                "    \"\"\"\n",
                "    Apply pruning to all Linear layers in the model.\n",
                "    \n",
                "    Args:\n",
                "        model: PyTorch model\n",
                "        amount: Fraction of parameters to prune (0-1)\n",
                "        prune_type: 'l1' for magnitude-based, 'random' for random\n",
                "    \"\"\"\n",
                "    for name, module in model.named_modules():\n",
                "        if isinstance(module, nn.Linear):\n",
                "            if prune_type == 'l1':\n",
                "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
                "            elif prune_type == 'random':\n",
                "                prune.random_unstructured(module, name='weight', amount=amount)\n",
                "    \n",
                "    return model\n",
                "\n",
                "def remove_pruning_reparametrization(model):\n",
                "    \"\"\"\n",
                "    Make pruning permanent by removing the reparametrization.\n",
                "    \"\"\"\n",
                "    for name, module in model.named_modules():\n",
                "        if isinstance(module, nn.Linear):\n",
                "            try:\n",
                "                prune.remove(module, 'weight')\n",
                "            except:\n",
                "                pass\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for pruning\n",
                "model_pruned = deepcopy(model)\n",
                "\n",
                "# Apply 30% pruning\n",
                "prune_amount = 0.3\n",
                "model_pruned = apply_pruning(model_pruned, amount=prune_amount)\n",
                "\n",
                "print(f\"Applied {prune_amount*100:.0f}% L1 unstructured pruning\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check sparsity after pruning\n",
                "total, nonzero = count_parameters(model_pruned)\n",
                "sparsity = (1 - nonzero/total) * 100\n",
                "\n",
                "print(f\"After pruning:\")\n",
                "print(f\"  Total parameters: {total:,}\")\n",
                "print(f\"  Non-zero parameters: {nonzero:,}\")\n",
                "print(f\"  Sparsity: {sparsity:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize weight distribution after pruning\n",
                "weights_after = get_all_weights(model_pruned)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Before\n",
                "axes[0].hist(weights_before, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
                "axes[0].set_xlabel('Weight Value')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title('Before Pruning')\n",
                "\n",
                "# After\n",
                "axes[1].hist(weights_after, bins=100, alpha=0.7, color='coral', edgecolor='black')\n",
                "axes[1].set_xlabel('Weight Value')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].set_title(f'After Pruning ({prune_amount*100:.0f}%)')\n",
                "\n",
                "# Add zero spike annotation\n",
                "zero_count = (np.abs(weights_after) < 1e-8).sum()\n",
                "axes[1].annotate(f'{zero_count:,} zeros\\n({zero_count/len(weights_after)*100:.1f}%)', \n",
                "                 xy=(0, axes[1].get_ylim()[1]*0.5),\n",
                "                 fontsize=10, ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Visualize Sparsity per Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_layer_sparsity(model):\n",
                "    \"\"\"Calculate sparsity for each layer.\"\"\"\n",
                "    sparsity_data = []\n",
                "    \n",
                "    for name, module in model.named_modules():\n",
                "        if isinstance(module, nn.Linear):\n",
                "            weight = module.weight.data\n",
                "            total = weight.numel()\n",
                "            zeros = (weight == 0).sum().item()\n",
                "            sparsity = zeros / total * 100\n",
                "            \n",
                "            sparsity_data.append({\n",
                "                'Layer': name.split('.')[-2] + '.' + name.split('.')[-1] if '.' in name else name,\n",
                "                'Total Params': total,\n",
                "                'Zeros': zeros,\n",
                "                'Sparsity (%)': sparsity\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(sparsity_data)\n",
                "\n",
                "sparsity_df = get_layer_sparsity(model_pruned)\n",
                "print(\"Layer-wise Sparsity:\")\n",
                "print(sparsity_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sparsity per layer\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.barh(range(len(sparsity_df)), sparsity_df['Sparsity (%)'], color='coral')\n",
                "plt.yticks(range(len(sparsity_df)), sparsity_df['Layer'], fontsize=8)\n",
                "plt.xlabel('Sparsity (%)')\n",
                "plt.title('Sparsity per Layer')\n",
                "plt.axvline(x=30, color='red', linestyle='--', label='Target (30%)')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluate Accuracy Impact"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "dataset = load_dataset(\"glue\", \"sst2\")\n",
                "test_data = dataset['validation'].shuffle(seed=42).select(range(500))\n",
                "\n",
                "def evaluate_accuracy(model, tokenizer, test_data, device='cpu'):\n",
                "    model = model.to(device)\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for example in test_data:\n",
                "            inputs = tokenizer(\n",
                "                example['sentence'],\n",
                "                return_tensors='pt',\n",
                "                truncation=True,\n",
                "                max_length=128\n",
                "            ).to(device)\n",
                "            \n",
                "            outputs = model(**inputs)\n",
                "            pred = outputs.logits.argmax(dim=-1).item()\n",
                "            \n",
                "            if pred == example['label']:\n",
                "                correct += 1\n",
                "    \n",
                "    return correct / len(test_data)\n",
                "\n",
                "print(\"Evaluating models...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare accuracy\n",
                "acc_original = evaluate_accuracy(model, tokenizer, test_data, device)\n",
                "acc_pruned = evaluate_accuracy(model_pruned, tokenizer, test_data, device)\n",
                "\n",
                "print(f\"\\nAccuracy Comparison:\")\n",
                "print(f\"  Original Model: {acc_original:.2%}\")\n",
                "print(f\"  Pruned Model ({prune_amount*100:.0f}%): {acc_pruned:.2%}\")\n",
                "print(f\"  Accuracy Drop: {(acc_original - acc_pruned)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Sparsity vs. Accuracy Trade-off"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different pruning levels\n",
                "prune_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
                "results = []\n",
                "\n",
                "print(\"Testing different pruning levels...\")\n",
                "for level in prune_levels:\n",
                "    # Create fresh copy\n",
                "    test_model = deepcopy(model)\n",
                "    \n",
                "    if level > 0:\n",
                "        test_model = apply_pruning(test_model, amount=level)\n",
                "    \n",
                "    # Evaluate\n",
                "    acc = evaluate_accuracy(test_model, tokenizer, test_data, device)\n",
                "    total, nonzero = count_parameters(test_model)\n",
                "    actual_sparsity = (1 - nonzero/total) * 100\n",
                "    \n",
                "    results.append({\n",
                "        'Target Sparsity': level * 100,\n",
                "        'Actual Sparsity': actual_sparsity,\n",
                "        'Accuracy': acc * 100\n",
                "    })\n",
                "    \n",
                "    print(f\"  {level*100:.0f}% pruning â†’ {acc:.2%} accuracy\")\n",
                "    \n",
                "    del test_model\n",
                "\n",
                "results_df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot trade-off\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "ax.plot(results_df['Target Sparsity'], results_df['Accuracy'], 'bo-', linewidth=2, markersize=10)\n",
                "\n",
                "# Highlight \"sweet spot\"\n",
                "ax.axhspan(results_df['Accuracy'].iloc[0] - 2, results_df['Accuracy'].iloc[0], \n",
                "           alpha=0.2, color='green', label='<2% accuracy drop zone')\n",
                "\n",
                "ax.set_xlabel('Sparsity (%)', fontsize=12)\n",
                "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
                "ax.set_title('Sparsity vs. Accuracy Trade-off', fontsize=14)\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.legend()\n",
                "\n",
                "# Annotate points\n",
                "for _, row in results_df.iterrows():\n",
                "    ax.annotate(f\"{row['Accuracy']:.1f}%\", \n",
                "                (row['Target Sparsity'], row['Accuracy']),\n",
                "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nResults Summary:\")\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Making Pruning Permanent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove pruning reparametrization to make it permanent\n",
                "model_final = deepcopy(model_pruned)\n",
                "model_final = remove_pruning_reparametrization(model_final)\n",
                "\n",
                "# Verify\n",
                "total, nonzero = count_parameters(model_final)\n",
                "print(f\"Final pruned model:\")\n",
                "print(f\"  Total parameters: {total:,}\")\n",
                "print(f\"  Non-zero parameters: {nonzero:,}\")\n",
                "print(f\"  Sparsity: {(1 - nonzero/total)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the pruned model\n",
                "save_path = \"./pruned_model\"\n",
                "model_final.save_pretrained(save_path)\n",
                "tokenizer.save_pretrained(save_path)\n",
                "print(f\"\\nPruned model saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "### Challenge: Find Optimal Pruning Level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find the maximum pruning level that keeps accuracy drop < 1%\n",
                "\n",
                "# 1. Based on the results above, what is the sweet spot?\n",
                "# 2. Try finer-grained pruning levels (e.g., 0.35, 0.40, 0.45)\n",
                "# 3. What if you use random pruning instead of L1? Is L1 better?\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Pruning removes weights** based on importance (usually magnitude)\n",
                "2. **Unstructured pruning** is flexible but may not speed up inference\n",
                "3. **L1 pruning** removes smallest absolute values\n",
                "4. **30-50% sparsity** often has minimal accuracy impact\n",
                "5. **Trade-off exists** between sparsity and accuracy\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `04_quantization.ipynb` for precision reduction techniques!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
