{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ Mastering LLM Deployment\n",
                "\n",
                "**Welcome to the LLM Deployment Course!**\n",
                "\n",
                "This course will teach you how to deploy Large Language Models (LLMs) efficiently and cost-effectively. By the end, you'll have hands-on experience with model optimization techniques and cloud deployment.\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Course Modules\n",
                "\n",
                "| Module | Topic | What You'll Learn |\n",
                "|--------|-------|-------------------|\n",
                "| **01** | Foundations | Transformers, tokenization, model architecture |\n",
                "| **02** | Fine-Tuning | Transfer learning, sentiment analysis, summarization |\n",
                "| **03** | Optimization | Distillation, pruning, quantization, benchmarking |\n",
                "| **04** | Deployment | FastAPI, Gradio, Docker, AWS ECS |\n",
                "| **05** | Capstone | End-to-end project |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì Learning Objectives\n",
                "\n",
                "By completing this course, you will be able to:\n",
                "\n",
                "1. **Load and use** pre-trained transformer models from Hugging Face\n",
                "2. **Fine-tune** models for classification and text generation tasks\n",
                "3. **Optimize** models using distillation, pruning, and quantization\n",
                "4. **Benchmark** model performance (latency, memory, accuracy)\n",
                "5. **Deploy** models using REST APIs, Gradio UIs, and Docker containers\n",
                "6. **Scale** deployments on AWS ECS with cost optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Environment Setup\n",
                "\n",
                "This course is designed to run on **Google Colab** for free GPU access.\n",
                "\n",
                "### Step 1: Enable GPU Runtime\n",
                "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
                "2. Select **T4 GPU** (or any available GPU)\n",
                "3. Click **Save**\n",
                "\n",
                "### Step 2: Verify GPU Access\n",
                "Run the cell below to confirm GPU is available:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime > Change runtime type > Select GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Install Dependencies\n",
                "\n",
                "Run this cell to install all required libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate rouge-score\n",
                "!pip install gradio fastapi uvicorn\n",
                "!pip install bitsandbytes\n",
                "\n",
                "print(\"‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Verify Transformers Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import transformers\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "print(f\"Transformers version: {transformers.__version__}\")\n",
                "\n",
                "# Quick test: Load a small model\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "print(\"\\n‚úÖ Transformers library working correctly!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìã Prerequisites Checklist\n",
                "\n",
                "Before starting, make sure you're comfortable with:\n",
                "\n",
                "- [ ] **Python basics**: functions, classes, list comprehensions\n",
                "- [ ] **NumPy/Pandas**: array operations, DataFrames\n",
                "- [ ] **Basic ML concepts**: training vs. inference, loss functions\n",
                "- [ ] **Neural networks**: layers, forward pass, backpropagation (conceptual)\n",
                "\n",
                "### Quick Self-Assessment\n",
                "\n",
                "Can you explain what this code does?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Self-assessment: What does this code do?\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Answer: This defines a simple 2-layer neural network with ReLU activation\n",
                "model = SimpleNet(10, 32, 2)\n",
                "print(f\"Model architecture:\\n{model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üó∫Ô∏è Course Navigation\n",
                "\n",
                "### Module 01: Foundations\n",
                "Start here to understand how transformers work:\n",
                "- `01_Foundations/01_transformers_basics.ipynb` - Model loading and tokenization\n",
                "- `01_Foundations/02_model_architecture.ipynb` - Understanding model internals\n",
                "\n",
                "### Module 02: Fine-Tuning\n",
                "Learn to adapt pre-trained models for specific tasks:\n",
                "- `02_Fine_Tuning/01_transfer_learning.ipynb` - Transfer learning concepts\n",
                "- `02_Fine_Tuning/02_sentiment_analysis.ipynb` - IMDB classification\n",
                "- `02_Fine_Tuning/03_summarization.ipynb` - Text summarization\n",
                "\n",
                "### Module 03: Optimization\n",
                "Make models faster and smaller:\n",
                "- `03_Model_Optimization/01_intro_to_optimization.ipynb` - Why optimize?\n",
                "- `03_Model_Optimization/02_knowledge_distillation.ipynb` - Teacher-student training\n",
                "- `03_Model_Optimization/03_pruning.ipynb` - Removing unnecessary weights\n",
                "- `03_Model_Optimization/04_quantization.ipynb` - Reducing precision\n",
                "- `03_Model_Optimization/05_benchmarking.ipynb` - Comparing techniques\n",
                "\n",
                "### Module 04: Deployment\n",
                "Put models into production:\n",
                "- `04_Deployment/01_local_serving.ipynb` - FastAPI endpoints\n",
                "- `04_Deployment/02_gradio_ui.ipynb` - Interactive demos\n",
                "- `04_Deployment/03_docker_packaging.md` - Containerization\n",
                "- `04_Deployment/04_aws_ecs_deployment.md` - Cloud deployment\n",
                "\n",
                "### Module 05: Capstone\n",
                "Bring it all together:\n",
                "- `05_Capstone/capstone_project.ipynb` - End-to-end project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üöÄ Quick Start: Your First Transformer\n",
                "\n",
                "Let's run a quick example to see transformers in action!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Load a text generation pipeline\n",
                "generator = pipeline(\"text-generation\", model=\"gpt2\", device=0 if torch.cuda.is_available() else -1)\n",
                "\n",
                "# Generate text\n",
                "prompt = \"The future of AI is\"\n",
                "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
                "\n",
                "print(f\"Prompt: {prompt}\")\n",
                "print(f\"\\nGenerated: {result[0]['generated_text']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try sentiment analysis\n",
                "classifier = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
                "\n",
                "texts = [\n",
                "    \"I love this course! It's amazing!\",\n",
                "    \"This is confusing and frustrating.\",\n",
                "    \"The weather is okay today.\"\n",
                "]\n",
                "\n",
                "print(\"Sentiment Analysis Results:\")\n",
                "print(\"-\" * 50)\n",
                "for text in texts:\n",
                "    result = classifier(text)[0]\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"  ‚Üí {result['label']} (confidence: {result['score']:.2%})\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Ready to Start!\n",
                "\n",
                "You've successfully:\n",
                "- ‚úÖ Set up your environment\n",
                "- ‚úÖ Verified GPU access\n",
                "- ‚úÖ Installed dependencies\n",
                "- ‚úÖ Run your first transformer models\n",
                "\n",
                "**Next Step**: Open `01_Foundations/01_transformers_basics.ipynb` to begin Module 01!\n",
                "\n",
                "---\n",
                "\n",
                "## üìû Getting Help\n",
                "\n",
                "If you encounter issues:\n",
                "1. **Runtime errors**: Restart the runtime (Runtime ‚Üí Restart runtime)\n",
                "2. **Out of memory**: Use a smaller model or reduce batch size\n",
                "3. **Import errors**: Re-run the pip install cell\n",
                "4. **GPU unavailable**: Check you've enabled GPU in runtime settings"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}