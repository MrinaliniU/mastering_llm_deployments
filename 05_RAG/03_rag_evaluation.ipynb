{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "evaluation-intro-title",
            "metadata": {},
            "source": [
                "# RAG Evaluation and Observability with MLflow\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "\n",
                "1. **Build** a complete RAG pipeline using LangChain v1.0+\n",
                "2. **Understand** why evaluation is critical for production RAG systems\n",
                "3. **Create** a \"Golden Dataset\" for systematic evaluation\n",
                "4. **Use MLflow** to track experiments and enable observability\n",
                "5. **Interpret** LLM-as-a-Judge metrics (Faithfulness, Answer Relevance)\n",
                "6. **Debug** RAG failures using per-question analysis\n",
                "7. **Iterate** on RAG configurations using experiment comparison\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "why-evaluate-rag",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Why Evaluate RAG Systems?\n",
                "\n",
                "Before we dive into building, let's understand **why evaluation matters**.\n",
                "\n",
                "### The Problem: \"It Looks Good\" is Not Enough\n",
                "\n",
                "When you test a chatbot manually, you might ask 5-10 questions and think: *\"The answers seem reasonable!\"* But in production:\n",
                "\n",
                "- You can't manually check thousands of queries\n",
                "- Users will ask questions you never anticipated\n",
                "- Small changes (new documents, different LLM) can break things silently\n",
                "\n",
                "### What Can Go Wrong in RAG?\n",
                "\n",
                "| Failure Type | Description | Example |\n",
                "|--------------|-------------|--------|\n",
                "| **Retrieval Failure** | Wrong documents were fetched | User asks about \"Python\" (the language) but retrieves documents about \"python\" (the snake) |\n",
                "| **Hallucination** | LLM invents information not in documents | LLM confidently states a date that doesn't exist in your PDFs |\n",
                "| **Irrelevant Answer** | Answer is technically correct but doesn't address the question | User asks \"How do I install X?\" and gets \"X was developed in 2020...\" |\n",
                "| **Context Window Overflow** | Too many chunks stuffed into prompt | The LLM gets confused or ignores important context |\n",
                "\n",
                "### The Solution: Systematic Evaluation\n",
                "\n",
                "We need:\n",
                "1. **A benchmark** (Golden Dataset) with known correct answers\n",
                "2. **Automated metrics** that can score answers at scale\n",
                "3. **Observability** to trace what happened at each step\n",
                "4. **Experiment tracking** to compare different configurations\n",
                "\n",
                "> ðŸ’¡ **Key Insight**: Evaluation is not just about qualityâ€”it's about **confidence**. You need to know *when* your system will fail, not just hope it won't.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "observability-intro",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ”­ Introduction to LLM Observability\n",
                "\n",
                "### What is Observability?\n",
                "\n",
                "**Observability** is the ability to understand what's happening *inside* your system by examining its *outputs*. For LLM applications, this means:\n",
                "\n",
                "- **Traces**: The complete journey of a request (query â†’ retrieval â†’ generation â†’ response)\n",
                "- **Metrics**: Quantitative measurements (latency, token count, quality scores)\n",
                "- **Logs**: Detailed records of inputs, outputs, and intermediate steps\n",
                "\n",
                "### Why Observability for RAG?\n",
                "\n",
                "RAG systems are **multi-step pipelines**. When something goes wrong, you need to know:\n",
                "\n",
                "```\n",
                "User Query â†’ [Embedding] â†’ [Retrieval] â†’ [Prompt Construction] â†’ [LLM Generation] â†’ Response\n",
                "     â†“            â†“             â†“                â†“                     â†“              â†“\n",
                "  Logged?      Traced?      What docs?      What prompt?          What output?    Scored?\n",
                "```\n",
                "\n",
                "Without observability, debugging is like finding a needle in a haystack.\n",
                "\n",
                "### MLflow for LLM Observability\n",
                "\n",
                "**MLflow** is an open-source platform originally designed for ML experiment tracking. It now supports:\n",
                "\n",
                "| Feature | Description |\n",
                "|---------|-------------|\n",
                "| **Autologging** | Automatically capture LangChain traces (no code changes!) |\n",
                "| **Experiment Tracking** | Compare different configurations side-by-side |\n",
                "| **LLM Evaluation** | Built-in metrics for faithfulness, relevance, etc. |\n",
                "| **Artifacts** | Store evaluation datasets and results |\n",
                "| **UI** | Visual dashboard to explore all of the above |\n",
                "\n",
                "> ðŸ“Œ **In this notebook**, we use `mlflow.langchain.autolog()` to automatically capture every LangChain call, then `mlflow.evaluate()` to score our RAG responses.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "install-packages-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 0: Install Required Packages\n",
                "\n",
                "Before we begin, we need to install the required packages. This cell installs:\n",
                "\n",
                "- **`langchain`**: The core LangChain framework\n",
                "- **`langchain-community`**: Community integrations (document loaders, etc.)\n",
                "- **`langchain-openai`**: OpenAI-specific components (embeddings, chat models)\n",
                "- **`langchain-chroma`**: Chroma vector store integration\n",
                "- **`langgraph`**: Graph-based agent orchestration (required for modern agents in v1.0+)\n",
                "- **`pypdf`**: PDF parsing library\n",
                "- **`gradio`**: Web interface for interactive demos\n",
                "- **`python-dotenv`**: Environment variable management\n",
                "\n",
                "> âš ï¸ **Important**: After running this cell, you may need to **restart the kernel** to ensure all packages are properly loaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "50791208",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Packages installed! Please restart the kernel if this is your first time running this cell.\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages\n",
                "# Note: The --force-reinstall for numpy and scipy fixes potential binary incompatibility issues\n",
                "# !uv pip install -U -q langchain langchain-community langchain-openai langchain-chroma langgraph pypdf gradio python-dotenv\n",
                "# !uv pip install --force-reinstall -q numpy scipy\n",
                "\n",
                "print(\"âœ… Packages installed! Please restart the kernel if this is your first time running this cell.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "d83dec03",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… MLflow installed - restart kernel if this is your first time\n"
                    ]
                }
            ],
            "source": [
                "# Install MLflow with GenAI evaluation support\n",
                "!uv pip install -q mlflow\n",
                "\n",
                "print(\"âœ… MLflow installed - restart kernel if this is your first time\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b741c31",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Environment Setup\n",
                "\n",
                "We need to configure our API keys to authenticate with OpenAI. This notebook supports both:\n",
                "\n",
                "- **Google Colab**: Uses `google.colab.userdata` to securely access keys stored in Colab Secrets\n",
                "- **Local Execution**: Uses `python-dotenv` to load keys from a `.env` file\n",
                "\n",
                "### Setting Up Your API Key\n",
                "\n",
                "**For local development**, create a `.env` file in this directory with:\n",
                "```\n",
                "OPENAI_API_KEY=your-api-key-here\n",
                "```\n",
                "\n",
                "**For Colab**, add your key to Colab Secrets with the name `OPENAI_API_KEY`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f587032c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… OPENAI_API_KEY loaded successfully\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Configuration\n",
                "MODEL = \"gpt-4o-mini\"  # The LLM model to use\n",
                "db_name = \"vector_db\"  # Directory name for the vector store\n",
                "\n",
                "# Option 1: Set your API key directly (for Colab)\n",
                "#from google.colab import userdata\n",
                "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "\n",
                "# Option 2\n",
                "# Load environment variables from .env file\n",
                "# from dotenv import load_dotenv\n",
                "# load_dotenv()\n",
                "\n",
                "# Verify API key is set\n",
                "if os.environ.get(\"OPENAI_API_KEY\"):\n",
                "    print(\"âœ… OPENAI_API_KEY loaded successfully\")\n",
                "else:\n",
                "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found. Please set it in your .env file or environment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "architecture-diagram",
            "metadata": {},
            "source": [
                "### RAG Chain Architecture\n",
                "\n",
                "Here's the complete flow of our RAG system:\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚  RAG Chain Flow                                     â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚                                                     â”‚\n",
                "â”‚  1. User Question + Chat History                    â”‚\n",
                "â”‚     â†“                                               â”‚\n",
                "â”‚  2. History-Aware Retriever                         â”‚\n",
                "â”‚     (Reformulates question to be standalone)        â”‚\n",
                "â”‚     â†“                                               â”‚\n",
                "â”‚  3. Vector Store Search (Chroma)                    â”‚\n",
                "â”‚     (Finds top-k most similar chunks)               â”‚\n",
                "â”‚     â†“                                               â”‚\n",
                "â”‚  4. Question-Answer Chain                           â”‚\n",
                "â”‚     (LLM generates answer using retrieved context)  â”‚\n",
                "â”‚     â†“                                               â”‚\n",
                "â”‚  5. Final Response                                  â”‚\n",
                "â”‚                                                     â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "**Key Components:**\n",
                "- **History-Aware Retriever**: Handles follow-up questions by reformulating them\n",
                "- **Vector Store**: Stores embeddings and performs semantic search\n",
                "- **Stuff Documents Chain**: \"Stuffs\" all retrieved docs into the LLM prompt\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df53314a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Import Dependencies\n",
                "\n",
                "Now we import all the necessary modules from LangChain and other libraries. Here's what each import does:\n",
                "\n",
                "### Document Processing\n",
                "- **`DirectoryLoader`**: Loads multiple files from a directory\n",
                "- **`PyPDFLoader`**: Parses PDF files into text\n",
                "- **`RecursiveCharacterTextSplitter`**: Splits text into chunks while respecting natural boundaries\n",
                "\n",
                "### Embeddings & Vector Store\n",
                "- **`OpenAIEmbeddings`**: Converts text to vector embeddings using OpenAI's models\n",
                "- **`Chroma`**: A fast, open-source vector database\n",
                "\n",
                "### LLM & Chains\n",
                "- **`ChatOpenAI`**: OpenAI's chat models (GPT-4, etc.)\n",
                "- **`create_history_aware_retriever`**: Creates a retriever that understands conversation context\n",
                "- **`create_retrieval_chain`**: Combines retrieval and generation into a single chain\n",
                "- **`create_stuff_documents_chain`**: Creates a chain that \"stuffs\" documents into the prompt\n",
                "\n",
                "### Prompts & Messages\n",
                "- **`ChatPromptTemplate`**: Templates for structured prompts\n",
                "- **`MessagesPlaceholder`**: Placeholder for conversation history\n",
                "- **`HumanMessage` / `AIMessage`**: Message types for chat history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "f8941dc4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… All imports successful!\n"
                    ]
                }
            ],
            "source": [
                "import glob\n",
                "import os\n",
                "\n",
                "# Document loading and processing\n",
                "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Embeddings and LLM\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "\n",
                "# Vector store\n",
                "from langchain_chroma import Chroma\n",
                "\n",
                "# Chains for RAG\n",
                "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
                "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
                "\n",
                "# Prompts and messages\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "# UI\n",
                "import gradio as gr\n",
                "\n",
                "print(\"âœ… All imports successful!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "4f0b0349",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/12 14:18:17 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
                        "2025/12/12 14:18:17 INFO mlflow.store.db.utils: Updating database tables\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
                        "2025-12-12 14:18:17 INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
                        "2025-12-12 14:18:17 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
                        "2025-12-12 14:18:17 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
                        "2025-12-12 14:18:17 INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
                        "2025-12-12 14:18:18 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
                        "2025/12/12 14:18:18 INFO mlflow.tracking.fluent: Experiment with name 'RAG_PDF_Embeddings_Evaluation_v3' does not exist. Creating a new experiment.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… MLflow configured with SQLite backend\n",
                        "ðŸ’¡ Your experiments are stored in: mlflow.db\n",
                        "ðŸ’¡ Start MLflow UI in terminal: mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
                        "   Then visit: http://localhost:5000\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import mlflow\n",
                "\n",
                "# Configure MLflow to use SQLite database instead of deprecated filesystem\n",
                "# This provides better durability, easier collaboration, and is future-proof\n",
                "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
                "\n",
                "# Optional: import GenAI evaluation metrics if you plan to use mlflow.evaluate\n",
                "from mlflow.metrics.genai import (\n",
                "    faithfulness,\n",
                "    answer_relevance,\n",
                ")\n",
                "\n",
                "# Enable automatic tracing for your LangChain RAG pipeline\n",
                "# By default, trace logging is enabled; you can add more options per your MLflow version.\n",
                "mlflow.langchain.autolog(\n",
                "    log_traces=True,\n",
                ")\n",
                "\n",
                "# Set experiment name (all runs will be grouped here)\n",
                "mlflow.set_experiment(\"RAG_PDF_Embeddings_Evaluation_v3\")\n",
                "\n",
                "print(\"âœ… MLflow configured with SQLite backend\")\n",
                "print(\"ðŸ’¡ Your experiments are stored in: mlflow.db\")\n",
                "print(\"ðŸ’¡ Start MLflow UI in terminal: mlflow ui --backend-store-uri sqlite:///mlflow.db\")\n",
                "print(\"   Then visit: http://localhost:5000\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd2d05cd",
            "metadata": {},
            "source": [
                "### ðŸ” Detailed Breakdown of Imports\n",
                "\n",
                "Let's break down exactly what each imported component does:\n",
                "\n",
                "- **`DirectoryLoader, PyPDFLoader`**: `DirectoryLoader` helps us grab all files in a folder. `PyPDFLoader` is the specialist that knows how to read PDF files page by page.\n",
                "- **`RecursiveCharacterTextSplitter`**: This is a \"smart\" splitter. Instead of just chopping text every 1000 characters (which might cut a sentence in half), it tries to split at natural boundaries like paragraphs `\\n\\n` or sentences `\\n` to check context.\n",
                "- **`OpenAIEmbeddings`**: This tool takes text and turns it into a list of numbers (vectors). It uses OpenAI's models to do this translation.\n",
                "- **`Chroma`**: This is our database. It stores the vectors we created with `OpenAIEmbeddings` so we can search them later.\n",
                "- **`create_history_aware_retriever`**: A special chain that takes your follow-up question (e.g., \"How does it work?\") and your chat history, and rewrites it into a full question (e.g., \"How does RAG work?\") so the database can understand it.\n",
                "- **`create_retrieval_chain`**: The manager that coordinates everything: it gets the question, sends it to the retriever, gets documents back, and sends them to the LLM.\n",
                "- **`create_stuff_documents_chain`**: The worker that actually sends the prompt to the LLM. It \"stuffs\" all the retrieved text into the system prompt.\n",
                "- **`ChatPromptTemplate`**: A flexible template builder. It lets us create prompts with placeholders (like `{context}` or `{input}`) that get filled in dynamically.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e662569",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Load Documents\n",
                "\n",
                "The first step in building a RAG application is loading your documents. We use:\n",
                "\n",
                "- **`glob.glob()`**: To find all PDF files in the `pdfs/` directory and current directory\n",
                "- **`PyPDFLoader`**: To parse each PDF and extract text content\n",
                "\n",
                "### Document Structure\n",
                "\n",
                "Each loaded document contains:\n",
                "- **`page_content`**: The actual text content\n",
                "- **`metadata`**: Information about the document (source file, page number, etc.)\n",
                "\n",
                "> ðŸ“ **Note**: Place your PDF files in a `pdfs/` subdirectory or in the same directory as this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "089ba94c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ“„ Found 9 PDF file(s)\n",
                        "âœ… Loaded 126 pages from 9 file(s)\n"
                    ]
                }
            ],
            "source": [
                "# Find all PDF files in the pdfs/ subdirectory and current directory\n",
                "folders = glob.glob(\"pdfs/*.pdf\") + glob.glob(\"*.pdf\")\n",
                "\n",
                "if not folders:\n",
                "    print(\"âš ï¸ No PDF files found. Please add PDF files to the 'pdfs/' directory or current directory.\")\n",
                "else:\n",
                "    print(f\"ðŸ“„ Found {len(folders)} PDF file(s)\")\n",
                "\n",
                "# Load all documents\n",
                "documents = []\n",
                "for file_path in folders:\n",
                "    loader = PyPDFLoader(file_path)\n",
                "    docs = loader.load()\n",
                "    for doc in docs:\n",
                "        # Add custom metadata to track source file\n",
                "        doc.metadata[\"source_file\"] = os.path.basename(file_path)\n",
                "        documents.append(doc)\n",
                "\n",
                "print(f\"âœ… Loaded {len(documents)} pages from {len(folders)} file(s)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0f8ddf60",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First document metadata:\n",
                        "pdfs/1901.09069v2.pdf\n",
                        "11\n",
                        "1901.09069v2.pdf\n"
                    ]
                }
            ],
            "source": [
                "# print first document metadata, such as file name, source, total number of pages, etc.\n",
                "print(\"First document metadata:\")\n",
                "print(documents[0].metadata['source'])\n",
                "print(documents[0].metadata['total_pages'])\n",
                "print(documents[0].metadata['source_file'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "ab9baf75",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex Â´eoâˆ—\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building ï¬xed-length,\n",
                        "dense and distributed representations for\n",
                        "words, based on the distributional hypoth-\n",
                        "esis. These representations are now com-\n",
                        "monly called word embeddings and, in ad-\n",
                        "dition to encoding surprisingly good syn-\n",
                        "tactic and semantic information, have been\n",
                        "proven useful as extra features in many\n",
                        "downstream NLP tasks.\n",
                        "1 Introduction\n",
                        "The task of representing words and documents is\n",
                        "part and parcel of most, if not all, Natural Lan-\n",
                        "guage Processing (NLP) tasks. In general, it has\n",
                        "been found to be useful to represent them as vec-\n",
                        "tors, which have an appealing, intuitive interpreta-\n",
                        "tion, can be the subject of useful operations (e.g.\n",
                        "addition, subtraction, distance measures, etc) and\n",
                        "lend themselves well to be used in many Machine\n",
                        "Learning (ML) algorithms and strategies.\n",
                        "The Vector Space Model (VSM), generally at-\n",
                        "tributed to Salton (1975) and stemming from the\n",
                        "Information Retrieval (IR) community, is arguably\n",
                        "the most successful and inï¬‚uential model to en-\n",
                        "code words and documents as vectors.\n",
                        "Another very important part of natural\n",
                        "language-based solutions is, of course, the study\n",
                        "of language models. A language model is a statis-\n",
                        "tical model of language usage. It focuses mainly\n",
                        "on predicting the next word given a number of\n",
                        "previous words. This is very useful, for instance,\n",
                        "in speech recognition software, where one needs\n",
                        "to correctly decide what is the word said by the\n",
                        "speaker, even when signal quality is poor or there\n",
                        "is a lot of background noise.\n",
                        "These two seemingly independent ï¬elds have\n",
                        "arguably been brought together by recent research\n",
                        "âˆ—Geraldo Xex Â´eo is also with the Mathematics Institute\n",
                        "(IM-UFRJ), Federal University of Rio de Janeiro\n",
                        "on Neural Network Language Models (NNLMs),\n",
                        "with Bengio et al. (2003)) having developed the\n",
                        "ï¬rst1 large-scale language models based on neural\n",
                        "nets.\n",
                        "Their idea was to reframe the problem as an\n",
                        "unsupervised learning problem. A key feature of\n",
                        "this solution is the way raw words vectors are ï¬rst\n",
                        "projected onto a so-called embedding layer be-\n",
                        "fore being fed into other layers of the network.\n",
                        "Among other reasons, this was imagined to help\n",
                        "ease the effect of the curse of dimensionality on\n",
                        "language models, and help generalization (Bengio\n",
                        "et al. (2003)).\n",
                        "With time, such word embeddings have\n",
                        "emerged as a topic of research in and of them-\n",
                        "selves, with the realization that they can be used\n",
                        "as standalone features in many NLP tasks (Turian\n",
                        "et al. (2010)) and the fact that they encode surpris-\n",
                        "ingly accurate syntactic and semantic word rela-\n",
                        "tionships (Mikolov et al. (2013a)).\n",
                        "More recently2, other ways of creating embed-\n",
                        "dings have surfaced, which rely not on neural net-\n",
                        "works and embedding layers but on leveraging\n",
                        "word-context matrices to arrive at vector repre-\n",
                        "sentations for words. Among the most inï¬‚uential\n",
                        "models we can cite the GloVe model (Pennington\n",
                        "et al. (2014)).\n",
                        "These two types of model have something in\n",
                        "common, namely their reliance on the assump-\n",
                        "tion that words with similar contexts (other words)\n",
                        "have the same meaning. This has been called the\n",
                        "distributional hypothesis, and has been suggested\n",
                        "some time ago by Harris (1954), among others.\n",
                        "This brings us to the deï¬nition of word embed-\n",
                        "dings we will use in this article, as suggested by\n",
                        "the literature (for instance, Turian et al. (2010);\n",
                        "Blacoe and Lapata (2012); Schnabel et al. (2015)),\n",
                        "1They claim this idea has been put forward before (Mi-\n",
                        "ikkulainen and Dyer (1991)), but not used at scale.\n",
                        "2Their roots, however, date back at least two decades, with\n",
                        "the work of Deerwester et al. (1990).\n",
                        "arXiv:1901.09069v2  [cs.CL]  2 May 2023\n"
                    ]
                }
            ],
            "source": [
                "# show the content of first document\n",
                "print(documents[0].page_content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "8d6c8ef7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "3856\n"
                    ]
                }
            ],
            "source": [
                "print(len(documents[0].page_content))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24be028a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Split Documents into Chunks\n",
                "\n",
                "LLMs have a **context window limit** (maximum tokens they can process at once). Additionally, for effective retrieval, we want to find *specific* relevant passages, not entire documents.\n",
                "\n",
                "We use **`RecursiveCharacterTextSplitter`** which:\n",
                "- Splits text hierarchically (paragraphs â†’ sentences â†’ words)\n",
                "- Tries to keep semantically related text together\n",
                "- Creates overlapping chunks to preserve context at boundaries\n",
                "\n",
                "### Key Parameters\n",
                "\n",
                "| Parameter | Value | Description |\n",
                "|-----------|-------|-------------|\n",
                "| `chunk_size` | 1000 | Maximum characters per chunk |\n",
                "| `chunk_overlap` | 200 | Characters shared between adjacent chunks |\n",
                "| `add_start_index` | True | Tracks the position of each chunk in the original document |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c49cc1a5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Split 126 pages into 688 chunks\n",
                        "\n",
                        "ðŸ“ Example Chunk:\n",
                        "--------------------------------------------------\n",
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex Â´eoâˆ—\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building ï¬xed-length,\n",
                        "dense...\n",
                        "--------------------------------------------------\n",
                        "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-03T00:58:57+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-03T00:58:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/1901.09069v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1901.09069v2.pdf', 'start_index': 0}\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the text splitter\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,       # Max characters per chunk\n",
                "    chunk_overlap=200,     # Overlap between chunks for context continuity\n",
                "    add_start_index=True   # Track position in original document\n",
                ")\n",
                "\n",
                "# Split documents into chunks\n",
                "chunks = text_splitter.split_documents(documents)\n",
                "print(f\"âœ… Split {len(documents)} pages into {len(chunks)} chunks\")\n",
                "\n",
                "# Show example chunk\n",
                "if chunks:\n",
                "    print(\"\\nðŸ“ Example Chunk:\")\n",
                "    print(\"-\" * 50)\n",
                "    print(chunks[0].page_content[:300] + \"...\")\n",
                "    print(\"-\" * 50)\n",
                "    print(f\"Metadata: {chunks[0].metadata}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "3dec8c73",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex Â´eoâˆ—\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building ï¬xed-length,\n",
                        "dense and distributed representations for\n",
                        "words, based on the distributional hypoth-\n",
                        "esis. These representations are now com-\n",
                        "monly called word embeddings and, in ad-\n",
                        "dition to encoding surprisingly good syn-\n",
                        "tactic and semantic information, have been\n",
                        "proven useful as extra features in many\n",
                        "downstream NLP tasks.\n",
                        "1 Introduction\n",
                        "The task of representing words and documents is\n",
                        "part and parcel of most, if not all, Natural Lan-\n",
                        "guage Processing (NLP) tasks. In general, it has\n",
                        "been found to be useful to represent them as vec-\n",
                        "tors, which have an appealing, intuitive interpreta-\n",
                        "tion, can be the subject of useful operations (e.g.\n",
                        "addition, subtraction, distance measures, etc) and\n"
                    ]
                }
            ],
            "source": [
                "# inspect a chunk \n",
                "print(chunks[0].page_content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "f2df3c75",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "976\n"
                    ]
                }
            ],
            "source": [
                "print(len(chunks[0].page_content))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7a04768",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 5: Create Embeddings and Vector Store\n",
                "\n",
                "### What are Embeddings?\n",
                "\n",
                "**Embeddings** are numerical representations (vectors) of text that capture semantic meaning. Texts with similar meanings will have vectors that are close together in the embedding space.\n",
                "\n",
                "### What is a Vector Store?\n",
                "\n",
                "A **Vector Store** is a specialized database optimized for:\n",
                "- Storing high-dimensional vectors\n",
                "- Performing fast similarity searches\n",
                "- Enabling \"semantic search\" (finding text by meaning, not just keywords)\n",
                "\n",
                "### Our Setup\n",
                "\n",
                "- **`OpenAIEmbeddings`**: Uses OpenAI's `text-embedding-3-small` model (fast and cost-effective)\n",
                "- **`Chroma`**: Open-source vector database that persists to disk\n",
                "\n",
                "> ðŸ’¡ **Tip**: The embeddings are stored locally, so subsequent runs will be faster as you won't need to re-embed documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "64d4820b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ†• Creating new vector store: vector_db...\n",
                        "âœ… Vector store created with 688 documents\n"
                    ]
                }
            ],
            "source": [
                "# Initialize embedding model\n",
                "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
                "\n",
                "# Clean up existing database if it exists (to ensure fresh data)\n",
                "# NOTE: In production, you would likely load the existing DB mostly.\n",
                "# For this lab, we check if it exists and load it to save time/cost.\n",
                "\n",
                "if os.path.exists(db_name):\n",
                "    # Load existing vector store\n",
                "    vectorstore = Chroma(\n",
                "        persist_directory=db_name, \n",
                "        embedding_function=embeddings\n",
                "    )\n",
                "    print(f\"âœ… Loaded existing vector store: {db_name}\")\n",
                "    try:\n",
                "        count = vectorstore._collection.count()\n",
                "        print(f\"ðŸ“Š Document count: {count}\")\n",
                "    except:\n",
                "        print(\"ðŸ“Š Could not get document count\")\n",
                "else:\n",
                "    # Create new vector store\n",
                "    print(f\"ðŸ†• Creating new vector store: {db_name}...\")\n",
                "    vectorstore = Chroma.from_documents(\n",
                "        documents=chunks,\n",
                "        embedding=embeddings,\n",
                "        persist_directory=db_name\n",
                "    )\n",
                "    print(f\"âœ… Vector store created with {vectorstore._collection.count()} documents\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "7adb3ee0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Document: arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "LÂ´ aszlÂ´ o Kozma and Johannes Voderholzer\n",
                        "Institut fÂ¨ ur Informatik, Freie UniversitÂ¨ at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: ï¬n ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n",
                        "Score: 0.6386286020278931\n",
                        "\n",
                        "Document: encoding (BPE), which lets us learn a vocabulary\n",
                        "that provides a good compression rate of the text.\n",
                        "3.2 Byte Pair Encoding (BPE)\n",
                        "Byte Pair Encoding (BPE) (Gage, 1994) is a sim-\n",
                        "ple data compression technique that iteratively re-\n",
                        "places the most frequent pair of bytes in a se-\n",
                        "quence with a single, unused byte. We adapt this\n",
                        "algorithm for word segmentation. Instead of merg-\n",
                        "ing frequent pairs of bytes, we merge characters or\n",
                        "character sequences.\n",
                        "Firstly, we initialize the symbol vocabulary with\n",
                        "the character vocabulary, and represent each word\n",
                        "as a sequence of characters, plus a special end-of-\n",
                        "word symbol â€˜ Â·â€™, which allows us to restore the\n",
                        "original tokenization after translation. We itera-\n",
                        "tively count all symbol pairs and replace each oc-\n",
                        "currence of the most frequent pair (â€˜Aâ€™, â€˜Bâ€™) with\n",
                        "a new symbol â€˜ABâ€™. Each merge operation pro-\n",
                        "duces a new symbol which represents a charac-\n",
                        "ter n-gram. Frequent character n-grams (or whole\n",
                        "words) are eventually merged into a single sym-\n",
                        "Score: 0.6800229549407959\n",
                        "\n",
                        "Document: input (we precisely deï¬ne this problem â€“ optimal pair encoding â€“ later in this section).\n",
                        "The problem formulation we use closely resembles the one rec ently introduced by Zouhar et\n",
                        "al. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\n",
                        "problem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\n",
                        "that we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\n",
                        "treatment of whitespace and punctuation or language-speci ï¬c rules [RWC+19, AFT+23]).\n",
                        "An algorithm A for optimal pair encoding has approximation ratio Î± â‰¤ 1, if the compression\n",
                        "utility of A is at least Î± times the optimum for all inputs ( s, k). The greedy step of BPE is locally\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "Score: 0.8841046094894409\n",
                        "\n",
                        "Document: to directly optimize for, tokenization is usually solved he uristically, or formulated as a diï¬€erent\n",
                        "but closely related task: data compression. Indeed, the dictionary-encoding of tokens reduces text\n",
                        "length; the amount of compression is easy to measure, and was found to be a good predictor of\n",
                        "the quality of tokenization for downstream tasks, e.g., for translation accuracy [ Gal19]. It is thus\n",
                        "natural to study tokenization with the proxy optimization g oal of compression utility .\n",
                        "Byte-Pair Encoding (BPE), introduced by Gage in 1994 [Gag94], is a commonly used heuristic for\n",
                        "tokenization. It proceeds by repeatedly identifying the most frequently occurring pair of symbols\n",
                        "and replacing all occurrences of this pair with a new symbol, thereby shortening the text. The\n",
                        "new symbols, together with the pairs they replace, are store d in a lookup-table, which allows the\n",
                        "reconstruction of the original text. In typical applicatio ns, the number of new symbols (and thus\n",
                        "Score: 0.9346836805343628\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# test chroma db for similarity search \n",
                "query = \"What is Byte Pair Encoding?\"\n",
                "docs_with_scores = vectorstore.similarity_search_with_score(query)\n",
                "for doc, score in docs_with_scores:\n",
                "    print(f\"Document: {doc.page_content}\\nScore: {score}\\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l2-distance-explanation",
            "metadata": {},
            "source": [
                "### Understanding Chroma Similarity Scores\n",
                "\n",
                "Chroma uses **L2 (Euclidean) distance** for similarity search. The score represents how \"far apart\" two vectors are in the embedding space.\n",
                "\n",
                "| Score Range | Interpretation |\n",
                "|-------------|---------------|\n",
                "| **< 0.5** | Highly relevant - strong semantic match |\n",
                "| **0.5 - 1.0** | Moderately relevant - related content |\n",
                "| **> 1.0** | Potentially irrelevant - consider filtering these out |\n",
                "\n",
                "> ðŸ’¡ **Lower is better** for L2 distance. If you see high scores (>1.5), the retrieved chunks may not actually be relevant to the query.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "228c56ab",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 6: Build the RAG Chain\n",
                "\n",
                "Now we create the complete RAG pipeline using **LangChain Expression Language (LCEL)**. The chain consists of two main components:\n",
                "\n",
                "### 1. History-Aware Retriever\n",
                "\n",
                "This component reformulates the user's question to be **standalone** (understandable without context). \n",
                "\n",
                "**Example:**\n",
                "- Chat history: \"Tell me about SecLM\"\n",
                "- Follow-up: \"What are its main features?\"\n",
                "- Reformulated: \"What are the main features of SecLM?\"\n",
                "\n",
                "### 2. Question-Answer Chain\n",
                "\n",
                "This component:\n",
                "1. Takes the retrieved documents and the question\n",
                "2. \"Stuffs\" the documents into the prompt as context\n",
                "3. Generates a grounded answer using the LLM\n",
                "\n",
                "### The Complete Flow\n",
                "\n",
                "```\n",
                "User Question â†’ Contextualize â†’ Retrieve â†’ Generate Answer\n",
                "      â†‘              â†“            â†“            â†“\n",
                "  Chat History    Standalone    Relevant    Final\n",
                "                   Question     Documents   Response\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "7655086e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize the LLM\n",
                "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "d31270eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Create a retriever from the vector store\n",
                "# k=5 means we retrieve the top 5 most relevant chunks\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "k-tracking-note",
            "metadata": {},
            "source": [
                "> ðŸ“Š **Experiment Tracking Note**: The value of `k` (number of retrieved chunks) will be logged as a parameter in MLflow. This allows you to compare runs with different retrieval depths to see how it affects answer quality.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "32d1a82f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieved 5 documents\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "5"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query = \"What is Byte Pair Encoding?\"\n",
                "\n",
                "# Just check what the retriever returns (raw documents)\n",
                "docs = retriever.invoke(query)\n",
                "print(f\"Retrieved {len(docs)} documents\")\n",
                "len(docs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "b68e83cd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "LÂ´ aszlÂ´ o Kozma and Johannes Voderholzer\n",
                        "Institut fÂ¨ ur Informatik, Freie UniversitÂ¨ at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: ï¬n ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n"
                    ]
                }
            ],
            "source": [
                "print(docs[0].page_content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "bc64f3ca",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "------------------------------------------------------------\n",
                        "=== Document 1 sourced from 2411.08671v1.pdf page 0 ===\n",
                        "=== Content of Document 1 ===\n",
                        "arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "LÂ´ aszlÂ´ o Kozma and Johannes Voderholzer\n",
                        "Institut fÂ¨ ur Informatik, Freie UniversitÂ¨ at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: ï¬n ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n",
                        "------------------------------------------------------------\n",
                        "=== Document 2 sourced from 1508.07909v5.pdf page 2 ===\n",
                        "=== Content of Document 2 ===\n",
                        "encoding (BPE), which lets us learn a vocabulary\n",
                        "that provides a good compression rate of the text.\n",
                        "3.2 Byte Pair Encoding (BPE)\n",
                        "Byte Pair Encoding (BPE) (Gage, 1994) is a sim-\n",
                        "ple data compression technique that iteratively re-\n",
                        "places the most frequent pair of bytes in a se-\n",
                        "quence with a single, unused byte. We adapt this\n",
                        "algorithm for word segmentation. Instead of merg-\n",
                        "ing frequent pairs of bytes, we merge characters or\n",
                        "character sequences.\n",
                        "Firstly, we initialize the symbol vocabulary with\n",
                        "the character vocabulary, and represent each word\n",
                        "as a sequence of characters, plus a special end-of-\n",
                        "word symbol â€˜ Â·â€™, which allows us to restore the\n",
                        "original tokenization after translation. We itera-\n",
                        "tively count all symbol pairs and replace each oc-\n",
                        "currence of the most frequent pair (â€˜Aâ€™, â€˜Bâ€™) with\n",
                        "a new symbol â€˜ABâ€™. Each merge operation pro-\n",
                        "duces a new symbol which represents a charac-\n",
                        "ter n-gram. Frequent character n-grams (or whole\n",
                        "words) are eventually merged into a single sym-\n",
                        "------------------------------------------------------------\n",
                        "=== Document 3 sourced from 2411.08671v1.pdf page 1 ===\n",
                        "=== Content of Document 3 ===\n",
                        "input (we precisely deï¬ne this problem â€“ optimal pair encoding â€“ later in this section).\n",
                        "The problem formulation we use closely resembles the one rec ently introduced by Zouhar et\n",
                        "al. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\n",
                        "problem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\n",
                        "that we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\n",
                        "treatment of whitespace and punctuation or language-speci ï¬c rules [RWC+19, AFT+23]).\n",
                        "An algorithm A for optimal pair encoding has approximation ratio Î± â‰¤ 1, if the compression\n",
                        "utility of A is at least Î± times the optimum for all inputs ( s, k). The greedy step of BPE is locally\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "------------------------------------------------------------\n",
                        "=== Document 4 sourced from 2411.08671v1.pdf page 0 ===\n",
                        "=== Content of Document 4 ===\n",
                        "to directly optimize for, tokenization is usually solved he uristically, or formulated as a diï¬€erent\n",
                        "but closely related task: data compression. Indeed, the dictionary-encoding of tokens reduces text\n",
                        "length; the amount of compression is easy to measure, and was found to be a good predictor of\n",
                        "the quality of tokenization for downstream tasks, e.g., for translation accuracy [ Gal19]. It is thus\n",
                        "natural to study tokenization with the proxy optimization g oal of compression utility .\n",
                        "Byte-Pair Encoding (BPE), introduced by Gage in 1994 [Gag94], is a commonly used heuristic for\n",
                        "tokenization. It proceeds by repeatedly identifying the most frequently occurring pair of symbols\n",
                        "and replacing all occurrences of this pair with a new symbol, thereby shortening the text. The\n",
                        "new symbols, together with the pairs they replace, are store d in a lookup-table, which allows the\n",
                        "reconstruction of the original text. In typical applicatio ns, the number of new symbols (and thus\n",
                        "------------------------------------------------------------\n",
                        "=== Document 5 sourced from 2411.08671v1.pdf page 1 ===\n",
                        "=== Content of Document 5 ===\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "As our main complexity-result, we show that optimal pair encoding is APX-complete. This means\n",
                        "(informally) that no polynomial-time algorithm can approx imate it to a factor arbitrarily close to\n",
                        "1, unless P=NP. On the positive side, we show that BPE achieve s an approximation ratio Î± , with\n",
                        "0.333 < Î± â‰¤ 0.625. We note that previously no constant-approximation gua rantee was known\n",
                        "for BPE or other algorithms. The question of whether optimal pair encoding is NP-complete was\n",
                        "raised recently by Zouhar et al. [ ZMG+23]; our result settles this question in a stronger form.\n",
                        "Before precisely stating our results, we review some furthe r related work and give a formal\n",
                        "deï¬nition of the problem and the algorithms that we study.\n",
                        "Related work. BPE has its origins in text compression, in particular, the c lass of grammar-based\n"
                    ]
                }
            ],
            "source": [
                "for i, doc in enumerate(docs):\n",
                "    print(f'---'*20)\n",
                "    print(f'=== Document {i+1} sourced from {doc.metadata[\"source_file\"]} page {doc.metadata[\"page\"]} ===')\n",
                "    print(f'=== Content of Document {i+1} ===')\n",
                "    print(doc.page_content)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "9bde396d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "------------------------------------------------------------\n",
                        "=== Document 1 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 1 ===\n",
                        "resulting vectors can be used to answer very subtle semantic relationships between words, such as\n",
                        "a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\n",
                        "with such semantic relationships could be used to improve many existing NLP applications, such\n",
                        "as machine translation, information retrieval and question answering systems, and may enable other\n",
                        "future applications yet to be invented.\n",
                        "5\n",
                        "------------------------------------------------------------\n",
                        "=== Document 2 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 2 ===\n",
                        "Somewhat surprisingly, these questions can be answered by performing simple algebraic operations\n",
                        "with the vector representation of words. To ï¬nd a word that is similar to small in the same sense as\n",
                        "biggest is similar to big, we can simply compute vectorX = vector(â€biggestâ€) âˆ’vector(â€bigâ€) +\n",
                        "vector(â€smallâ€). Then, we search in the vector space for the word closest toX measured by cosine\n",
                        "distance, and use it as the answer to the question (we discard the input question words during this\n",
                        "search). When the word vectors are well trained, it is possible to ï¬nd the correct answer (word\n",
                        "smallest) using this method.\n",
                        "Finally, we found that when we train high dimensional word vectors on a large amount of data, the\n",
                        "resulting vectors can be used to answer very subtle semantic relationships between words, such as\n",
                        "a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\n",
                        "------------------------------------------------------------\n",
                        "=== Document 3 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 3 ===\n",
                        "showing example words and their most similar words, and understand them intuitively. Although\n",
                        "it is easy to show that word France is similar to Italy and perhaps some other countries, it is much\n",
                        "more challenging when subjecting those vectors in a more complex similarity task, as follows. We\n",
                        "follow previous observation that there can be many different types of similarities between words, for\n",
                        "example, word big is similar to bigger in the same sense that small is similar to smaller. Example\n",
                        "of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\n",
                        "denote two pairs of words with the same relationship as a question, as we can ask: â€What is the\n",
                        "word that is similar to small in the same sense as biggest is similar to big?â€\n",
                        "Somewhat surprisingly, these questions can be answered by performing simple algebraic operations\n",
                        "with the vector representation of words. To ï¬nd a word that is similar to small in the same sense as\n",
                        "------------------------------------------------------------\n",
                        "=== Document 4 sourced from 1301.3781v3.pdf page 9 ===\n",
                        "=== Content of Document 4 ===\n",
                        "Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\n",
                        "gram model trained on 783M words with 300 dimensionality).\n",
                        "Relationship Example 1 Example 2 Example 3\n",
                        "France - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee\n",
                        "big - bigger small: larger cold: colder quick: quicker\n",
                        "Miami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii\n",
                        "Einstein - scientist Messi: midï¬elder Mozart: violinist Picasso: painter\n",
                        "Sarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan\n",
                        "copper - Cu zinc: Zn gold: Au uranium: plutonium\n",
                        "Berlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack\n",
                        "Microsoft - Windows Google: Android IBM: Linux Apple: iPhone\n",
                        "Microsoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs\n",
                        "Japan - sushi Germany: bratwurst France: tapas USA: pizza\n",
                        "assumes exact match, the results in Table 8 would score only about 60%). We believe that word\n",
                        "------------------------------------------------------------\n",
                        "=== Document 5 sourced from 1508.07909v5.pdf page 8 ===\n",
                        "=== Content of Document 5 ===\n",
                        "Beijing, China.\n",
                        "Rohan Chitnis and John DeNero. 2015. Variable-\n",
                        "Length Word Encodings for Neural Translation\n",
                        "Models. In Proceedings of the 2015 Conference on\n",
                        "Empirical Methods in Natural Language Processing\n",
                        "(EMNLP).\n"
                    ]
                }
            ],
            "source": [
                "query = \"What is the capital of France?\"\n",
                "# This query might not be in the documents, so retrieval might return irrelevant info\n",
                "docs = retriever.invoke(query)\n",
                "for i, doc in enumerate(docs):\n",
                "    print(f'---'*20)\n",
                "    print(f'=== Document {i+1} sourced from {doc.metadata[\"source_file\"]} page {doc.metadata[\"page\"]} ===')\n",
                "    print(f'=== Content of Document {i+1} ===')\n",
                "    print(doc.page_content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "e196646c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "llm_basic = ChatOpenAI(model_name=MODEL, temperature=0)\n",
                "# 3. Define the contextualization prompt\n",
                "# This prompt helps reformulate questions based on chat history\n",
                "contextualize_q_system_prompt = (\n",
                "    \"Given a chat history and the latest user question \"\n",
                "    \"which might reference context in the chat history, \"\n",
                "    \"formulate a standalone question which can be understood \"\n",
                "    \"without the chat history. Do NOT answer the question, \"\n",
                "    \"just reformulate it if needed and otherwise return it as is.\"\n",
                ")\n",
                "\n",
                "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", contextualize_q_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the history-aware retriever\n",
                "history_aware_retriever = create_history_aware_retriever(\n",
                "    llm, retriever, contextualize_q_prompt\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39ee7429",
            "metadata": {},
            "source": [
                "__What contextualize_q_system_prompt does__\n",
                "* This prompt is used only by create_history_aware_retriever.â€‹\n",
                "* It tells the LLM: â€œGiven chat history + latest user input, rewrite the question so itâ€™s standalone; donâ€™t answer it.â€ That rewritten question is then sent to the retriever.â€‹\n",
                "* You need this only if:\n",
                "    * You want followâ€‘up questions like â€œWhat about its limitations?â€ to still retrieve the right chunks, and\n",
                "    * You are using create_history_aware_retriever (or an equivalent â€œconversational retrieverâ€).\n",
                "\n",
                "If you donâ€™t care about multiâ€‘turn context in retrieval, you can skip the historyâ€‘aware retriever entirely and just use retriever = vectorstore.as_retriever(...) as you did with RetrievalQA. In that case, contextualize_q_system_prompt and its prompt are not needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "2d569ed2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define the QA prompt\n",
                "# This prompt instructs the LLM how to use the retrieved context\n",
                "qa_system_prompt = (\n",
                "    \"You are an assistant for question-answering tasks. \"\n",
                "    \"Use the following pieces of retrieved context to answer \"\n",
                "    \"the question. If you don't know the answer, just say that you \"\n",
                "    \"don't know. Use three sentences maximum and keep the \"\n",
                "    \"answer concise.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "qa_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", qa_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the question-answer chain\n",
                "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f9bc762",
            "metadata": {},
            "source": [
                "__What qa_system_prompt does__\n",
                "* This is the system prompt used by the answerâ€‘generation step (create_stuff_documents_chain).â€‹\n",
                "* It controls how the LLM:\n",
                "    * Uses {context} (retrieved docs),\n",
                "    * Handles â€œI donâ€™t knowâ€ cases,\n",
                "    * Constrains length and style of answers\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "cb04d5ce",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… RAG chain created successfully!\n"
                    ]
                }
            ],
            "source": [
                "# 5. Combine into the final RAG chain\n",
                "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
                "\n",
                "print(\"âœ… RAG chain created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt-logging-intro",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“ Logging Prompts for Reproducibility\n",
                "\n",
                "Prompts are **hyperparameters** of your LLM system. Even small changes can dramatically affect results:\n",
                "\n",
                "- Changing \"answer concisely\" to \"answer in detail\" â†’ Different response lengths\n",
                "- Adding \"cite sources\" â†’ Better grounded responses\n",
                "\n",
                "By logging prompts as artifacts:\n",
                "- âœ… You can compare Prompt V1 vs V2 side-by-side in MLflow\n",
                "- âœ… You know exactly what prompt produced which results\n",
                "- âœ… You can roll back to previous prompt versions\n",
                "\n",
                "> ðŸŽ“ **Advanced**: MLflow has a Prompt Registry feature for managing prompt versions at scale.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "prompt-logging-code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ“ Contextualize Prompt:\n",
                        "----------------------------------------\n",
                        "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\n",
                        "\n",
                        "ðŸ“ QA System Prompt:\n",
                        "----------------------------------------\n",
                        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
                        "\n",
                        "{context}\n",
                        "\n",
                        "ðŸ’¡ These prompts will be logged as artifacts during the evaluation run.\n"
                    ]
                }
            ],
            "source": [
                "# Preview the prompts that will be logged during evaluation\n",
                "print(\"ðŸ“ Contextualize Prompt:\")\n",
                "print(\"-\" * 40)\n",
                "print(contextualize_q_system_prompt)\n",
                "print()\n",
                "print(\"ðŸ“ QA System Prompt:\")\n",
                "print(\"-\" * 40)\n",
                "print(qa_system_prompt)\n",
                "print()\n",
                "print(\"ðŸ’¡ These prompts will be logged as artifacts during the evaluation run.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f48e93f6",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 7: Test the RAG Chain\n",
                "\n",
                "Let's test our RAG chain with a simple query. The chain will:\n",
                "\n",
                "1. Take the user's question\n",
                "2. Retrieve relevant document chunks from the vector store\n",
                "3. Generate a response based on the retrieved context\n",
                "\n",
                "The response object contains:\n",
                "- **`answer`**: The generated response\n",
                "- **`context`**: The retrieved document chunks used to generate the answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "5e931695",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â“ Question: What is the main topic of these documents?\n",
                        "\n",
                        "ðŸ’¬ Answer: The main topic of these documents is the development and evaluation of universal text embeddings, which are models designed to perform well across a variety of natural language processing tasks. They discuss the challenges of creating effective embeddings, the importance of diverse and high-quality datasets, and recent advancements in the field. Additionally, they mention benchmarks like the Massive Text Embedding Benchmark (MTEB) that assess the performance of these models across multiple languages and tasks.\n",
                        "\n",
                        "âœ… Chat history updated. You can now ask follow-up questions!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize empty chat history\n",
                "chat_history = []\n",
                "\n",
                "# Ask a question\n",
                "query = \"What is the main topic of these documents?\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
                "\n",
                "print(\"â“ Question:\", query)\n",
                "print(\"\\nðŸ’¬ Answer:\", response[\"answer\"])\n",
                "\n",
                "# Update chat history for follow-up questions\n",
                "chat_history.extend([\n",
                "    HumanMessage(content=query),\n",
                "    AIMessage(content=response[\"answer\"])\n",
                "])\n",
                "\n",
                "print(\"\\nâœ… Chat history updated. You can now ask follow-up questions!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "tracing-visualization-guide",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ” Understanding Traces in MLflow\n",
                "\n",
                "MLflow automatically captured a **trace** of that RAG chain execution. Let's explore it!\n",
                "\n",
                "### What is a Trace?\n",
                "\n",
                "A trace is like an X-ray of your RAG pipeline. It shows:\n",
                "\n",
                "- **Each step**: Retrieval â†’ Prompt construction â†’ LLM call â†’ Response\n",
                "- **Timing**: How long each step took (find bottlenecks!)\n",
                "- **Inputs/Outputs**: What data flowed through each step\n",
                "- **Token counts**: How many tokens were used (costs!)\n",
                "\n",
                "### How to View Traces\n",
                "\n",
                "1. Start the MLflow UI (if not already running):\n",
                "   ```bash\n",
                "   mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5000\n",
                "   ```\n",
                "\n",
                "2. Open browser: `http://localhost:5000`\n",
                "\n",
                "3. Navigate to: **Traces** tab (top menu)\n",
                "\n",
                "4. Click on any trace to see:\n",
                "   - **Timeline view**: Visual representation of execution time\n",
                "   - **Span details**: Click each span to see inputs/outputs\n",
                "   - **Retrieval inspection**: See which documents were retrieved\n",
                "   - **LLM calls**: See the exact prompt sent to the LLM\n",
                "\n",
                "### ðŸŽ¯ Debugging with Traces\n",
                "\n",
                "Traces help you diagnose problems:\n",
                "\n",
                "| Problem | What to Check in Trace |\n",
                "|---------|------------------------|\n",
                "| Wrong answer | **Retrieval span**: Were the right documents retrieved? |\n",
                "| Hallucination | **Context vs Answer**: Does answer contain info NOT in context? |\n",
                "| Slow responses | **Timeline**: Is retrieval slow? LLM call slow? |\n",
                "| High costs | **Token counts**: Are you retrieving too many chunks? |\n",
                "\n",
                "> ðŸ’¡ **Try This**: Run the chain with `k=10` instead of `k=5` and compare the traces. You'll see more retrieval time and higher token usage!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "ad84af45",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"repeat the answer but this time in bullet points please\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "bbc39e1e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â“ Question: repeat the answer but this time in bullet points please\n",
                        "\n",
                        "ðŸ’¬ Answer: - The main topic is the development and evaluation of universal text embeddings for natural language processing tasks.\n",
                        "- It discusses challenges in creating effective embeddings and the importance of diverse, high-quality datasets.\n",
                        "- Recent advancements in the field and benchmarks like the Massive Text Embedding Benchmark (MTEB) are highlighted, assessing model performance across multiple languages and tasks.\n"
                    ]
                }
            ],
            "source": [
                "print(\"â“ Question:\", query)\n",
                "print(\"\\nðŸ’¬ Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "f57a5f40",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â“ Question: What is byte pair encoding?\n",
                        "\n",
                        "ðŸ’¬ Answer: Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. It is adapted for word segmentation by merging characters or character sequences instead of bytes, creating a token dictionary of variable-length subword units. BPE is commonly used in natural language processing tasks, such as machine translation, to improve tokenization and reduce vocabulary size.\n"
                    ]
                }
            ],
            "source": [
                "query = \"What is byte pair encoding?\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
                "print(\"â“ Question:\", query)\n",
                "print(\"\\nðŸ’¬ Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0c9af49",
            "metadata": {},
            "source": [
                "## ðŸ“ Step 3: Create Golden Evaluation Dataset\n",
                "\n",
                "### ðŸ§  Educational Context: The \"Golden Dataset\"\n",
                "\n",
                "To scientifically evaluate a RAG system, we cannot just \"eyeball\" a few answers. We need a **benchmark**â€”often called a \"Golden Dataset\" or \"Ground Truth\" set.\n",
                "\n",
                "#### What makes a good evaluation dataset?\n",
                "1.  **Diversity**: Questions should cover different topics within your documents.\n",
                "2.  **Complexity**: Include simple fact lookups (\"What is X?\") and reasoning questions (\"Compare X and Y\").\n",
                "3.  **Ground Truth**: You must provide the *ideal* answer. The LLM Judge will compare the RAG system's output against this reference.\n",
                "\n",
                "**Measurement Goals**:\n",
                "*   **Retrieval Quality**: Did the system find the right page in the PDF?\n",
                "*   **Generation Quality**: Did the LLM answer accurately based on that page?\n",
                "\n",
                "ðŸ‘‡ **Action**: The code below creates a list of dictionaries, where each item has a `question` and a `ground_truth` answer. We convert this to a pandas DataFrame for easy handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "5fbee472",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Golden dataset ready: 8 evaluation questions\n",
                        "ðŸ“„ Covering: Word Embeddings, BPE, NMT, Vector Models\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>question</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>What is Byte Pair Encoding (BPE)?</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>What complexity result did Kozma and Voderholz...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>What is the distributional hypothesis in NLP?</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>What is the Vector Space Model (VSM)?</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Who introduced the GloVe word embedding model ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                            question\n",
                            "0                  What is Byte Pair Encoding (BPE)?\n",
                            "1  What complexity result did Kozma and Voderholz...\n",
                            "2      What is the distributional hypothesis in NLP?\n",
                            "3              What is the Vector Space Model (VSM)?\n",
                            "4  Who introduced the GloVe word embedding model ..."
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Golden dataset: Questions with ground truth answers\n",
                "# Based on your PDFs: Word Embeddings, BPE, NMT, MTEB\n",
                "eval_data = [\n",
                "    {\n",
                "        \"question\": \"What is Byte Pair Encoding (BPE)?\",\n",
                "        \"ground_truth\": \"BPE is a data compression technique that iteratively replaces the most frequent pair of bytes/symbols with a new symbol. It's used for subword tokenization in NLP tasks like machine translation.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What complexity result did Kozma and Voderholzer prove about optimal pair encoding?\",\n",
                "        \"ground_truth\": \"They proved that optimal pair encoding is APX-complete, meaning it's unlikely to admit a polynomial-time approximation scheme unless P=NP.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the distributional hypothesis in NLP?\",\n",
                "        \"ground_truth\": \"Words that appear in similar contexts tend to have similar meanings. This principle, suggested by Harris (1954), underlies modern word embeddings.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the Vector Space Model (VSM)?\",\n",
                "        \"ground_truth\": \"The VSM represents words and documents as vectors in high-dimensional space, enabling mathematical operations like cosine similarity for information retrieval. Generally attributed to Salton (1975).\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Who introduced the GloVe word embedding model and when?\",\n",
                "        \"ground_truth\": \"GloVe (Global Vectors for Word Representation) was introduced by Pennington et al. in 2014.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the main contribution of Neural Network Language Models (NNLMs)?\",\n",
                "        \"ground_truth\": \"NNLMs, pioneered by Bengio et al. (2003), reframed language modeling as unsupervised learning and introduced embedding layers that project words into dense vector spaces.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What benchmark is used to evaluate text embedding models across multiple languages?\",\n",
                "        \"ground_truth\": \"The Massive Text Embedding Benchmark (MTEB) evaluates embedding models across multiple languages and diverse NLP tasks.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the key advantage of subword tokenization in neural machine translation?\",\n",
                "        \"ground_truth\": \"Subword tokenization (like BPE) enables open-vocabulary translation, handling rare words and achieving better compression while maintaining translation quality.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "eval_df = pd.DataFrame(eval_data)\n",
                "print(f\"âœ… Golden dataset ready: {len(eval_df)} evaluation questions\")\n",
                "print(f\"ðŸ“„ Covering: Word Embeddings, BPE, NMT, Vector Models\")\n",
                "eval_df[[\"question\"]].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "golden-dataset-stats",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ“Š GOLDEN DATASET STATISTICS:\n",
                        "==================================================\n",
                        "Total evaluation questions: 8\n",
                        "Average question length: 61 characters\n",
                        "Average ground truth length: 152 characters\n",
                        "\n",
                        "ðŸ“Œ Topic Coverage:\n",
                        "  - BPE/Tokenization: 2 question(s)\n",
                        "  - Embeddings: 3 question(s)\n",
                        "  - Language Models: 2 question(s)\n",
                        "  - Benchmarks: 1 question(s)\n"
                    ]
                }
            ],
            "source": [
                "# Analyze the golden dataset before proceeding\n",
                "print(\"ðŸ“Š GOLDEN DATASET STATISTICS:\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Total evaluation questions: {len(eval_df)}\")\n",
                "print(f\"Average question length: {eval_df['question'].str.len().mean():.0f} characters\")\n",
                "print(f\"Average ground truth length: {eval_df['ground_truth'].str.len().mean():.0f} characters\")\n",
                "\n",
                "# Show topic distribution (based on keywords)\n",
                "print(\"\\nðŸ“Œ Topic Coverage:\")\n",
                "topics = {\n",
                "    'BPE/Tokenization': eval_df['question'].str.contains('BPE|Byte Pair|tokeniz', case=False).sum(),\n",
                "    'Embeddings': eval_df['question'].str.contains('embed|vector|GloVe|word2vec', case=False).sum(),\n",
                "    'Language Models': eval_df['question'].str.contains('language model|NNLM|neural', case=False).sum(),\n",
                "    'Benchmarks': eval_df['question'].str.contains('benchmark|MTEB|evaluat', case=False).sum(),\n",
                "}\n",
                "for topic, count in topics.items():\n",
                "    print(f\"  - {topic}: {count} question(s)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34cc8b27",
            "metadata": {},
            "source": [
                "## ðŸ” Step 4: Run RAG Inference\n",
                "\n",
                "### ðŸ§  Educational Context: Batch Inference\n",
                "\n",
                "Now that we have our questions, we need to generate answers using our RAG pipeline. This is called **Inference**.\n",
                "\n",
                "#### Why are we doing this loop?\n",
                "We need to capture two things for every question:\n",
                "1.  **The Generated Answer**: What the LLM actually said.\n",
                "2.  **The Retrieved Contexts**: The specific text chunks the system found in your PDF.\n",
                "\n",
                "**Why context matters**: To measure \"Faithfulness\" (hallucination), the Judge needs to see *exactly* what the LLM read before it answered. If the LLM answers correctly but the context was irrelevant, it might be using its pre-trained knowledge instead of your data!\n",
                "\n",
                "ðŸ‘‡ **Action**: The loop below iterates through each question in our golden dataset, sends it to the `rag_chain`, and saves the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "id": "1873c559",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ” Running RAG evaluation inference...\n",
                        "\n",
                        "  âœ“ Q1: What is Byte Pair Encoding (BPE)?...\n",
                        "  âœ“ Q2: What complexity result did Kozma and Voderholzer prove about optimal p...\n",
                        "  âœ“ Q3: What is the distributional hypothesis in NLP?...\n",
                        "  âœ“ Q4: What is the Vector Space Model (VSM)?...\n",
                        "  âœ“ Q5: Who introduced the GloVe word embedding model and when?...\n",
                        "  âœ“ Q6: What is the main contribution of Neural Network Language Models (NNLMs...\n",
                        "  âœ“ Q7: What benchmark is used to evaluate text embedding models across multip...\n",
                        "  âœ“ Q8: What is the key advantage of subword tokenization in neural machine tr...\n",
                        "\n",
                        "âœ… Inference complete: 8/8 questions answered\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>question</th>\n",
                            "      <th>answer</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>What is Byte Pair Encoding (BPE)?</td>\n",
                            "      <td>Byte Pair Encoding (BPE) is a data compression...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>What complexity result did Kozma and Voderholz...</td>\n",
                            "      <td>Kozma and Voderholzer proved that the problem ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>What is the distributional hypothesis in NLP?</td>\n",
                            "      <td>The distributional hypothesis in NLP posits th...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                            question  \\\n",
                            "0                  What is Byte Pair Encoding (BPE)?   \n",
                            "1  What complexity result did Kozma and Voderholz...   \n",
                            "2      What is the distributional hypothesis in NLP?   \n",
                            "\n",
                            "                                              answer  \n",
                            "0  Byte Pair Encoding (BPE) is a data compression...  \n",
                            "1  Kozma and Voderholzer proved that the problem ...  \n",
                            "2  The distributional hypothesis in NLP posits th...  "
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results = []\n",
                "print(\"ðŸ” Running RAG evaluation inference...\\n\")\n",
                "\n",
                "for idx, row in eval_df.iterrows():\n",
                "    try:\n",
                "        # Invoke your existing RAG chain (single-turn evaluation)\n",
                "        response = rag_chain.invoke({\n",
                "            \"input\": row[\"question\"],\n",
                "            \"chat_history\": []  # Empty history for clean evaluation\n",
                "        })\n",
                "        \n",
                "        # Extract answer and retrieved contexts\n",
                "        answer = response[\"answer\"]\n",
                "        contexts = [doc.page_content for doc in response[\"context\"]]\n",
                "        \n",
                "        results.append({\n",
                "            \"question\": row[\"question\"],\n",
                "            \"ground_truth\": row[\"ground_truth\"],\n",
                "            \"answer\": answer,\n",
                "            \"contexts\": contexts  # Required for faithfulness metric\n",
                "        })\n",
                "        \n",
                "        print(f\"  âœ“ Q{idx+1}: {row['question'][:70]}...\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  âœ— Q{idx+1} failed: {e}\")\n",
                "        continue\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(f\"\\nâœ… Inference complete: {len(results_df)}/{len(eval_df)} questions answered\")\n",
                "results_df[[\"question\", \"answer\"]].head(3)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "quality-guardrails",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ›¡ï¸ QUALITY GUARDRAILS CHECK:\n",
                        "==================================================\n",
                        "âœ“ Questions answered: 8/8\n",
                        "âš  Empty answers: 0\n",
                        "âš  Zero retrieved contexts: 0\n",
                        "ðŸ“ Average answer length: 378 characters\n",
                        "\n",
                        "âœ… All checks passed. Ready for LLM Judge evaluation.\n"
                    ]
                }
            ],
            "source": [
                "# Quality checks before running expensive LLM Judge\n",
                "print(\"ðŸ›¡ï¸ QUALITY GUARDRAILS CHECK:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "empty_answers = sum(1 for r in results if not r.get('answer', '').strip())\n",
                "zero_contexts = sum(1 for r in results if not r.get('contexts', []))\n",
                "avg_answer_len = sum(len(r.get('answer', '')) for r in results) / len(results) if results else 0\n",
                "\n",
                "print(f\"âœ“ Questions answered: {len(results)}/{len(eval_df)}\")\n",
                "print(f\"âš  Empty answers: {empty_answers}\")\n",
                "print(f\"âš  Zero retrieved contexts: {zero_contexts}\")\n",
                "print(f\"ðŸ“ Average answer length: {avg_answer_len:.0f} characters\")\n",
                "\n",
                "if empty_answers > 0 or zero_contexts > 0:\n",
                "    print(\"\\nðŸ”´ WARNING: Some questions have issues. Review before running the Judge.\")\n",
                "else:\n",
                "    print(\"\\nâœ… All checks passed. Ready for LLM Judge evaluation.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics-reference",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“ RAG Evaluation Metrics Reference\n",
                "\n",
                "Before we run evaluation, let's understand the metrics we'll use.\n",
                "\n",
                "### How LLM-as-a-Judge Works\n",
                "\n",
                "Traditional metrics like **BLEU** or **ROUGE** compare word overlap. But for conversational AI:\n",
                "- \"The capital of France is Paris\" â‰  \"Paris is the capital city of France\" (different words, same meaning!)\n",
                "\n",
                "**LLM-as-a-Judge** uses a powerful LLM (like GPT-4) to evaluate responses semantically. We give it:\n",
                "- The original question\n",
                "- The retrieved context (from your documents)\n",
                "- The generated answer\n",
                "- The ground truth answer\n",
                "\n",
                "The Judge LLM then applies a rubric to score the response.\n",
                "\n",
                "### Metrics We Use\n",
                "\n",
                "| Metric | Question the Judge Asks | Score Range | What It Measures |\n",
                "|--------|------------------------|-------------|------------------|\n",
                "| **Faithfulness** | \"Is the answer supported *only* by the retrieved context?\" | 1-5 | Anti-hallucination: did the LLM make things up? |\n",
                "| **Answer Relevance** | \"Does the answer actually address the user's question?\" | 1-5 | Is the response on-topic and helpful? |\n",
                "\n",
                "### Interpreting Scores\n",
                "\n",
                "| Score | Interpretation | Action |\n",
                "|-------|----------------|--------|\n",
                "| **5** | Excellent - no issues | âœ… Keep configuration |\n",
                "| **4** | Good - minor issues | ðŸ‘€ Monitor, may need attention |\n",
                "| **3** | Acceptable - noticeable issues | âš ï¸ Investigate specific failures |\n",
                "| **1-2** | Poor - significant problems | ðŸ”´ Debug and fix before production |\n",
                "\n",
                "### Additional Metrics Available\n",
                "\n",
                "MLflow and other frameworks offer more metrics:\n",
                "\n",
                "- **Context Precision**: Did we retrieve the *right* documents?\n",
                "- **Context Recall**: Did we retrieve *all* relevant documents?\n",
                "- **Answer Correctness**: How close is the answer to the ground truth?\n",
                "- **Toxicity**: Is the response harmful or inappropriate?\n",
                "\n",
                "> ðŸ’¡ **Note**: Each Judge LLM call costs money. Start with 2-3 key metrics, then expand as needed.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5ed071b4",
            "metadata": {},
            "source": [
                "## ðŸ“Š Step 5: MLflow LLM-as-a-Judge Evaluation\n",
                "\n",
                "### ðŸ§  Educational Context: LLM-as-a-Judge\n",
                "\n",
                "Evaluating free-text answers is hard. In the past, we used **BLEU** or **ROUGE** scores (checking for exact word overlap), but these are bad for chatbots. A correct answer might use completely different words than the ground truth!\n",
                "\n",
                "**The Solution**: Use a smart LLM (like GPT-4) as a \"Judge\". We give the Judge the evidence (Question, Context, Answer, Ground Truth) and a rubric, and it assigns a score.\n",
                "\n",
                "### Understanding the Metrics\n",
                "\n",
                "We are using MLflow's GenAI metrics:\n",
                "\n",
                "#### 1. Faithfulness (Anti-Hallucination)\n",
                "*   **Question asked to Judge**: \"Is the generated answer based *solely* on the provided context?\"\n",
                "*   **Interpretation**: \n",
                "    *   Score 1.0 (High): The model acted like a faithful storage retrieval system. \n",
                "    *   Score 0.0 (Low): The model made things up (hallucinated) or used outside knowledge.\n",
                "\n",
                "#### 2. Answer Relevance\n",
                "*   **Question asked to Judge**: \"Does this answer actually address the user's question?\"\n",
                "*   **Interpretation**:\n",
                "    *   Score 1.0 (High): The answer is on-topic and helpful.\n",
                "    *   Score 0.0 (Low): The answer is irrelevant, evaded the question, or just rambled.\n",
                "\n",
                "ðŸ‘‡ **Action**: `mlflow.evaluate` runs this entire process automatically. It sends prompts to the Judge model for every row in your dataframe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "c71c8432",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ“ Verifying files before logging:\n",
                        "   âœ… contextualize_prompt.txt (271 bytes)\n",
                        "   âœ… qa_prompt.txt (248 bytes)\n",
                        "   âœ… golden_dataset.csv (1752 bytes)\n",
                        "   âœ… evaluation_results.csv (43373 bytes)\n",
                        "   âœ… Dataset logged via log_input()\n",
                        "   âœ… Tables logged via log_table()\n",
                        "\n",
                        "ðŸ“ Artifact URI: /Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/activities/03_Generative_AI/rag_demo/mlruns/1/093a8d47bc354efb908b0bb3dfd691f8/artifacts\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/48/j6k669vx63qd_68k2_502cl40000gn/T/ipykernel_80214/2719218871.py:3: FutureWarning: ``mlflow.metrics.genai.metric_definitions.faithfulness`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  faithfulness(model=\"openai:/gpt-4o-mini\"),        # Anti-hallucination: answer supported by context?\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/metrics/genai/metric_definitions.py:266: FutureWarning: ``mlflow.metrics.genai.genai_metric.make_genai_metric`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  return make_genai_metric(\n",
                        "/var/folders/48/j6k669vx63qd_68k2_502cl40000gn/T/ipykernel_80214/2719218871.py:4: FutureWarning: ``mlflow.metrics.genai.metric_definitions.answer_relevance`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  answer_relevance(model=\"openai:/gpt-4o-mini\"),    # Does answer address the question?\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/metrics/genai/metric_definitions.py:351: FutureWarning: ``mlflow.metrics.genai.genai_metric.make_genai_metric`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  return make_genai_metric(\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:148: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'golden_evaluation_dataset'. Exception: \n",
                        "  return _dataset_source_registry.resolve(\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:148: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
                        "  return _dataset_source_registry.resolve(\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/deprecated.py:9: FutureWarning: The `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. Please use these new alternatives:\n",
                        "\n",
                        " - For traditional ML or deep learning models: Use `mlflow.models.evaluate`, which maintains full compatibility with the original `mlflow.evaluate` API.\n",
                        "\n",
                        " - For LLMs or GenAI applications: Use the new `mlflow.genai.evaluate` API, which offers enhanced features specifically designed for evaluating LLMs and GenAI applications.\n",
                        "\n",
                        "  warnings.warn(\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:100: FutureWarning: ``mlflow.metrics.token_count`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  token_count(),\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:101: FutureWarning: ``mlflow.metrics.toxicity`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  toxicity(),\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:102: FutureWarning: ``mlflow.metrics.flesch_kincaid_grade_level`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  flesch_kincaid_grade_level(),\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:103: FutureWarning: ``mlflow.metrics.ari_grade_level`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  ari_grade_level(),\n",
                        "/Users/tarekatwan/Repos/MyWork/Teach/repos/advanced_machine_learning/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:109: FutureWarning: ``mlflow.metrics.exact_match`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
                        "  builtin_metrics = [*text_metrics, exact_match()]\n",
                        "2025/12/12 14:19:21 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
                        "2025-12-12 14:19:23 WARNI [evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity] Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
                        "Device set to use mps:0\n",
                        "2025/12/12 14:19:24 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
                        "2025/12/12 14:19:24 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
                        "2025/12/12 14:19:24 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
                        "2025/12/12 14:19:24 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "854361b6da7c4baaa6ec42088152c771",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "411da5a6139e471487b385aaa5b9e931",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/12 14:19:30 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
                        "2025/12/12 14:19:30 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
                        "2025/12/12 14:19:30 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
                        "2025/12/12 14:19:30 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "18de7212b4ec49c7af5e9c8b4f0e4cf6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/8 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "be74899ca7814983951cf0dd4d9d6100",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/8 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ðŸŽ‰ Evaluation complete!\n",
                        "ðŸ†” Run ID: 093a8d47bc354efb908b0bb3dfd691f8\n",
                        "ðŸŒ View in MLflow UI: http://localhost:5000/#/experiments/1/runs/093a8d47bc354efb908b0bb3dfd691f8\n",
                        "\n",
                        "ðŸ“Š AGGREGATE METRICS (1-5 scale, higher is better):\n",
                        "============================================================\n",
                        "  faithfulness/v1/mean.............................. 4.375\n",
                        "  answer_relevance/v1/mean.......................... 5.000\n"
                    ]
                }
            ],
            "source": [
                "# Define evaluation metrics (GPT-4o-mini as judge)\n",
                "metrics = [\n",
                "    faithfulness(model=\"openai:/gpt-4o-mini\"),        # Anti-hallucination: answer supported by context?\n",
                "    answer_relevance(model=\"openai:/gpt-4o-mini\"),    # Does answer address the question?\n",
                "]\n",
                "\n",
                "# Log current configuration as baseline\n",
                "current_config = {\n",
                "    \"chunk_size\": 1000,\n",
                "    \"chunk_overlap\": 200,\n",
                "    \"retrieval_k\": 5,\n",
                "    \"embedding_model\": \"text-embedding-3-small\",\n",
                "    \"llm_model\": MODEL\n",
                "}\n",
                "\n",
                "import os\n",
                "\n",
                "# Run evaluation and log to MLflow\n",
                "with mlflow.start_run(run_name=\"RAG_Baseline_v1\") as run:\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(current_config)\n",
                "    \n",
                "    # Also log prompts as parameters (shows in Parameters tab)\n",
                "    mlflow.log_param(\"contextualize_prompt\", contextualize_q_system_prompt[:250] + \"...\")  # Truncate for param limit\n",
                "    mlflow.log_param(\"qa_prompt\", qa_system_prompt[:250] + \"...\")  # Truncate for param limit\n",
                "    \n",
                "    # Save prompt files to disk\n",
                "    with open(\"contextualize_prompt.txt\", \"w\") as f:\n",
                "        f.write(contextualize_q_system_prompt)\n",
                "    with open(\"qa_prompt.txt\", \"w\") as f:\n",
                "        f.write(qa_system_prompt)\n",
                "    \n",
                "    # Save dataset files to disk\n",
                "    eval_df.to_csv(\"golden_dataset.csv\", index=False)\n",
                "    results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
                "    \n",
                "    # Verify files exist before logging\n",
                "    files_to_log = [\"contextualize_prompt.txt\", \"qa_prompt.txt\", \"golden_dataset.csv\", \"evaluation_results.csv\"]\n",
                "    print(\"ðŸ“ Verifying files before logging:\")\n",
                "    for f in files_to_log:\n",
                "        if os.path.exists(f):\n",
                "            size = os.path.getsize(f)\n",
                "            print(f\"   âœ… {f} ({size} bytes)\")\n",
                "        else:\n",
                "            print(f\"   âŒ {f} NOT FOUND!\")\n",
                "    \n",
                "    # Log all artifacts\n",
                "    for f in files_to_log:\n",
                "        if os.path.exists(f):\n",
                "            mlflow.log_artifact(f)\n",
                "    \n",
                "    # Also log dataset using log_input for Datasets tab\n",
                "    try:\n",
                "        import mlflow.data\n",
                "        dataset = mlflow.data.from_pandas(\n",
                "            results_df,\n",
                "            source=\"golden_evaluation_dataset\",\n",
                "            name=\"rag_eval_questions\"\n",
                "        )\n",
                "        mlflow.log_input(dataset, context=\"evaluation\")\n",
                "        print(\"   âœ… Dataset logged via log_input()\")\n",
                "    except Exception as e:\n",
                "        print(f\"   âš ï¸ log_input() failed: {e}\")\n",
                "    \n",
                "    # Log tables for better UI display (MLflow 2.9+)\n",
                "    try:\n",
                "        mlflow.log_table(data=eval_df, artifact_file=\"golden_dataset.json\")\n",
                "        mlflow.log_table(data=results_df[[\"question\", \"ground_truth\", \"answer\"]], artifact_file=\"results_summary.json\")\n",
                "        print(\"   âœ… Tables logged via log_table()\")\n",
                "    except Exception as e:\n",
                "        print(f\"   âš ï¸ log_table() failed: {e}\")\n",
                "    \n",
                "    # Get artifact URI for debugging\n",
                "    artifact_uri = mlflow.get_artifact_uri()\n",
                "    print(f\"\\nðŸ“ Artifact URI: {artifact_uri}\")\n",
                "    \n",
                "    # Run LLM judge evaluation\n",
                "    eval_results = mlflow.evaluate(\n",
                "        data=results_df,\n",
                "        targets=\"ground_truth\",\n",
                "        predictions=\"answer\",\n",
                "        extra_metrics=metrics,\n",
                "        model_type=\"question-answering\",\n",
                "        evaluator_config={\"col_mapping\": {\"inputs\": \"question\", \"context\": \"contexts\"}}\n",
                "    )\n",
                "    \n",
                "    run_id = run.info.run_id\n",
                "    experiment_id = run.info.experiment_id\n",
                "\n",
                "print(f\"\\nðŸŽ‰ Evaluation complete!\")\n",
                "print(f\"ðŸ†” Run ID: {run_id}\")\n",
                "print(f\"ðŸŒ View in MLflow UI: http://localhost:5000/#/experiments/{experiment_id}/runs/{run_id}\")\n",
                "\n",
                "# Display aggregate metrics\n",
                "print(\"\\nðŸ“Š AGGREGATE METRICS (1-5 scale, higher is better):\")\n",
                "print(\"=\"*60)\n",
                "key_metrics = [\n",
                "    \"faithfulness/v1/mean\",\n",
                "    \"answer_relevance/v1/mean\", \n",
                "]\n",
                "for metric in key_metrics:\n",
                "    if metric in eval_results.metrics:\n",
                "        score = eval_results.metrics[metric]\n",
                "        print(f\"  {metric:.<50} {score:.3f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7da16cfd",
            "metadata": {},
            "source": [
                "## ðŸ”¬ Step 6: Per-Question Breakdown\n",
                "\n",
                "### ðŸ§  Educational Context: Debugging\n",
                "\n",
                "Averages hide details. To improve your system, you must look at **individual failures**.\n",
                "\n",
                "#### How to interpret this table:\n",
                "1.  **Low Faithfulness, High Relevance**: The model gave a good-sounding answer, but it wasn't in the document! This is dangerous (Hallucination).\n",
                "    *   *Fix*: Check if the retrieval step failed to find the right chunk.\n",
                "2.  **High Faithfulness, Low Relevance**: The model quoted the document perfectly, but it didn't answer the user's question.\n",
                "    *   *Fix*: The retrieved chunk might be irrelevant to the question.\n",
                "\n",
                "ðŸ‘‡ **Action**: We highlight scores below 0.7 in red. Focus on these rows to understand *why* the system failed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "da78262b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cc89271042914d43aabeb4f22a9d1ce5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4981c4d4784b4e9dbf4201f321c52efd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ“‹ PER-QUESTION PERFORMANCE:\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "</style>\n",
                            "<table id=\"T_5fc5f\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_5fc5f_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
                            "      <th id=\"T_5fc5f_level0_col1\" class=\"col_heading level0 col1\" >Faithfulness</th>\n",
                            "      <th id=\"T_5fc5f_level0_col2\" class=\"col_heading level0 col2\" >Relevance</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_5fc5f_row0_col0\" class=\"data row0 col0\" >What is Byte Pair Encoding (BPE)?</td>\n",
                            "      <td id=\"T_5fc5f_row0_col1\" class=\"data row0 col1\" >5</td>\n",
                            "      <td id=\"T_5fc5f_row0_col2\" class=\"data row0 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_5fc5f_row1_col0\" class=\"data row1 col0\" >What complexity result did Kozma and Voderholzer prove about optimal pair encoding?</td>\n",
                            "      <td id=\"T_5fc5f_row1_col1\" class=\"data row1 col1\" >4</td>\n",
                            "      <td id=\"T_5fc5f_row1_col2\" class=\"data row1 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_5fc5f_row2_col0\" class=\"data row2 col0\" >What is the distributional hypothesis in NLP?</td>\n",
                            "      <td id=\"T_5fc5f_row2_col1\" class=\"data row2 col1\" >4</td>\n",
                            "      <td id=\"T_5fc5f_row2_col2\" class=\"data row2 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
                            "      <td id=\"T_5fc5f_row3_col0\" class=\"data row3 col0\" >What is the Vector Space Model (VSM)?</td>\n",
                            "      <td id=\"T_5fc5f_row3_col1\" class=\"data row3 col1\" >4</td>\n",
                            "      <td id=\"T_5fc5f_row3_col2\" class=\"data row3 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
                            "      <td id=\"T_5fc5f_row4_col0\" class=\"data row4 col0\" >Who introduced the GloVe word embedding model and when?</td>\n",
                            "      <td id=\"T_5fc5f_row4_col1\" class=\"data row4 col1\" >5</td>\n",
                            "      <td id=\"T_5fc5f_row4_col2\" class=\"data row4 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
                            "      <td id=\"T_5fc5f_row5_col0\" class=\"data row5 col0\" >What is the main contribution of Neural Network Language Models (NNLMs)?</td>\n",
                            "      <td id=\"T_5fc5f_row5_col1\" class=\"data row5 col1\" >4</td>\n",
                            "      <td id=\"T_5fc5f_row5_col2\" class=\"data row5 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
                            "      <td id=\"T_5fc5f_row6_col0\" class=\"data row6 col0\" >What benchmark is used to evaluate text embedding models across multiple languages?</td>\n",
                            "      <td id=\"T_5fc5f_row6_col1\" class=\"data row6 col1\" >4</td>\n",
                            "      <td id=\"T_5fc5f_row6_col2\" class=\"data row6 col2\" >5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_5fc5f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
                            "      <td id=\"T_5fc5f_row7_col0\" class=\"data row7 col0\" >What is the key advantage of subword tokenization in neural machine translation?</td>\n",
                            "      <td id=\"T_5fc5f_row7_col1\" class=\"data row7 col1\" >5</td>\n",
                            "      <td id=\"T_5fc5f_row7_col2\" class=\"data row7 col2\" >5</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x351cb67d0>"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Extract per-question scores\n",
                "eval_table = eval_results.tables[\"eval_results_table\"]\n",
                "\n",
                "# Display detailed breakdown\n",
                "print(\"ðŸ“‹ PER-QUESTION PERFORMANCE:\\n\")\n",
                "display_cols = [\n",
                "    \"question\",\n",
                "    \"faithfulness/v1/score\",\n",
                "    \"answer_relevance/v1/score\",\n",
                "    # \"context_precision/v1/score\", \n",
                "    # \"context_recall/v1/score\"\n",
                "]\n",
                "\n",
                "breakdown = eval_table[display_cols].round(3)\n",
                "breakdown.columns = [\"Question\", \"Faithfulness\", \"Relevance\"]\n",
                "\n",
                "# Highlight low-scoring questions (< 0.7)\n",
                "# Note: style for dataframe\n",
                "styled = breakdown.style.map(\n",
                "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x < 3.5 else '',\n",
                "    subset=[\"Faithfulness\", \"Relevance\"]\n",
                ")\n",
                "\n",
                "styled\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "failure-patterns",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ” Common RAG Failure Patterns & Fixes\n",
                "\n",
                "Use this reference when analyzing your per-question scores.\n",
                "\n",
                "### Pattern 1: Low Faithfulness + High Relevance\n",
                "\n",
                "**Symptom**: The answer sounds great and addresses the question, but it's not actually in the documents.\n",
                "\n",
                "**Diagnosis**: The LLM is **hallucinating** - using its pretrained knowledge instead of your documents.\n",
                "\n",
                "**Fixes**:\n",
                "- Make your system prompt stricter: \"Answer ONLY based on the provided context\"\n",
                "- Lower the LLM temperature to reduce creativity\n",
                "- Check if retrieval is returning irrelevant chunks (forcing the LLM to guess)\n",
                "\n",
                "---\n",
                "\n",
                "### Pattern 2: High Faithfulness + Low Relevance\n",
                "\n",
                "**Symptom**: The answer quotes the document perfectly but doesn't answer the question.\n",
                "\n",
                "**Diagnosis**: **Retrieval failure** - the wrong chunks were fetched.\n",
                "\n",
                "**Fixes**:\n",
                "- Increase `k` (number of retrieved chunks) to get more options\n",
                "- Improve chunk overlap to preserve context boundaries\n",
                "- Try a different embedding model (some are better for your domain)\n",
                "- Add metadata filtering (e.g., only search specific document types)\n",
                "\n",
                "---\n",
                "\n",
                "### Pattern 3: Low Faithfulness + Low Relevance\n",
                "\n",
                "**Symptom**: The answer is both wrong and off-topic.\n",
                "\n",
                "**Diagnosis**: Complete system failure - likely the question is **out of scope**.\n",
                "\n",
                "**Fixes**:\n",
                "- Add a fallback: \"I don't have information about that topic\"\n",
                "- Check if your documents even contain relevant information\n",
                "- Review your embedding model - it may not understand the domain\n",
                "\n",
                "---\n",
                "\n",
                "### Pattern 4: Inconsistent Scores Across Similar Questions\n",
                "\n",
                "**Symptom**: \"What is X?\" scores 5, but \"Explain X\" scores 2.\n",
                "\n",
                "**Diagnosis**: Your chunking or retrieval is **sensitive to phrasing**.\n",
                "\n",
                "**Fixes**:\n",
                "- Use a history-aware retriever (like we have) to normalize queries\n",
                "- Add query expansion or rewriting before retrieval\n",
                "- Consider hybrid search (combine semantic + keyword matching)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "failure-analysis-code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ” FAILURE ANALYSIS:\n",
                        "==================================================\n",
                        "âœ… No low-faithfulness questions found!\n"
                    ]
                }
            ],
            "source": [
                "# Identify and analyze low-scoring questions\n",
                "THRESHOLD = 3.5  # Scores below this are concerning\n",
                "\n",
                "print(\"ðŸ” FAILURE ANALYSIS:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Find questions with low faithfulness\n",
                "low_faith = eval_table[eval_table['faithfulness/v1/score'] < THRESHOLD]\n",
                "\n",
                "if len(low_faith) > 0:\n",
                "    print(f\"\\nâš  Found {len(low_faith)} question(s) with Faithfulness < {THRESHOLD}:\")\n",
                "    for idx, row in low_faith.iterrows():\n",
                "        print(f\"\\n  Q: {row['question'][:70]}...\")\n",
                "        print(f\"  Score: {row['faithfulness/v1/score']}\")\n",
                "        \n",
                "        # Show what was retrieved (from results_df)\n",
                "        matching_result = results_df[results_df['question'] == row['question']]\n",
                "        if len(matching_result) > 0 and 'contexts' in matching_result.columns:\n",
                "            contexts = matching_result.iloc[0]['contexts']\n",
                "            print(f\"  Retrieved {len(contexts)} chunks. First chunk preview:\")\n",
                "            if contexts:\n",
                "                print(f\"    '{contexts[0][:150]}...'\")\n",
                "else:\n",
                "    print(\"âœ… No low-faithfulness questions found!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f0dc2819",
            "metadata": {},
            "source": [
                "## ðŸ§ª Step 7: Experiment Comparison & Iteration\n",
                "\n",
                "### ðŸ§  Educational Context: The Scientific Method for RAG\n",
                "\n",
                "RAG is not \"set and forget\". It requires tuning. This is the **Experiment Loop**:\n",
                "\n",
                "1.  **Baseline**: Run the system with default settings (e.g., chunk_size=1000).\n",
                "2.  **Hypothesis**: \"I think smaller chunks will capture details better.\"\n",
                "3.  **Experiment**: Change `chunk_size` to 500.\n",
                "4.  **Evaluate**: Rerun this notebook.\n",
                "5.  **Compare**: Look at the table below. Did Faithfulness go up?\n",
                "\n",
                "#### Parameters you can tune:\n",
                "*   **Chunk Size**: 500, 1000, 2000 characters.\n",
                "*   **Overlap**: 10%, 20% of chunk size.\n",
                "*   **k (Retrieval Count)**: Provide 3, 5, or 10 chunks to the LLM.\n",
                "\n",
                "ðŸ‘‡ **Action**: This table aggregates all your MLflow runs so you can pick the best configuration."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-interpretation",
            "metadata": {},
            "source": [
                "### How to Interpret This Table\n",
                "\n",
                "**Finding the Best Configuration:**\n",
                "\n",
                "1. **Primary Metric**: Look for highest `faithfulness/v1/mean` (prevents hallucination)\n",
                "2. **Secondary Metric**: Among high-faithfulness runs, pick highest `answer_relevance/v1/mean`\n",
                "3. **Tradeoffs**: Smaller chunks may increase faithfulness but could reduce relevance\n",
                "\n",
                "**Reading the Parameters:**\n",
                "- `chunk_size`: Larger = more context per chunk, but may include noise\n",
                "- `chunk_overlap`: Higher = better context continuity at boundaries\n",
                "- `retrieval_k`: More chunks = better recall, but may confuse the LLM\n",
                "\n",
                "> ðŸ’¡ **Pro Tip**: If two runs have similar scores, prefer the one with lower API costs (smaller chunks = more embeddings, higher k = more tokens in prompt).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "e11617f9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/12 14:19:36 WARNING mlflow.tracking.fluent: Cannot retrieve experiment by name RAG_PDF_Embeddings_Evaluation\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ”¬ EXPERIMENT COMPARISON:\n",
                        "====================================================================================================\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>run_id</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "Empty DataFrame\n",
                            "Columns: [run_id]\n",
                            "Index: []"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# After you modify chunk_size, k, or embeddings â†’ rerun Steps 4-6\n",
                "# This cell shows all runs side-by-side\n",
                "\n",
                "runs_df = mlflow.search_runs(\n",
                "    experiment_names=[\"RAG_PDF_Embeddings_Evaluation\"],\n",
                "    order_by=[\"start_time DESC\"]\n",
                ")\n",
                "\n",
                "# Select key columns for comparison\n",
                "comparison_cols = [\n",
                "    \"run_id\",\n",
                "    \"params.chunk_size\",\n",
                "    \"params.chunk_overlap\", \n",
                "    \"params.retrieval_k\",\n",
                "    \"params.embedding_model\",\n",
                "    \"metrics.faithfulness/v1/mean\",\n",
                "    \"metrics.answer_relevance/v1/mean\",\n",
                "    \"metrics.context_precision/v1/mean\",\n",
                "    \"metrics.context_recall/v1/mean\"\n",
                "]\n",
                "\n",
                "# Ensure cols exist\n",
                "available_cols = [c for c in comparison_cols if c in runs_df.columns]\n",
                "comparison = runs_df[available_cols].round(3)\n",
                "\n",
                "print(\"ðŸ”¬ EXPERIMENT COMPARISON:\")\n",
                "print(\"=\"*100)\n",
                "comparison.head(10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "student-challenge",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "Now it's your turn to experiment!\n",
                "\n",
                "### Challenge 1: Tune the Chunking Strategy\n",
                "\n",
                "**Hypothesis**: Smaller chunks might capture specific details better.\n",
                "\n",
                "**Task**:\n",
                "1. Go back to **Step 4** and change `chunk_size` from 1000 to 500\n",
                "2. Delete the `vector_db` folder to force re-indexing\n",
                "3. Re-run Steps 4, 5, and 6 to rebuild the vector store\n",
                "4. Re-run Steps 3-7 (Evaluation) with a new run name (e.g., \"RAG_SmallChunks_v1\")\n",
                "5. Compare results in the experiment comparison table\n",
                "\n",
                "**Questions to Answer**:\n",
                "- Did faithfulness improve or get worse?\n",
                "- What about answer relevance?\n",
                "- Why do you think you saw these changes?\n",
                "\n",
                "---\n",
                "\n",
                "### Challenge 2: Expand the Golden Dataset\n",
                "\n",
                "**Task**: Add 3 new questions to the evaluation dataset:\n",
                "\n",
                "1. One **factual question** (\"When was X published?\")\n",
                "2. One **reasoning question** (\"Compare X and Y\")\n",
                "3. One **out-of-scope question** (something NOT in your documents)\n",
                "\n",
                "**Questions to Answer**:\n",
                "- How did the system handle the out-of-scope question?\n",
                "- Did it admit it didn't know, or did it hallucinate?\n",
                "\n",
                "---\n",
                "\n",
                "### Challenge 3 (Bonus): Try a Different Embedding Model\n",
                "\n",
                "**Task**:\n",
                "1. Change from `text-embedding-3-small` to `text-embedding-3-large`\n",
                "2. Delete the `vector_db` folder\n",
                "3. Re-run the entire pipeline\n",
                "4. Compare the cost vs. quality tradeoff\n",
                "\n",
                "---\n",
                "\n",
                "### Reflection Questions\n",
                "\n",
                "After completing the challenges, answer these:\n",
                "\n",
                "1. Which parameter had the biggest impact on quality?\n",
                "2. What tradeoffs did you observe (quality vs. cost vs. speed)?\n",
                "3. How would you decide on the best configuration for production?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "next-steps",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“š Next Steps & Resources\n",
                "\n",
                "### Further Reading\n",
                "\n",
                "| Resource | Description | Link |\n",
                "|----------|-------------|------|\n",
                "| **MLflow LLM Evaluation Docs** | Complete guide to GenAI metrics | [mlflow.org/docs](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) |\n",
                "| **RAGAS Framework** | Alternative evaluation framework with more metrics | [ragas.io](https://docs.ragas.io/) |\n",
                "| **LangSmith** | LangChain's own observability platform | [docs.smith.langchain.com](https://docs.smith.langchain.com/) |\n",
                "| **Arize Phoenix** | Open-source LLM observability | [phoenix.arize.com](https://phoenix.arize.com/) |\n",
                "\n",
                "### Research Papers\n",
                "\n",
                "- **\"RAGAS: Automated Evaluation of Retrieval Augmented Generation\"** - Original paper on RAG metrics\n",
                "- **\"Judging LLM-as-a-Judge\"** - Meta-analysis of using LLMs for evaluation\n",
                "\n",
                "### What's Next in Your Learning Path?\n",
                "\n",
                "1. **Production Deployment**: Learn to deploy RAG with proper monitoring\n",
                "2. **Advanced Retrieval**: Explore hybrid search, re-ranking, and multi-index strategies\n",
                "3. **Agentic RAG**: Combine RAG with tool use for more complex tasks\n",
                "4. **Fine-tuning**: Train custom embedding models for your domain\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ Summary\n",
                "\n",
                "In this notebook, you learned:\n",
                "\n",
                "- âœ… RAG systems need **systematic evaluation**, not just manual testing\n",
                "- âœ… **Golden Datasets** provide the ground truth for benchmarking\n",
                "- âœ… **LLM-as-a-Judge** enables semantic evaluation at scale\n",
                "- âœ… **MLflow** provides observability and experiment tracking\n",
                "- âœ… **Faithfulness** measures hallucination, **Relevance** measures answer quality\n",
                "- âœ… Per-question analysis helps you **debug specific failures**\n",
                "- âœ… Experiment comparison helps you **iterate on configurations**\n",
                "\n",
                "**Remember**: A RAG system is only as good as your ability to measure and improve it!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0d20606",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "c6e909aa",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "advanced-machine-learning",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
