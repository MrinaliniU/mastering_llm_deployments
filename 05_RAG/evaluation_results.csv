question,ground_truth,answer,contexts
What is Byte Pair Encoding (BPE)?,BPE is a data compression technique that iteratively replaces the most frequent pair of bytes/symbols with a new symbol. It's used for subword tokenization in NLP tasks like machine translation.,"Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. It is adapted for word segmentation by merging characters or character sequences instead of bytes. BPE is widely used in language processing tasks, such as machine translation and large language model pretraining, to create a token dictionary of a specified size.","['encoding (BPE), which lets us learn a vocabulary\nthat provides a good compression rate of the text.\n3.2 Byte Pair Encoding (BPE)\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-\nple data compression technique that iteratively re-\nplaces the most frequent pair of bytes in a se-\nquence with a single, unused byte. We adapt this\nalgorithm for word segmentation. Instead of merg-\ning frequent pairs of bytes, we merge characters or\ncharacter sequences.\nFirstly, we initialize the symbol vocabulary with\nthe character vocabulary, and represent each word\nas a sequence of characters, plus a special end-of-\nword symbol ‘ ·’, which allows us to restore the\noriginal tokenization after translation. We itera-\ntively count all symbol pairs and replace each oc-\ncurrence of the most frequent pair (‘A’, ‘B’) with\na new symbol ‘AB’. Each merge operation pro-\nduces a new symbol which represents a charac-\nter n-gram. Frequent character n-grams (or whole\nwords) are eventually merged into a single sym-', 'arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\nTheoretical Analysis of Byte-Pair Encoding\nL´ aszl´ o Kozma and Johannes Voderholzer\nInstitut f¨ ur Informatik, Freie Universit¨ at Berlin, Germany\nAbstract\nByte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\ngrammar-based text compression. It is employed in a variety of lang uage processing tasks such\nas machine translation or large language model (LLM) pretraining, t o create a token dictionary\nof a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\npractical performance are not well understood.\nIn this paper we focus on the optimization problem underlying BPE: ﬁn ding a pair encoding\nthat achieves optimal compression utility. We show that this problem is APX-complete, indi-\ncating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\nstronger form, a question recently raised by Zouhar et al. [ ZMG+23].', 'input (we precisely deﬁne this problem – optimal pair encoding – later in this section).\nThe problem formulation we use closely resembles the one rec ently introduced by Zouhar et\nal. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\nproblem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\nthat we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\ntreatment of whitespace and punctuation or language-speci ﬁc rules [RWC+19, AFT+23]).\nAn algorithm A for optimal pair encoding has approximation ratio α ≤ 1, if the compression\nutility of A is at least α times the optimum for all inputs ( s, k). The greedy step of BPE is locally\noptimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\nshow that BPE may not achieve optimal compression (see Figur e 1).', 'optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\nshow that BPE may not achieve optimal compression (see Figur e 1).\nAs our main complexity-result, we show that optimal pair encoding is APX-complete. This means\n(informally) that no polynomial-time algorithm can approx imate it to a factor arbitrarily close to\n1, unless P=NP. On the positive side, we show that BPE achieve s an approximation ratio α , with\n0.333 < α ≤ 0.625. We note that previously no constant-approximation gua rantee was known\nfor BPE or other algorithms. The question of whether optimal pair encoding is NP-complete was\nraised recently by Zouhar et al. [ ZMG+23]; our result settles this question in a stronger form.\nBefore precisely stating our results, we review some furthe r related work and give a formal\ndeﬁnition of the problem and the algorithms that we study.\nRelated work. BPE has its origins in text compression, in particular, the c lass of grammar-based', 'deﬁnition of the problem and the algorithms that we study.\nRelated work. BPE has its origins in text compression, in particular, the c lass of grammar-based\ncompression methods or macro schemes , e.g., see [\nSS82, KY00, CLL+05, Loh12] for surveys. (The\nencoding obtained by BPE can be seen as a restricted kind of co ntext-free grammar or string\nstraight-line program.) A compression method closely rela ted to BPE is Re-Pair [ LM00]. Re-Pair\ndiﬀers from BPE in that its number of replacement rounds k is not ﬁxed; instead, it performs\n1See also https://github.com/google/sentencepiece and\nhttps://github.com/openai/tiktoken.\n2']"
What complexity result did Kozma and Voderholzer prove about optimal pair encoding?,"They proved that optimal pair encoding is APX-complete, meaning it's unlikely to admit a polynomial-time approximation scheme unless P=NP.","Kozma and Voderholzer proved that the problem of optimal pair encoding is APX-complete, indicating that it is unlikely to admit a polynomial-time approximation scheme. They also showed that Byte-Pair Encoding (BPE) achieves a constant-factor approximation ratio between 0.333 and 0.625 for this task. This result answers a question raised by Zouhar et al. in a stronger form.","['optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\nshow that BPE may not achieve optimal compression (see Figur e 1).\nAs our main complexity-result, we show that optimal pair encoding is APX-complete. This means\n(informally) that no polynomial-time algorithm can approx imate it to a factor arbitrarily close to\n1, unless P=NP. On the positive side, we show that BPE achieve s an approximation ratio α , with\n0.333 < α ≤ 0.625. We note that previously no constant-approximation gua rantee was known\nfor BPE or other algorithms. The question of whether optimal pair encoding is NP-complete was\nraised recently by Zouhar et al. [ ZMG+23]; our result settles this question in a stronger form.\nBefore precisely stating our results, we review some furthe r related work and give a formal\ndeﬁnition of the problem and the algorithms that we study.\nRelated work. BPE has its origins in text compression, in particular, the c lass of grammar-based', 'whose removal makes the remainder non-overlapping (by a sim ple greedy strategy). In the worst\ncase, however, no algorithm in this class can improve the app roximation ratio 0.5.\nIndeed, consider an input string s consisting of 2n identical symbols, followed by 2(k−1) pairwise\ndistinct symbols. Setting k = log2 n + 1, the optimal pair encoding of s will collapse the ﬁrst 2 n\nsymbols to one, with utility 2 n − 1. An algorithm that can only merge input pairs can achieve a\nmaximum utility of n on the ﬁrst part of the string (in one merge), and an additiona l k − 1 on the\nlast part, yielding an approximation ratio of (n +log2 n)/(2n −1) = 0.5+ o(1). In this example, the\noptimal encoding coincides with the one found by BPE, also sh owing a factor 2 − o(1) gap between\nBPE and input-symbol-only algorithms, to the advantage of t he former.\n6. Conclusion and open questions\nIn this paper we studied the complexity of optimal pair encoding: the task of compressing a string by', 'arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\nTheoretical Analysis of Byte-Pair Encoding\nL´ aszl´ o Kozma and Johannes Voderholzer\nInstitut f¨ ur Informatik, Freie Universit¨ at Berlin, Germany\nAbstract\nByte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\ngrammar-based text compression. It is employed in a variety of lang uage processing tasks such\nas machine translation or large language model (LLM) pretraining, t o create a token dictionary\nof a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\npractical performance are not well understood.\nIn this paper we focus on the optimization problem underlying BPE: ﬁn ding a pair encoding\nthat achieves optimal compression utility. We show that this problem is APX-complete, indi-\ncating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\nstronger form, a question recently raised by Zouhar et al. [ ZMG+23].', '6. Conclusion and open questions\nIn this paper we studied the complexity of optimal pair encoding: the task of compressing a string by\nreplacing pairs with new symbols, such as to maximize the ove rall reduction in length. We showed\nthat the problem is APX-complete and that BPE, a popular gree dy heuristic, achieves a constant-\nfactor approximation for this task. Our work can be seen as an initial theoretical investigation with\na number of open questions remaining. We list those we ﬁnd the most interesting.\nFinding the best approximation ratio for the OPE or OMS probl ems, by BPE or by other\npolynomial-time algorithms, i.e., closing the gaps betwee n our bounds in Theorem\n1.2, resp., The-\norems 1.1 and 1.4 are the central remaining questions. In particular: is ther e an eﬃcient algorithm\nfor OPE with approximation ratio above 0 .5? Note that analyzing natural greedy strategies for\nsome other string problems turned out to be very diﬃcult. For instance, the famous greedy super-', 'input (we precisely deﬁne this problem – optimal pair encoding – later in this section).\nThe problem formulation we use closely resembles the one rec ently introduced by Zouhar et\nal. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\nproblem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\nthat we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\ntreatment of whitespace and punctuation or language-speci ﬁc rules [RWC+19, AFT+23]).\nAn algorithm A for optimal pair encoding has approximation ratio α ≤ 1, if the compression\nutility of A is at least α times the optimum for all inputs ( s, k). The greedy step of BPE is locally\noptimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\nshow that BPE may not achieve optimal compression (see Figur e 1).']"
What is the distributional hypothesis in NLP?,"Words that appear in similar contexts tend to have similar meanings. This principle, suggested by Harris (1954), underlies modern word embeddings.","The distributional hypothesis in NLP posits that words that occur in similar contexts tend to have similar meanings. This idea underlies the development of word embeddings, where dense, distributed representations of words are created based on their co-occurrence statistics in large text corpora. Essentially, it suggests that the meaning of a word can be inferred from the patterns of its usage in language.","['principle for unnormalized statistical models.\nZellig S. Harris, 1954. Distributional struc-\nture. Available at http://dx.doi.org/\n10.1080/00437956.1954.11659520.\nGeoffrey E. Hinton, August 2002. Training prod-\nucts of experts by minimizing contrastive diver-\ngence. Available at http://dx.doi.org/\n10.1162/089976602760128018.\nHarold Hotelling, 1935. Canonical correlation\nanalysis (cca).\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nand Tomas Mikolov, 2016. Bag of tricks for\nefﬁcient text classiﬁcation. Available at http:\n//arxiv.org/abs/1607.01759.\nSlava M. Katz. 1987. Estimation of probabilities\nfrom sparse data for the language model com-\nponent of a speech recognizer. In IEEE Trans-\nactions on Acoustics, Speech and Signal Pro-\ncessing.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Antonio Torralba, Raquel\nUrtasun, and Sanja Fidler, 2015. Skip-thought\nvectors. Available at http://arxiv.org/\nabs/1506.06726.\nIgor Labutov and Hod Lipson, 2013. Re-\nembedding words.', 'according to which word embeddings are dense,\ndistributed, ﬁxed-length word vectors, built us-\ning word co-occurrence statistics as per the dis-\ntributional hypothesis.\nEmbedding models derived from neural net-\nwork language models have (Baroni et al. (2014))\nbeen called prediction-based models, since they\nusually leverage language models, which predict\nthe next word given its context. Other matrix-\nbased models have been called count-based mod-\nels, due to their taking into account global word-\ncontext co-occurrence counts to derive word em-\nbeddings. 3 These are described next.\nThis survey is structured as follows: in sec-\ntion 2 we describe the origins of statistical lan-\nguage modelling. In section 3 we give an\noverview of word embeddings, generated both by\nso-called prediction-based models and by count-\nbased methods. In Section 4 we conclude and in\nSection 5 we provide some pointers to promising\nfurther research topics.\n1.1 Motivation', 'In this section, we propose two new model architectures for learning distributed representations\nof words that try to minimize computational complexity. The main observation from the previous\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\nmore data efﬁciently.\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\ndistributed representations of words. While there has been later substantial amount of work that', 'yond about 10. Presumably this is because the\nnegative sampling method does not approximate\nthe target probability distribution well.9\nFor the same corpus, vocabulary, window size,\nand training time, GloVe consistently outperforms\nword2vec. It achieves better results faster, and\nalso obtains the best results irrespective of speed.\n5 Conclusion\nRecently, considerable attention has been focused\non the question of whether distributional word\nrepresentations are best learned from count-based\n9In contrast, noise-contrastive estimation is an approxi-\nmation which improves with more negative samples. In Ta-\nble 1 of (Mnih et al., 2013), accuracy on the analogy task is a\nnon-decreasing function of the number of negative samples.\nmethods or from prediction-based methods. Cur-\nrently, prediction-based models garner substantial\nsupport; for example, Baroni et al. (2014) argue\nthat these models perform better across a range of\ntasks. In this work we argue that the two classes', 'operations such as calculating cross-entropies and softmax\nterms.\nguage models is ﬁrst suggested; the argument is\nthat RNNs keep state in the hidden layers, helping\nthe model remember arbitrarily long contexts, and\none would not need to decide, beforehand, how\nmany words to use as context in either side.\nIn 2012 Mnih and Teh have suggested further\nefﬁciency gains to the training of NNLMs. By\nleveraging Noise-contrastive Estimation (NCE).11\nNCE (Gutmann and Hyv ¨arinen (2010)) is a way\nof estimating probability distributions by means of\nbinary decisions over true/false examples.12. This\nhas enabled the authors to further reduce training\ntimes for NNLMs. In addition to faster training\ntimes, they also report better perplexity score w.r.t.\nprevious neural language models.\nIt could be said that, in 2013, with Mikolov et al.\n(2013a; 2013b; 2013c) the NLP community have\nagain (the main other example being Collobert and\nWeston (2008)) had its attention drawn to word']"
What is the Vector Space Model (VSM)?,"The VSM represents words and documents as vectors in high-dimensional space, enabling mathematical operations like cosine similarity for information retrieval. Generally attributed to Salton (1975).","The Vector Space Model (VSM) is a framework used to represent words and documents as vectors in a multi-dimensional space, allowing for operations like addition, subtraction, and similarity measures. It was popularized by Salton in 1975 and is widely used in information retrieval and various machine learning applications. The model encodes documents as vectors where each element corresponds to a distinct term, often using weighting schemes like TF-IDF.","['tors, which have an appealing, intuitive interpreta-\ntion, can be the subject of useful operations (e.g.\naddition, subtraction, distance measures, etc) and\nlend themselves well to be used in many Machine\nLearning (ML) algorithms and strategies.\nThe Vector Space Model (VSM), generally at-\ntributed to Salton (1975) and stemming from the\nInformation Retrieval (IR) community, is arguably\nthe most successful and inﬂuential model to en-\ncode words and documents as vectors.\nAnother very important part of natural\nlanguage-based solutions is, of course, the study\nof language models. A language model is a statis-\ntical model of language usage. It focuses mainly\non predicting the next word given a number of\nprevious words. This is very useful, for instance,\nin speech recognition software, where one needs\nto correctly decide what is the word said by the\nspeaker, even when signal quality is poor or there\nis a lot of background noise.\nThese two seemingly independent ﬁelds have', 'pseudo document), etc.\nTurney and Pantel (2010) provide a very thor-\nough survey of different ways to leverage the\nVSM, while explaining the particular applications\nmost suitable for them.\n2.2 Statistical Language Modelling\nStatistical language models are probabilistic mod-\nels of the distribution of words in a language. For\nexample, they can be used to calculate the likeli-\nhood of the next word given the words immedi-', 'statistical information by training only on\nthe nonzero elements in a word-word co-\noccurrence matrix, rather than on the en-\ntire sparse matrix or on individual context\nwindows in a large corpus. The model pro-\nduces a vector space with meaningful sub-\nstructure, as evidenced by its performance\nof 75% on a recent word analogy task. It\nalso outperforms related models on simi-\nlarity tasks and named entity recognition.\n1 Introduction\nSemantic vector space models of language repre-\nsent each word with a real-valued vector. These\nvectors can be used as features in a variety of ap-\nplications, such as information retrieval (Manning\net al., 2008), document classiﬁcation (Sebastiani,\n2002), question answering (Tellex et al., 2003),\nnamed entity recognition (Turian et al., 2010), and\nparsing (Socher et al., 2013).\nMost word vector methods rely on the distance\nor angle between pairs of word vectors as the pri-\nmary method for evaluating the intrinsic quality', 'One of the earliest approaches to that end was\nsuggested in the ﬁeld of Information Retrieval\n(IR), with the work of Salton et al. (1975). They\nsuggest an encoding procedure whereby each\ndocument in a collection is represented by a t-\ndimensional vector, each element representing a\ndistinct term contained in that document. These\nelements may be binary or real numbers, option-\nally normalized using a weighting scheme such as\nTF-IDF, to account for the difference in informa-\ntion provided by each term.\nWith such a vector space in place, one can\nthen proceed onto doing useful work on these vec-\ntors, such as calculating the similarity between\ndocument vectors (using even simple operations\nsuch as the inner-product between them), scor-\ning search results (viewing the search terms as a\npseudo document), etc.\nTurney and Pantel (2010) provide a very thor-\nough survey of different ways to leverage the\nVSM, while explaining the particular applications\nmost suitable for them.', 'Modern research on word embeddings (particu-\nlarly prediction-based models) has been, to some\nextent, borne out of attempts to make language\nmodelling more efﬁcient and more accurate. In\nfact, word embeddings (Bengio et al. (2003); Ben-\ngio and Sen´ecal (2003); Mnih and Hinton (2007),\nto cite a few) have been treated as by-products of\nlanguage models, and only after some time (ar-\nguably after Collobert and Weston (2008)) has\nthe building of word embeddings been decoupled\nfrom the task of language models.\nWe give brief introductions to these two topics\nnext.\n2.1 The Vector Space Model\nThe ﬁrst problem one encounters when trying to\napply analytical methods to text data is probably\nthat of how to represent it in a way that is amenable\nto operations such as similarity, composition, etc.\nOne of the earliest approaches to that end was\nsuggested in the ﬁeld of Information Retrieval\n(IR), with the work of Salton et al. (1975). They\nsuggest an encoding procedure whereby each']"
Who introduced the GloVe word embedding model and when?,GloVe (Global Vectors for Word Representation) was introduced by Pennington et al. in 2014.,"The GloVe word embedding model was introduced by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in October 2014.","['ported to be better than previous count-based mod-\nels such as LR-MVL and neural embeddings, such\nas those by Collobert and Weston (2008) and\nHLBL Mnih and Hinton (2008).\nThe last model we will cover in this section\nis the well-known GloVe18 by Pennington et al.\n(2014). This model starts at the insight that ratios\nof co-occurrences, rather than raw counts, encode\nactual semantic information about pair of words.\nThis relationship is used to derive a suitable loss\nfunction for a log-linear model, which is then\ntrained to maximize the similarity of every word\npair, as measured by the ratios of co-occurrences\n17This amounts to minimizing the distance between princi-\npal components and actual data, but using the Hellinger dis-\ntance instead of the more common Euclidean distance.\n18https://nlp.stanford.edu/projects/glove/\nmentioned earlier. Authors report better results\nthan other count-based models, as well as predic-\ntion based models such as SGNS (Mikolov et al.', 'other methods.\nDhillon et\nal. 2011\nLR-MVL is introduced. Uses CCA (Canonical Correla-\ntion Analysis) between left and right contexts to induce\nword embeddings.\nReports gains over C&W embeddings\n(Collobert and Weston (2008)), HLBL (Mnih\nand Hinton (2008)) and other methods, over\nmany NLP tasks.\nLebret and\nCollobert\n2013\nApplied a modiﬁed version of Principal Component\nAnalysis (called Hellinger PCA) to the word-context ma-\ntrix.\nEmbeddings can be tuned before being used in\nactual NLP tasks. Also reports gains over\nC&W embeddings, HLBL and other methods,\nover many NLP tasks.\nPennington\net al. 2014\nIntroduced GloVe, a log-linear model trained to encode\nsemantic relationships between words as vector offsets\nin the learned vector space, using the insight that co-\noccurrence ratios, rather than raw counts, are the actual\nconveyors of word meaning.\nReports gains over all previous count-based\nmodels and also SGNS (Mikolov et al.\n(2013c)), in multiple NLP tasks.', 'GloVe: Global Vectors for Word Representation\nJeffrey Pennington, Richard Socher, Christopher D. Manning\nComputer Science Department, Stanford University, Stanford, CA 94305\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\nAbstract\nRecent methods for learning vector space\nrepresentations of words have succeeded\nin capturing ﬁne-grained semantic and\nsyntactic regularities using vector arith-\nmetic, but the origin of these regularities\nhas remained opaque. We analyze and\nmake explicit the model properties needed\nfor such regularities to emerge in word\nvectors. The result is a new global log-\nbilinear regression model that combines\nthe advantages of the two major model\nfamilies in the literature: global matrix\nfactorization and local context window\nmethods. Our model efﬁciently leverages\nstatistical information by training only on\nthe nonzero elements in a word-word co-\noccurrence matrix, rather than on the en-\ntire sparse matrix or on individual context', 'ligence and Statistics.\nJeffrey Pennington, Richard Socher, and Christo-\npher Manning. October 2014. Glove: Global\nvectors for word representation. In Proceedings\nof the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). As-\nsociation for Computational Linguistics.\nDouglas L. T. Rohde, Laura M. Gonnerman, and\nDavid C. Plaut, 2006. An improved model\nof semantic similarity based on lexical co-\noccurence.\nG. Salton, A. Wong, and C. S. Yang, Novem-\nber 1975. A vector space model for automatic\nindexing. Available at http://doi.acm.\norg/10.1145/361219.361220.\nTobias Schnabel, Igor Labutov, David Mimno, and\nThorsten Joachims. 2015. Evaluation meth-\nods for unsupervised word embeddings. In Pro-\nceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nRichard Socher, Jeffrey Pennington, Eric H.\nHuang, Andrew Y . Ng, and Christopher D.\nManning. 2011. Semi-supervised recursive\nautoencoders for predicting sentiment distribu-', 'ease the effect of the curse of dimensionality on\nlanguage models, and help generalization (Bengio\net al. (2003)).\nWith time, such word embeddings have\nemerged as a topic of research in and of them-\nselves, with the realization that they can be used\nas standalone features in many NLP tasks (Turian\net al. (2010)) and the fact that they encode surpris-\ningly accurate syntactic and semantic word rela-\ntionships (Mikolov et al. (2013a)).\nMore recently2, other ways of creating embed-\ndings have surfaced, which rely not on neural net-\nworks and embedding layers but on leveraging\nword-context matrices to arrive at vector repre-\nsentations for words. Among the most inﬂuential\nmodels we can cite the GloVe model (Pennington\net al. (2014)).\nThese two types of model have something in\ncommon, namely their reliance on the assump-\ntion that words with similar contexts (other words)\nhave the same meaning. This has been called the\ndistributional hypothesis, and has been suggested']"
What is the main contribution of Neural Network Language Models (NNLMs)?,"NNLMs, pioneered by Bengio et al. (2003), reframed language modeling as unsupervised learning and introduced embedding layers that project words into dense vector spaces.","The main contribution of Neural Network Language Models (NNLMs) is their ability to learn word vector representations and statistical language models jointly, improving the efficiency and effectiveness of language modeling. They leverage architectures like feedforward neural networks and recurrent neural networks to capture complex patterns in language, enabling better handling of context and reducing training times. Additionally, NNLMs have facilitated advancements in word embeddings, which have become a significant area of research in natural language processing.","['Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\nlearn jointly the word vector representation and a statistical language model. This work has been\nfollowed by many others.\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\nwork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many', 'operations such as calculating cross-entropies and softmax\nterms.\nguage models is ﬁrst suggested; the argument is\nthat RNNs keep state in the hidden layers, helping\nthe model remember arbitrarily long contexts, and\none would not need to decide, beforehand, how\nmany words to use as context in either side.\nIn 2012 Mnih and Teh have suggested further\nefﬁciency gains to the training of NNLMs. By\nleveraging Noise-contrastive Estimation (NCE).11\nNCE (Gutmann and Hyv ¨arinen (2010)) is a way\nof estimating probability distributions by means of\nbinary decisions over true/false examples.12. This\nhas enabled the authors to further reduce training\ntimes for NNLMs. In addition to faster training\ntimes, they also report better perplexity score w.r.t.\nprevious neural language models.\nIt could be said that, in 2013, with Mikolov et al.\n(2013a; 2013b; 2013c) the NLP community have\nagain (the main other example being Collobert and\nWeston (2008)) had its attention drawn to word', 'For all the following models, the training complexity is proportional to\nO = E × T × Q, (1)\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\ndeﬁned further for each model architecture. Common choice is E = 3− 50 and T up to one billion.\nAll models are trained using stochastic gradient descent and backpropagation [26].\n2.1 Feedforward Neural Net Language Model (NNLM)\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\nusing 1-of- V coding, where V is size of the vocabulary. The input layer is then projected to a\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.', 'to correctly decide what is the word said by the\nspeaker, even when signal quality is poor or there\nis a lot of background noise.\nThese two seemingly independent ﬁelds have\narguably been brought together by recent research\n∗Geraldo Xex ´eo is also with the Mathematics Institute\n(IM-UFRJ), Federal University of Rio de Janeiro\non Neural Network Language Models (NNLMs),\nwith Bengio et al. (2003)) having developed the\nﬁrst1 large-scale language models based on neural\nnets.\nTheir idea was to reframe the problem as an\nunsupervised learning problem. A key feature of\nthis solution is the way raw words vectors are ﬁrst\nprojected onto a so-called embedding layer be-\nfore being fed into other layers of the network.\nAmong other reasons, this was imagined to help\nease the effect of the curse of dimensionality on\nlanguage models, and help generalization (Bengio\net al. (2003)).\nWith time, such word embeddings have\nemerged as a topic of research in and of them-', 'butions by Mikolov et al. (2009; 2010), which\nhave been used in later models. In the ﬁrst work,\n(Mikolov et al. (2009)) a two-step method for\nbootstraping a NNLM was suggested, whereby a\nﬁrst model was trained using a single word as con-\ntext. Then, the full model (with larger context)\nwas trained, using as initial embeddings those\nfound by the ﬁrst step.\nIn (Mikolov et al. (2010)), the idea of using\nRecurrent Neural Networks (RNNs) to train lan-\npendix A for more information.\n8Previous models focused on building language models,\nso they just used the left context.\n9I.e. sequences of words with the actual centre word re-\nplaced by a random word from the vocabulary.\n10This has been (Mikolov et al. (2013c)) called negative\nsampling and speeds up training because one can avoid costly\noperations such as calculating cross-entropies and softmax\nterms.\nguage models is ﬁrst suggested; the argument is\nthat RNNs keep state in the hidden layers, helping']"
What benchmark is used to evaluate text embedding models across multiple languages?,The Massive Text Embedding Benchmark (MTEB) evaluates embedding models across multiple languages and diverse NLP tasks.,The benchmark used to evaluate text embedding models across multiple languages is the Massive Text Embedding Benchmark (MTEB). It covers 58 datasets and includes 112 languages across various embedding tasks.,"['fair comparison among different text embedding models. But\nwe can still get some insights from the overall performance\ncomparison. The overall performance of the top 25 best text\nembeddings methods on MTEB English benchmark are shown\nin Table III, where the Model size is measured in Million Pa-\nrameters, #Memory is Memory Usage measured in (GB, fp32),\n#Embedding is the Embedding dimension. It can be seen that\nsome of the top performing text embeddings are not introduced\nin this review including voyage-lite-02-instruct, voyage-lite-\n01-instruct, text-embedding-3-large, Cohere-embed-english-\nv3, Cohere-embed-multilingual-v3, ember-v1, sf model e5,\netc. The main reason is that these models do not disclose any\ndetailed documentation.\n8https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct\nFor the models with documentations available, it can be\nseen that SFR-Embedding-Mistral has the best performance 9,\nwith the average performance over 56 MTEB datasets of', 'text-to-text models,” arXiv preprint arXiv:2108.08877 , 2021.\n[34] J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko,\nR. Kapadia, W. Ding, et al., “Gecko: Versatile text embeddings distilled\nfrom large language models,” arXiv preprint arXiv:2403.20327 , 2024.\n[35] S. Xiao, Z. Liu, P. Zhang, and N. Muennighof, “C-pack: Packaged\nresources to advance general chinese embedding,” arXiv preprint\narXiv:2309.07597, 2023.\n[36] A. Asai, T. Schick, P. Lewis, X. Chen, G. Izacard, S. Riedel, H. Ha-\njishirzi, and W.-t. Yih, “Task-aware retrieval with instructions,” arXiv\npreprint arXiv:2211.09260, 2022.\n[37] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers, “Mteb: Massive\ntext embedding benchmark,” arXiv preprint arXiv:2210.07316 , 2022.\n[38] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei,\n“Multilingual e5 text embeddings: A technical report,” arXiv preprint\narXiv:2402.05672, 2024.\n[39] P. BehnamGhader, V . Adlakha, M. Mosbach, D. Bahdanau, N. Cha-', 'been stimulated by several recent developments. These include\nthe growth in quantity and refinement in quality of diverse text\ndatasets across various tasks [35], [36], the production of high-\nquality synthetic data by LLMs [34], [16], and benchmarks\nthat emphasize new task and domain generalization, such\nas the multi-lingual Massive Text Embedding Benchmark\n(MTEB) [37].\nB. Background\nIn this work, we study and analyze the top performing\ntext embedding models that are either open-source or well\ndocumented from MTEB English benchmark (because the\nEnglish benchmark has more and diverse evaluation tasks\ncompared to other languages). It can be found that BERT-\nbased models used in [12], [35], [21], [38], [50] and LLMs\nused in [38], [49], [39], [51], [48], [34] are two most popular\nbackbones of the top performing universal text embedding\nmodels on the MTEB English benchmark.\na) BERT [28]: To generate contextual embeddings,\nBERT, pre-trained on a massive corpus and fine-tuned using la-', 'Reranking, Retrieval, Semantic Textual Similarity (STS) and\nSummarization [37]. The leader-board results are available\non the Hugging Face Hub 1, where the results of English\n(56 datasets), Chinese (35 datasets), French (26 datasets)\nand Polish (26 datasets) benchmark results can be found\nrespectively.\nC. Taxonomy of universal text embeddings\nIn this section, the main focuses and contributions of the\nsome of the MTEB top performing state of the art text embed-\nding methods are analyzed (shown in Figure 2), including: E5:\nEmbEddings from bidirEctional Encoder rEpresentations [21],\nGTE: General-purpose Text Embedding model [12], BGE:\nBeijing Academy of Artificial Intelligence (BAAI) General\nEmbedding [35], UAE: Universal AnglE Embedding [50],\nMRL: Matryoshka Representation Learning [62], 2DMSE:\n2D Matryoshka Sentence Embeddings [63], GRIT: Generative\nRepresentational Instruction Tuning [49], LLM2Vec: [39],\nMultilingual E5: [38], E5-mistral-7b-instruct: [16], Gecko:', '4\nFig. 2. Representative state of the art universal text embeddings and their\nmain focus/contributions.\ncome before it. The authors from [39] hypothesize that causal\nattention mechanism might partly be the reason of the slow\nadoption of decoder-only LLMs for text embedding tasks as\nit inherently limits their ability to produce rich contextualized\nrepresentations. Several recent works such [38], [49], [39],\n[51], [48], [34] have proposed several solutions to mitigate\nsuch limitations.\nc) Massive Text Embedding Benchmark (MTEB) : The\nobjective of MTEB is to provide comprehensive understand-\nings on the universality of text embedding models, including\n58 datasets covering 112 languages from 8 embedding tasks:\nBitext mining, Classification, Clustering, Pair classification,\nReranking, Retrieval, Semantic Textual Similarity (STS) and\nSummarization [37]. The leader-board results are available\non the Hugging Face Hub 1, where the results of English']"
What is the key advantage of subword tokenization in neural machine translation?,"Subword tokenization (like BPE) enables open-vocabulary translation, handling rare words and achieving better compression while maintaining translation quality.","The key advantage of subword tokenization in neural machine translation is its ability to handle open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This approach improves the translation accuracy for rare words compared to large-vocabulary models and allows the generation of new words not seen during training. Additionally, it helps avoid the information bottleneck associated with fixed-length representations.","['level of subword units. Our main goal is to model\nopen-vocabulary translation in the NMT network\nitself, without requiring a back-off model for rare\nwords. In addition to making the translation pro-\ncess simpler, we also ﬁnd that the subword models\nachieve better accuracy for the translation of rare\nwords than large-vocabulary models and back-off\ndictionaries, and are able to productively generate\nnew words that were not seen at training time. Our\nanalysis shows that the neural networks are able to\nlearn compounding and transliteration from sub-\nword representations.\nThis paper has two main contributions:\n• We show that open-vocabulary neural ma-', 'ble at test time.\nVarious techniques have been proposed to pro-\nduce ﬁxed-length continuous word vectors based\non characters or morphemes (Luong et al., 2013;\nBotha and Blunsom, 2014; Ling et al., 2015a; Kim\net al., 2015). An effort to apply such techniques\nto NMT, parallel to ours, has found no signiﬁcant\nimprovement over word-based approaches (Ling\net al., 2015b). One technical difference from our\nwork is that the attention mechanism still oper-\nates on the level of words in the model by Ling\net al. (2015b), and that the representation of each\nword is ﬁxed-length. We expect that the attention\nmechanism beneﬁts from our variable-length rep-\nresentation: the network can learn to place atten-\ntion on different subword units at each step. Re-\ncall our introductory example Abwasserbehand-\nlungsanlange, for which a subword segmentation\navoids the information bottleneck of a ﬁxed-length\nrepresentation.\nNeural machine translation differs from phrase-', 'for instance names (via character copying\nor transliteration), compounds (via com-\npositional translation), and cognates and\nloanwords (via phonological and morpho-\nlogical transformations). We discuss the\nsuitability of different word segmentation\ntechniques, including simple character n-\ngram models and a segmentation based on\nthe byte pair encoding compression algo-\nrithm, and empirically show that subword\nmodels improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks\nEnglish→German and English →Russian\nby up to 1.1 and 1.3 BLEU , respectively.\n1 Introduction\nNeural machine translation has recently shown\nimpressive results (Kalchbrenner and Blunsom,\n2013; Sutskever et al., 2014; Bahdanau et al.,\n2015). However, the translation of rare words\nis an open problem. The vocabulary of neu-\nral models is typically limited to 30 000–50 000\nwords, but translation is an open-vocabulary prob-\nThe research presented in this publication was conducted', 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich and Barry Haddow and Alexandra Birch\nSchool of Informatics, University of Edinburgh\n{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.uk\nAbstract\nNeural machine translation (NMT) mod-\nels typically operate with a ﬁxed vocabu-\nlary, but translation is an open-vocabulary\nproblem. Previous work addresses the\ntranslation of out-of-vocabulary words by\nbacking off to a dictionary. In this pa-\nper, we introduce a simpler and more ef-\nfective approach, making the NMT model\ncapable of open-vocabulary translation by\nencoding rare and unknown words as se-\nquences of subword units. This is based on\nthe intuition that various word classes are\ntranslatable via smaller units than words,\nfor instance names (via character copying\nor transliteration), compounds (via com-\npositional translation), and cognates and\nloanwords (via phonological and morpho-', 'lungsanlange, for which a subword segmentation\navoids the information bottleneck of a ﬁxed-length\nrepresentation.\nNeural machine translation differs from phrase-\nbased methods in that there are strong incentives to\nminimize the vocabulary size of neural models to\nincrease time and space efﬁciency, and to allow for\ntranslation without back-off models. At the same\ntime, we also want a compact representation of the\ntext itself, since an increase in text length reduces\nefﬁciency and increases the distances over which\nneural models need to pass information.\nA simple method to manipulate the trade-off be-\ntween vocabulary size and text size is to use short-\nlists of unsegmented words, using subword units\nonly for rare words. As an alternative, we pro-\npose a segmentation algorithm based on byte pair\nencoding (BPE), which lets us learn a vocabulary\nthat provides a good compression rate of the text.\n3.2 Byte Pair Encoding (BPE)\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-']"
