{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîß Transformers Basics\n",
                "\n",
                "**Module 01 | Notebook 1 of 2**\n",
                "\n",
                "In this notebook, you'll learn the fundamental building blocks of working with transformer models using the Hugging Face Transformers library.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Load pre-trained models using `AutoModel` and `AutoTokenizer`\n",
                "2. Understand the tokenization process\n",
                "3. Perform model inference\n",
                "4. Use pipelines for common tasks\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets torch accelerate\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
                "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1Ô∏è‚É£ Understanding Tokenization\n",
                "\n",
                "### What is Tokenization?\n",
                "\n",
                "Tokenization is the process of converting text into smaller units (tokens) that the model can process. These tokens are then converted to numerical IDs.\n",
                "\n",
                "```\n",
                "Text: \"Hello, how are you?\"\n",
                "  ‚Üì Tokenization\n",
                "Tokens: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
                "  ‚Üì Convert to IDs\n",
                "Token IDs: [7592, 1010, 2129, 2024, 2017, 1029]\n",
                "```\n",
                "\n",
                "### Why Tokenization Matters\n",
                "\n",
                "- Models can only process numbers, not text\n",
                "- Different models use different tokenization strategies\n",
                "- Token count affects memory usage and processing time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a tokenizer\n",
                "model_name = \"bert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Simple tokenization example\n",
                "text = \"Hello, how are you doing today?\"\n",
                "\n",
                "# Tokenize\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(f\"Original text: {text}\")\n",
                "print(f\"Tokens: {tokens}\")\n",
                "print(f\"Number of tokens: {len(tokens)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert tokens to IDs\n",
                "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
                "print(f\"Token IDs: {token_ids}\")\n",
                "\n",
                "# We can also go back from IDs to tokens\n",
                "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
                "print(f\"Decoded tokens: {decoded_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Complete Tokenization Pipeline\n",
                "\n",
                "In practice, we use the tokenizer's `__call__` method which handles everything:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Complete tokenization with the __call__ method\n",
                "encoded = tokenizer(text, return_tensors=\"pt\")\n",
                "\n",
                "print(\"Encoded outputs:\")\n",
                "print(f\"  Keys: {list(encoded.keys())}\")\n",
                "print(f\"  input_ids shape: {encoded['input_ids'].shape}\")\n",
                "print(f\"  input_ids: {encoded['input_ids']}\")\n",
                "print(f\"  attention_mask: {encoded['attention_mask']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Outputs\n",
                "\n",
                "| Field | Description |\n",
                "|-------|-------------|\n",
                "| `input_ids` | Token IDs for the model |\n",
                "| `attention_mask` | 1s for real tokens, 0s for padding |\n",
                "| `token_type_ids` | Segment IDs (for sentence pairs) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the tokenization\n",
                "print(\"Token-by-token breakdown:\")\n",
                "print(\"-\" * 40)\n",
                "for token_id, attention in zip(encoded['input_ids'][0], encoded['attention_mask'][0]):\n",
                "    token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
                "    print(f\"ID: {token_id.item():5d} | Attention: {attention.item()} | Token: {token}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Special Tokens\n",
                "\n",
                "Models use special tokens to mark the beginning/end of sequences:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Special tokens:\")\n",
                "print(f\"  [CLS] token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
                "print(f\"  [SEP] token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
                "print(f\"  [PAD] token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
                "print(f\"  [UNK] token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2Ô∏è‚É£ Loading Pre-trained Models\n",
                "\n",
                "### The Auto Classes\n",
                "\n",
                "Hugging Face provides `Auto` classes that automatically detect the correct model architecture:\n",
                "\n",
                "| Class | Use Case |\n",
                "|-------|----------|\n",
                "| `AutoModel` | Base model (embeddings only) |\n",
                "| `AutoModelForSequenceClassification` | Text classification |\n",
                "| `AutoModelForSeq2SeqLM` | Sequence-to-sequence (summarization, translation) |\n",
                "| `AutoModelForCausalLM` | Text generation (GPT-style) |\n",
                "| `AutoModelForQuestionAnswering` | Extractive QA |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the base BERT model\n",
                "model = AutoModel.from_pretrained(model_name)\n",
                "\n",
                "print(f\"Model type: {type(model).__name__}\")\n",
                "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get model configuration\n",
                "config = model.config\n",
                "\n",
                "print(\"Model configuration:\")\n",
                "print(f\"  Hidden size: {config.hidden_size}\")\n",
                "print(f\"  Number of layers: {config.num_hidden_layers}\")\n",
                "print(f\"  Number of attention heads: {config.num_attention_heads}\")\n",
                "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
                "print(f\"  Max position embeddings: {config.max_position_embeddings}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model Inference\n",
                "\n",
                "Let's pass our tokenized text through the model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Move model to device\n",
                "model = model.to(device)\n",
                "\n",
                "# Prepare inputs\n",
                "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "\n",
                "# Run inference (no gradient computation needed)\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "\n",
                "print(f\"Output keys: {list(outputs.keys())}\")\n",
                "print(f\"Last hidden state shape: {outputs.last_hidden_state.shape}\")\n",
                "print(f\"  - Batch size: {outputs.last_hidden_state.shape[0]}\")\n",
                "print(f\"  - Sequence length: {outputs.last_hidden_state.shape[1]}\")\n",
                "print(f\"  - Hidden size: {outputs.last_hidden_state.shape[2]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Output\n",
                "\n",
                "The `last_hidden_state` contains embeddings for each token:\n",
                "\n",
                "```\n",
                "Shape: [batch_size, sequence_length, hidden_size]\n",
                "       [1,          10,              768]\n",
                "```\n",
                "\n",
                "Each token is now represented as a 768-dimensional vector that captures its meaning in context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the [CLS] token embedding (often used for classification)\n",
                "cls_embedding = outputs.last_hidden_state[0, 0, :]  # First token of first batch\n",
                "print(f\"CLS embedding shape: {cls_embedding.shape}\")\n",
                "print(f\"CLS embedding (first 10 values): {cls_embedding[:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ Task-Specific Models\n",
                "\n",
                "For specific tasks, use the appropriate model class:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a classification model\n",
                "classifier = AutoModelForSequenceClassification.from_pretrained(\n",
                "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                ").to(device)\n",
                "\n",
                "classifier_tokenizer = AutoTokenizer.from_pretrained(\n",
                "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                ")\n",
                "\n",
                "print(f\"Number of labels: {classifier.config.num_labels}\")\n",
                "print(f\"Label mapping: {classifier.config.id2label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run classification\n",
                "test_texts = [\n",
                "    \"I absolutely love this product!\",\n",
                "    \"This is the worst experience ever.\",\n",
                "    \"It's okay, nothing special.\"\n",
                "]\n",
                "\n",
                "for text in test_texts:\n",
                "    inputs = classifier_tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = classifier(**inputs)\n",
                "    \n",
                "    # Get probabilities\n",
                "    probs = torch.softmax(outputs.logits, dim=-1)\n",
                "    predicted_class = torch.argmax(probs).item()\n",
                "    confidence = probs[0, predicted_class].item()\n",
                "    \n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"  ‚Üí {classifier.config.id2label[predicted_class]} ({confidence:.2%})\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ Using Pipelines\n",
                "\n",
                "For quick prototyping, use the `pipeline` API which abstracts away tokenization and post-processing:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Sentiment analysis pipeline\n",
                "sentiment_pipeline = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "results = sentiment_pipeline(test_texts)\n",
                "\n",
                "print(\"Pipeline Results:\")\n",
                "for text, result in zip(test_texts, results):\n",
                "    print(f\"{text}\")\n",
                "    print(f\"  ‚Üí {result['label']} ({result['score']:.2%})\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=\"facebook/bart-large-cnn\",\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "long_text = \"\"\"\n",
                "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
                "in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
                "encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) \n",
                "are covered by the rainforest. This region includes territory belonging to nine nations \n",
                "and 3,344 formally acknowledged indigenous territories. The majority of the forest is \n",
                "contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia \n",
                "with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, \n",
                "and Venezuela.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(long_text, max_length=50, min_length=20, do_sample=False)\n",
                "print(f\"Original length: {len(long_text.split())} words\")\n",
                "print(f\"Summary: {summary[0]['summary_text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Available Pipelines\n",
                "\n",
                "| Pipeline | Task |\n",
                "|----------|------|\n",
                "| `text-classification` | Sentiment, topic classification |\n",
                "| `token-classification` | NER, POS tagging |\n",
                "| `question-answering` | Extractive QA |\n",
                "| `summarization` | Text summarization |\n",
                "| `translation` | Machine translation |\n",
                "| `text-generation` | GPT-style generation |\n",
                "| `fill-mask` | Masked language modeling |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ Batch Processing\n",
                "\n",
                "For efficiency, process multiple inputs at once:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Batch tokenization with padding\n",
                "texts = [\n",
                "    \"Short text.\",\n",
                "    \"This is a medium length sentence.\",\n",
                "    \"This is a much longer sentence that contains many more words and tokens.\"\n",
                "]\n",
                "\n",
                "# Tokenize with padding\n",
                "batch_encoded = tokenizer(\n",
                "    texts,\n",
                "    padding=True,           # Pad to longest in batch\n",
                "    truncation=True,        # Truncate if too long\n",
                "    max_length=32,          # Maximum length\n",
                "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
                ")\n",
                "\n",
                "print(f\"Batch shape: {batch_encoded['input_ids'].shape}\")\n",
                "print(f\"\\nInput IDs:\")\n",
                "print(batch_encoded['input_ids'])\n",
                "print(f\"\\nAttention Mask (0 = padding):\")\n",
                "print(batch_encoded['attention_mask'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "Now it's your turn! Complete the following exercises:\n",
                "\n",
                "### Challenge 1: Compare Tokenizers\n",
                "Load tokenizers for `bert-base-uncased` and `gpt2`, then compare how they tokenize the same sentence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Your code here\n",
                "# 1. Load both tokenizers\n",
                "# 2. Tokenize: \"The transformer architecture revolutionized natural language processing.\"\n",
                "# 3. Print the tokens and token counts for each\n",
                "\n",
                "# bert_tokenizer = ...\n",
                "# gpt2_tokenizer = ...\n",
                "\n",
                "test_sentence = \"The transformer architecture revolutionized natural language processing.\"\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Challenge 2: Model Size Comparison\n",
                "Load `distilbert-base-uncased` and `bert-base-uncased`, then compare their parameter counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Your code here\n",
                "# 1. Load both models\n",
                "# 2. Count parameters for each\n",
                "# 3. Calculate the size reduction percentage\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Key Takeaways\n",
                "\n",
                "1. **Tokenization** converts text to numerical IDs that models can process\n",
                "2. **Auto classes** automatically detect the right model architecture\n",
                "3. **Task-specific models** add appropriate heads for classification, generation, etc.\n",
                "4. **Pipelines** provide a high-level API for quick prototyping\n",
                "5. **Batch processing** with padding improves efficiency\n",
                "\n",
                "---\n",
                "\n",
                "## ‚û°Ô∏è Next Steps\n",
                "\n",
                "Continue to `02_model_architecture.ipynb` to learn about:\n",
                "- Encoder vs. Decoder architectures\n",
                "- Attention mechanism visualization\n",
                "- Memory and compute requirements"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}