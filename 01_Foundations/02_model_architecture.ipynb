{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèóÔ∏è Model Architecture Deep Dive\n",
                "\n",
                "**Module 01 | Notebook 2 of 2**\n",
                "\n",
                "In this notebook, we'll explore the internal architecture of transformer models to understand how they process information.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Distinguish between Encoder, Decoder, and Encoder-Decoder architectures\n",
                "2. Understand how attention mechanisms work\n",
                "3. Analyze model parameters and memory requirements\n",
                "4. Choose the right architecture for your task\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch matplotlib seaborn pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoModel, AutoTokenizer, AutoConfig,\n",
                "    BertModel, GPT2Model, T5Model,\n",
                "    AutoModelForSequenceClassification,\n",
                "    AutoModelForCausalLM,\n",
                "    AutoModelForSeq2SeqLM\n",
                ")\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1Ô∏è‚É£ Transformer Architecture Types\n",
                "\n",
                "There are three main types of transformer architectures:\n",
                "\n",
                "### Architecture Overview\n",
                "\n",
                "| Type | Examples | Best For | How It Works |\n",
                "|------|----------|----------|-------------|\n",
                "| **Encoder-only** | BERT, RoBERTa, DistilBERT | Classification, NER, embeddings | Bidirectional attention |\n",
                "| **Decoder-only** | GPT-2, GPT-3, LLaMA | Text generation | Causal (left-to-right) attention |\n",
                "| **Encoder-Decoder** | T5, BART, mT5 | Translation, summarization | Cross-attention between encoder/decoder |\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ                        Transformer Architectures                        ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ    ENCODER-ONLY     ‚îÇ     DECODER-ONLY      ‚îÇ     ENCODER-DECODER       ‚îÇ\n",
                "‚îÇ    (e.g., BERT)     ‚îÇ     (e.g., GPT)       ‚îÇ     (e.g., T5)            ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ                     ‚îÇ                       ‚îÇ                           ‚îÇ\n",
                "‚îÇ  [Input Tokens]     ‚îÇ   [Input Tokens]      ‚îÇ  [Input]    [Output]      ‚îÇ\n",
                "‚îÇ        ‚Üì            ‚îÇ         ‚Üì             ‚îÇ     ‚Üì          ‚Üì          ‚îÇ\n",
                "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
                "‚îÇ  ‚îÇ Encoder ‚îÇ        ‚îÇ   ‚îÇ Decoder ‚îÇ         ‚îÇ  ‚îÇ Enc ‚îÇ‚îÄ‚îÄ‚Üí‚îÇ Dec ‚îÇ        ‚îÇ\n",
                "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
                "‚îÇ        ‚Üì            ‚îÇ         ‚Üì             ‚îÇ               ‚Üì           ‚îÇ\n",
                "‚îÇ  [Embeddings]       ‚îÇ   [Next Token]        ‚îÇ         [Output]          ‚îÇ\n",
                "‚îÇ                     ‚îÇ                       ‚îÇ                           ‚îÇ\n",
                "‚îÇ  Bidirectional      ‚îÇ   Left-to-Right       ‚îÇ     Cross-Attention       ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load examples of each architecture type\n",
                "encoder_model = \"bert-base-uncased\"\n",
                "decoder_model = \"gpt2\"\n",
                "seq2seq_model = \"t5-small\"\n",
                "\n",
                "# Get configurations\n",
                "encoder_config = AutoConfig.from_pretrained(encoder_model)\n",
                "decoder_config = AutoConfig.from_pretrained(decoder_model)\n",
                "seq2seq_config = AutoConfig.from_pretrained(seq2seq_model)\n",
                "\n",
                "print(\"Model Configurations:\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nBERT (Encoder-only):\")\n",
                "print(f\"  Hidden size: {encoder_config.hidden_size}\")\n",
                "print(f\"  Layers: {encoder_config.num_hidden_layers}\")\n",
                "print(f\"  Attention heads: {encoder_config.num_attention_heads}\")\n",
                "\n",
                "print(f\"\\nGPT-2 (Decoder-only):\")\n",
                "print(f\"  Hidden size: {decoder_config.hidden_size}\")\n",
                "print(f\"  Layers: {decoder_config.n_layer}\")\n",
                "print(f\"  Attention heads: {decoder_config.n_head}\")\n",
                "\n",
                "print(f\"\\nT5 (Encoder-Decoder):\")\n",
                "print(f\"  Hidden size: {seq2seq_config.d_model}\")\n",
                "print(f\"  Encoder layers: {seq2seq_config.num_layers}\")\n",
                "print(f\"  Decoder layers: {seq2seq_config.num_decoder_layers}\")\n",
                "print(f\"  Attention heads: {seq2seq_config.num_heads}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2Ô∏è‚É£ Understanding Attention\n",
                "\n",
                "### What is Attention?\n",
                "\n",
                "Attention allows the model to focus on relevant parts of the input when processing each token.\n",
                "\n",
                "```\n",
                "Query: \"What does 'it' refer to?\"\n",
                "\n",
                "Sentence: \"The cat sat on the mat because it was comfortable.\"\n",
                "                                          ‚Üë\n",
                "                                         'it'\n",
                "                                          ‚îÇ\n",
                "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "            ‚îÇ\n",
                "            ‚ñº\n",
                "Attention:  The  cat  sat  on  the  mat  because  it  was  comfortable\n",
                "           [0.1][0.7][0.1][0.0][0.0][0.1]  [0.0] [1.0][0.0]    [0.0]\n",
                "```\n",
                "\n",
                "The model learns that \"it\" most likely refers to \"cat\" (high attention weight)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load BERT with attention outputs\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
                "model.eval()\n",
                "\n",
                "# Tokenize a sentence\n",
                "text = \"The cat sat on the mat.\"\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "\n",
                "# Get outputs with attention\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "\n",
                "# Attention shape: (batch, num_heads, seq_len, seq_len)\n",
                "attentions = outputs.attentions\n",
                "\n",
                "print(f\"Number of layers: {len(attentions)}\")\n",
                "print(f\"Attention shape per layer: {attentions[0].shape}\")\n",
                "print(f\"  - Batch size: {attentions[0].shape[0]}\")\n",
                "print(f\"  - Attention heads: {attentions[0].shape[1]}\")\n",
                "print(f\"  - Sequence length: {attentions[0].shape[2]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention for one layer and head\n",
                "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
                "\n",
                "# Get attention from first layer, first head\n",
                "attention = attentions[0][0, 0].numpy()  # Layer 0, Head 0\n",
                "\n",
                "# Create heatmap\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    attention,\n",
                "    xticklabels=tokens,\n",
                "    yticklabels=tokens,\n",
                "    cmap='Blues',\n",
                "    annot=True,\n",
                "    fmt='.2f'\n",
                ")\n",
                "plt.title('Attention Weights (Layer 1, Head 1)')\n",
                "plt.xlabel('Key (attending to)')\n",
                "plt.ylabel('Query (from)')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bidirectional vs. Causal Attention\n",
                "\n",
                "- **Bidirectional (Encoder)**: Each token can attend to ALL tokens\n",
                "- **Causal (Decoder)**: Each token can only attend to PREVIOUS tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention patterns\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "seq_len = 6\n",
                "tokens = ['[CLS]', 'The', 'cat', 'sat', 'down', '[SEP]']\n",
                "\n",
                "# Bidirectional (full attention)\n",
                "bidirectional = torch.ones(seq_len, seq_len)\n",
                "\n",
                "# Causal (lower triangular)\n",
                "causal = torch.tril(torch.ones(seq_len, seq_len))\n",
                "\n",
                "# Plot bidirectional\n",
                "sns.heatmap(bidirectional, ax=axes[0], cmap='Greens', cbar=False,\n",
                "            xticklabels=tokens, yticklabels=tokens)\n",
                "axes[0].set_title('Bidirectional Attention (BERT)\\n(Can see all tokens)')\n",
                "\n",
                "# Plot causal\n",
                "sns.heatmap(causal, ax=axes[1], cmap='Blues', cbar=False,\n",
                "            xticklabels=tokens, yticklabels=tokens)\n",
                "axes[1].set_title('Causal Attention (GPT)\\n(Can only see previous tokens)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ Parameter Analysis\n",
                "\n",
                "Understanding model size is crucial for deployment planning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_parameters(model):\n",
                "    \"\"\"Count trainable and total parameters.\"\"\"\n",
                "    total = sum(p.numel() for p in model.parameters())\n",
                "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    return total, trainable\n",
                "\n",
                "def get_model_size_mb(model):\n",
                "    \"\"\"Estimate model size in MB (assuming float32).\"\"\"\n",
                "    total_params, _ = count_parameters(model)\n",
                "    return total_params * 4 / (1024 ** 2)  # 4 bytes per float32\n",
                "\n",
                "# Load models for comparison\n",
                "models_to_compare = [\n",
                "    (\"distilbert-base-uncased\", \"DistilBERT\"),\n",
                "    (\"bert-base-uncased\", \"BERT-base\"),\n",
                "    (\"bert-large-uncased\", \"BERT-large\"),\n",
                "]\n",
                "\n",
                "comparison_data = []\n",
                "\n",
                "for model_name, display_name in models_to_compare:\n",
                "    print(f\"Loading {display_name}...\")\n",
                "    model = AutoModel.from_pretrained(model_name)\n",
                "    config = model.config\n",
                "    total, trainable = count_parameters(model)\n",
                "    size_mb = get_model_size_mb(model)\n",
                "    \n",
                "    comparison_data.append({\n",
                "        \"Model\": display_name,\n",
                "        \"Parameters (M)\": total / 1e6,\n",
                "        \"Size (MB)\": size_mb,\n",
                "        \"Layers\": config.num_hidden_layers,\n",
                "        \"Hidden Size\": config.hidden_size,\n",
                "        \"Attention Heads\": config.num_attention_heads\n",
                "    })\n",
                "    del model\n",
                "\n",
                "df = pd.DataFrame(comparison_data)\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Parameter count\n",
                "axes[0].barh(df['Model'], df['Parameters (M)'], color=['#2ecc71', '#3498db', '#9b59b6'])\n",
                "axes[0].set_xlabel('Parameters (Millions)')\n",
                "axes[0].set_title('Model Size Comparison')\n",
                "for i, v in enumerate(df['Parameters (M)']):\n",
                "    axes[0].text(v + 5, i, f'{v:.1f}M', va='center')\n",
                "\n",
                "# Memory size\n",
                "axes[1].barh(df['Model'], df['Size (MB)'], color=['#2ecc71', '#3498db', '#9b59b6'])\n",
                "axes[1].set_xlabel('Size (MB)')\n",
                "axes[1].set_title('Memory Footprint (FP32)')\n",
                "for i, v in enumerate(df['Size (MB)']):\n",
                "    axes[1].text(v + 20, i, f'{v:.0f}MB', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ Layer-by-Layer Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed layer analysis for BERT\n",
                "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "# Analyze each component\n",
                "layer_params = {}\n",
                "\n",
                "for name, param in model.named_parameters():\n",
                "    # Group by first two levels\n",
                "    parts = name.split('.')\n",
                "    if len(parts) >= 2:\n",
                "        group = f\"{parts[0]}.{parts[1]}\"\n",
                "    else:\n",
                "        group = parts[0]\n",
                "    \n",
                "    if group not in layer_params:\n",
                "        layer_params[group] = 0\n",
                "    layer_params[group] += param.numel()\n",
                "\n",
                "# Sort by parameter count\n",
                "sorted_layers = sorted(layer_params.items(), key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(\"BERT Parameter Distribution:\")\n",
                "print(\"=\" * 50)\n",
                "total = sum(layer_params.values())\n",
                "for name, count in sorted_layers[:10]:\n",
                "    pct = count / total * 100\n",
                "    print(f\"{name:40s} {count:>12,} ({pct:>5.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize parameter distribution\n",
                "labels = [name for name, _ in sorted_layers[:8]]\n",
                "sizes = [count / 1e6 for _, count in sorted_layers[:8]]\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(labels, sizes, color='steelblue')\n",
                "plt.xlabel('Parameters (Millions)')\n",
                "plt.title('BERT Parameter Distribution by Component')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ Memory Requirements\n",
                "\n",
                "### Memory During Training vs. Inference\n",
                "\n",
                "| Phase | What's Stored | Memory Factor |\n",
                "|-------|---------------|---------------|\n",
                "| **Inference** | Model weights only | 1x |\n",
                "| **Training** | Weights + Gradients + Optimizer states | ~4x |\n",
                "\n",
                "### Precision Impact\n",
                "\n",
                "| Precision | Bytes per Param | Example (110M params) |\n",
                "|-----------|-----------------|---------------------|\n",
                "| FP32 | 4 | 440 MB |\n",
                "| FP16/BF16 | 2 | 220 MB |\n",
                "| INT8 | 1 | 110 MB |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def estimate_memory(model, batch_size=8, seq_length=512, precision='fp32'):\n",
                "    \"\"\"Estimate memory requirements for inference.\"\"\"\n",
                "    total_params, _ = count_parameters(model)\n",
                "    \n",
                "    # Bytes per parameter\n",
                "    bytes_per_param = {'fp32': 4, 'fp16': 2, 'int8': 1}[precision]\n",
                "    \n",
                "    # Model weights\n",
                "    model_memory = total_params * bytes_per_param\n",
                "    \n",
                "    # Activation memory (rough estimate)\n",
                "    hidden_size = model.config.hidden_size\n",
                "    num_layers = model.config.num_hidden_layers\n",
                "    activation_memory = batch_size * seq_length * hidden_size * num_layers * bytes_per_param\n",
                "    \n",
                "    return {\n",
                "        'model_mb': model_memory / (1024**2),\n",
                "        'activation_mb': activation_memory / (1024**2),\n",
                "        'total_mb': (model_memory + activation_memory) / (1024**2)\n",
                "    }\n",
                "\n",
                "# Compare memory for different precisions\n",
                "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "print(\"Memory Estimates for BERT-base (batch=8, seq=512):\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for precision in ['fp32', 'fp16', 'int8']:\n",
                "    mem = estimate_memory(model, precision=precision)\n",
                "    print(f\"\\n{precision.upper()}:\")\n",
                "    print(f\"  Model weights: {mem['model_mb']:.1f} MB\")\n",
                "    print(f\"  Activations:   {mem['activation_mb']:.1f} MB\")\n",
                "    print(f\"  Total:         {mem['total_mb']:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6Ô∏è‚É£ Choosing the Right Architecture\n",
                "\n",
                "### Decision Tree\n",
                "\n",
                "```\n",
                "                        What's your task?\n",
                "                              ‚îÇ\n",
                "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "          ‚îÇ                   ‚îÇ                   ‚îÇ\n",
                "     Understanding       Generation         Transformation\n",
                "     (Classification,    (Text gen,        (Translation,\n",
                "      NER, QA)           Completion)        Summarization)\n",
                "          ‚îÇ                   ‚îÇ                   ‚îÇ\n",
                "          ‚ñº                   ‚ñº                   ‚ñº\n",
                "     ENCODER-ONLY        DECODER-ONLY       ENCODER-DECODER\n",
                "     (BERT, RoBERTa)     (GPT-2, LLaMA)     (T5, BART)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick reference table\n",
                "architecture_guide = pd.DataFrame({\n",
                "    'Task': [\n",
                "        'Sentiment Analysis',\n",
                "        'Named Entity Recognition',\n",
                "        'Text Classification',\n",
                "        'Text Generation',\n",
                "        'Chatbot/Dialogue',\n",
                "        'Translation',\n",
                "        'Summarization',\n",
                "        'Question Answering'\n",
                "    ],\n",
                "    'Architecture': [\n",
                "        'Encoder (BERT)',\n",
                "        'Encoder (BERT)',\n",
                "        'Encoder (BERT)',\n",
                "        'Decoder (GPT)',\n",
                "        'Decoder (GPT)',\n",
                "        'Encoder-Decoder (T5)',\n",
                "        'Encoder-Decoder (BART)',\n",
                "        'Encoder or Enc-Dec'\n",
                "    ],\n",
                "    'Recommended Model': [\n",
                "        'distilbert-base-uncased',\n",
                "        'bert-base-cased',\n",
                "        'roberta-base',\n",
                "        'gpt2 / gpt2-medium',\n",
                "        'facebook/blenderbot-400M-distill',\n",
                "        't5-base / mbart-large-50',\n",
                "        'facebook/bart-large-cnn',\n",
                "        'deepset/roberta-base-squad2'\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(\"Architecture Selection Guide\")\n",
                "print(\"=\" * 80)\n",
                "print(architecture_guide.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "### Challenge 1: Compare T5 Model Sizes\n",
                "\n",
                "Load `t5-small`, `t5-base`, and `t5-large`, then create a comparison table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Your code here\n",
                "# 1. Load each T5 variant\n",
                "# 2. Count parameters and estimate memory\n",
                "# 3. Create a comparison DataFrame\n",
                "# 4. Visualize with a bar chart\n",
                "\n",
                "t5_variants = [\"t5-small\", \"t5-base\"]  # Add t5-large if you have enough memory\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Challenge 2: Attention Visualization\n",
                "\n",
                "Visualize attention patterns for a longer sentence across multiple layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Your code here\n",
                "# 1. Use BERT with output_attentions=True\n",
                "# 2. Tokenize: \"The quick brown fox jumps over the lazy dog.\"\n",
                "# 3. Create a 2x2 grid showing attention from layers 1, 4, 8, 12\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Key Takeaways\n",
                "\n",
                "1. **Encoder models** (BERT) use bidirectional attention for understanding\n",
                "2. **Decoder models** (GPT) use causal attention for generation\n",
                "3. **Encoder-Decoder models** (T5) combine both for sequence-to-sequence tasks\n",
                "4. **Model size** impacts memory, latency, and accuracy\n",
                "5. **Precision reduction** (FP16, INT8) can significantly reduce memory\n",
                "\n",
                "---\n",
                "\n",
                "## ‚û°Ô∏è Next Steps\n",
                "\n",
                "Continue to Module 02: **Fine-Tuning**\n",
                "- `02_Fine_Tuning/01_transfer_learning.ipynb`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}