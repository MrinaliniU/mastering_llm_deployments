{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Summarization with DialogSum\n",
                "\n",
                "**Module 02 | Notebook 3 of 3**\n",
                "\n",
                "In this notebook, we'll fine-tune a sequence-to-sequence model for dialogue summarization.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Work with encoder-decoder models (T5, BART)\n",
                "2. Prepare data for seq2seq tasks\n",
                "3. Use ROUGE metrics for evaluation\n",
                "4. Generate summaries from dialogue\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate rouge-score nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
                "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
                "    DataCollatorForSeq2Seq\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import nltk\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Download NLTK data for sentence tokenization\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding Sequence-to-Sequence Models\n",
                "\n",
                "### How Summarization Works\n",
                "\n",
                "```\n",
                "Input (Dialogue):\n",
                "┌─────────────────────────────────────────────┐\n",
                "│ Person A: Hi, how are you?                  │\n",
                "│ Person B: I'm great! Just got promoted.     │\n",
                "│ Person A: Congratulations! That's amazing!  │\n",
                "│ Person B: Thanks! Let's celebrate tonight.  │\n",
                "└─────────────────────────────────────────────┘\n",
                "                    │\n",
                "                    ▼\n",
                "            ┌───────────────┐\n",
                "            │    ENCODER    │  (Understands input)\n",
                "            └───────────────┘\n",
                "                    │\n",
                "                    ▼\n",
                "            ┌───────────────┐\n",
                "            │    DECODER    │  (Generates output)\n",
                "            └───────────────┘\n",
                "                    │\n",
                "                    ▼\n",
                "Output (Summary):\n",
                "┌─────────────────────────────────────────────┐\n",
                "│ Person B got promoted and they plan to      │\n",
                "│ celebrate tonight.                          │\n",
                "└─────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "### Popular Summarization Models\n",
                "\n",
                "| Model | Parameters | Best For |\n",
                "|-------|------------|----------|\n",
                "| T5-small | 60M | Quick experiments |\n",
                "| T5-base | 220M | Good balance |\n",
                "| BART-base | 140M | News articles |\n",
                "| FLAN-T5 | Various | Instruction-tuned |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load the DialogSum Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0fab41c2b228410fa5157b35b2d6dc45",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "README.md: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0facfe968ee04978b2ca6229a8b8cf83",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "832c4ec550304ce19ef6a36166bb0303",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "validation.csv: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "873a1dfab60a471ab9b7a29a16fe6282",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "test.csv: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7d564f985bf34593b4c25e9942e33bf6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6929046cfc144a678108dcbb0f850e26",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "01688a51bf6b4f54ba5dd3d7a89e5a67",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset structure:\n",
                        "DatasetDict({\n",
                        "    train: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 12460\n",
                        "    })\n",
                        "    validation: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 500\n",
                        "    })\n",
                        "    test: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 1500\n",
                        "    })\n",
                        "})\n",
                        "\n",
                        "Train examples: 12460\n",
                        "Validation examples: 500\n",
                        "Test examples: 1500\n"
                    ]
                }
            ],
            "source": [
                "# Load DialogSum dataset\n",
                "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
                "\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset)\n",
                "print(f\"\\nTrain examples: {len(dataset['train'])}\")\n",
                "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
                "print(f\"Test examples: {len(dataset['test'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample dialogue and summary:\n",
                        "============================================================\n",
                        "DIALOGUE:\n",
                        "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
                        "#Person2#: I found it would be a good idea to get a check-up.\n",
                        "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
                        "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
                        "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
                        "#Person2#: Ok.\n",
                        "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
                        "#Person2#: Yes.\n",
                        "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
                        "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
                        "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
                        "#Person2#: Ok, thanks doctor.\n",
                        "\n",
                        "SUMMARY:\n",
                        "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n",
                        "\n",
                        "TOPIC:\n",
                        "get a check-up\n"
                    ]
                }
            ],
            "source": [
                "# Explore the data\n",
                "print(\"Sample dialogue and summary:\")\n",
                "print(\"=\" * 60)\n",
                "example = dataset['train'][0]\n",
                "\n",
                "print(\"DIALOGUE:\")\n",
                "print(example['dialogue'])\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(example['summary'])\n",
                "print(\"\\nTOPIC:\")\n",
                "print(example.get('topic', 'N/A'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Length Statistics (words):\n",
                        "\n",
                        "Dialogues:\n",
                        "  Mean: 131\n",
                        "  Max: 985\n",
                        "\n",
                        "Summaries:\n",
                        "  Mean: 23\n",
                        "  Max: 153\n",
                        "\n",
                        "Compression ratio: 18.5%\n"
                    ]
                }
            ],
            "source": [
                "# Analyze lengths\n",
                "dialogue_lengths = [len(d.split()) for d in dataset['train']['dialogue']]\n",
                "summary_lengths = [len(s.split()) for s in dataset['train']['summary']]\n",
                "\n",
                "print(\"Length Statistics (words):\")\n",
                "print(f\"\\nDialogues:\")\n",
                "print(f\"  Mean: {np.mean(dialogue_lengths):.0f}\")\n",
                "print(f\"  Max: {max(dialogue_lengths)}\")\n",
                "\n",
                "print(f\"\\nSummaries:\")\n",
                "print(f\"  Mean: {np.mean(summary_lengths):.0f}\")\n",
                "print(f\"  Max: {max(summary_lengths)}\")\n",
                "\n",
                "# Compression ratio\n",
                "ratios = [s/d for s, d in zip(summary_lengths, dialogue_lengths) if d > 0]\n",
                "print(f\"\\nCompression ratio: {np.mean(ratios):.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Data Preparation for Seq2Seq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "279b03e6230b4cf9bedca76431e5495b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ece5bc6bccb34c2499dd88ae37c79248",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "38b719573b9046b795e43713c2f50a04",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"t5-small\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# T5 uses a prefix for summarization\n",
                "prefix = \"summarize: \"\n",
                "\n",
                "# Tokenization parameters\n",
                "max_input_length = 512\n",
                "max_target_length = 128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize inputs and targets for seq2seq training.\"\"\"\n",
                "    # Add prefix to inputs\n",
                "    inputs = [prefix + doc for doc in examples['dialogue']]\n",
                "    \n",
                "    # Tokenize inputs\n",
                "    model_inputs = tokenizer(\n",
                "        inputs,\n",
                "        max_length=max_input_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    # Tokenize targets (summaries)\n",
                "    labels = tokenizer(\n",
                "        examples['summary'],\n",
                "        max_length=max_target_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    model_inputs['labels'] = labels['input_ids']\n",
                "    \n",
                "    return model_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f10aa416955e477bb419d724e25eee22",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3bc14b6c6004404aac80bd805855f91d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training samples: 1000\n",
                        "Validation samples: 200\n"
                    ]
                }
            ],
            "source": [
                "# Use smaller subsets for faster training\n",
                "train_size = 1000\n",
                "val_size = 200\n",
                "\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "val_data = dataset['validation'].shuffle(seed=42).select(range(val_size))\n",
                "\n",
                "# Tokenize\n",
                "train_tokenized = train_data.map(preprocess_function, batched=True)\n",
                "val_tokenized = val_data.map(preprocess_function, batched=True)\n",
                "\n",
                "print(f\"Training samples: {len(train_tokenized)}\")\n",
                "print(f\"Validation samples: {len(val_tokenized)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding ROUGE Metrics\n",
                "\n",
                "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures summary quality:\n",
                "\n",
                "| Metric | What it Measures |\n",
                "|--------|------------------|\n",
                "| ROUGE-1 | Unigram (word) overlap |\n",
                "| ROUGE-2 | Bigram (2-word) overlap |\n",
                "| ROUGE-L | Longest common subsequence |\n",
                "\n",
                "Higher scores = better summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ROUGE metric\n",
                "rouge = evaluate.load(\"rouge\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    \"\"\"\n",
                "    Compute ROUGE metrics for summarization evaluation.\n",
                "    \n",
                "    Note on Token Handling:\n",
                "    -----------------------\n",
                "    When using Seq2SeqTrainer with predict_with_generate=True, the predictions \n",
                "    and labels arrays come as numpy int64 values. The HuggingFace Fast Tokenizer \n",
                "    (Rust backend) can throw an OverflowError when decoding these values directly.\n",
                "    \n",
                "    This happens because:\n",
                "    1. Padding tokens are marked as -100 (a convention in HuggingFace for ignore_index)\n",
                "    2. Numpy int64 values may not convert cleanly to Rust's integer types\n",
                "    3. Token IDs outside the valid vocabulary range cause decoding issues\n",
                "    \n",
                "    The fix below explicitly:\n",
                "    - Converts each token to a Python int using int(tok)\n",
                "    - Validates tokens are within [0, vocab_size) range\n",
                "    - Replaces invalid tokens (including -100) with pad_token_id\n",
                "    \"\"\"\n",
                "    predictions, labels = eval_pred\n",
                "    \n",
                "    # Get pad token id and vocab size for validation\n",
                "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
                "    vocab_size = tokenizer.vocab_size\n",
                "    \n",
                "    # Clean predictions: convert to Python int and replace invalid values\n",
                "    predictions_clean = []\n",
                "    for row in predictions:\n",
                "        clean_row = []\n",
                "        for tok in row:\n",
                "            tok_int = int(tok)  # Explicitly convert to Python int\n",
                "            if tok_int < 0 or tok_int >= vocab_size:\n",
                "                clean_row.append(pad_id)\n",
                "            else:\n",
                "                clean_row.append(tok_int)\n",
                "        predictions_clean.append(clean_row)\n",
                "    \n",
                "    decoded_preds = tokenizer.batch_decode(predictions_clean, skip_special_tokens=True)\n",
                "    \n",
                "    # Clean labels: convert to Python int and replace -100 padding\n",
                "    labels_clean = []\n",
                "    for row in labels:\n",
                "        clean_row = []\n",
                "        for tok in row:\n",
                "            tok_int = int(tok)  # Explicitly convert to Python int\n",
                "            if tok_int < 0 or tok_int >= vocab_size:\n",
                "                clean_row.append(pad_id)\n",
                "            else:\n",
                "                clean_row.append(tok_int)\n",
                "        labels_clean.append(clean_row)\n",
                "    \n",
                "    decoded_labels = tokenizer.batch_decode(labels_clean, skip_special_tokens=True)\n",
                "    \n",
                "    # Strip whitespace\n",
                "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
                "    decoded_labels = [label.strip() for label in decoded_labels]\n",
                "    \n",
                "    # Compute ROUGE scores\n",
                "    result = rouge.compute(\n",
                "        predictions=decoded_preds,\n",
                "        references=decoded_labels,\n",
                "        use_stemmer=True\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        'rouge1': result['rouge1'],\n",
                "        'rouge2': result['rouge2'],\n",
                "        'rougeL': result['rougeL']\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model parameters: 60,506,624\n"
                    ]
                }
            ],
            "source": [
                "# Load model\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seq2Seq training arguments\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"./summarization_model\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=8,\n",
                "    per_device_eval_batch_size=8,\n",
                "    learning_rate=3e-5,\n",
                "    weight_decay=0.01,\n",
                "    warmup_ratio=0.1,\n",
                "    \n",
                "    # Generation during evaluation\n",
                "    predict_with_generate=True,\n",
                "    generation_max_length=max_target_length,\n",
                "    \n",
                "    # Evaluation\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"rougeL\",\n",
                "    \n",
                "    # Logging\n",
                "    logging_steps=50,\n",
                "    \n",
                "    # Performance\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "# Data collator for seq2seq\n",
                "data_collator = DataCollatorForSeq2Seq(\n",
                "    tokenizer=tokenizer,\n",
                "    model=model\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=val_tokenized,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "==================================================\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [375/375 05:16, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Rouge1</th>\n",
                            "      <th>Rouge2</th>\n",
                            "      <th>Rougel</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>1.182600</td>\n",
                            "      <td>0.866480</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>0.631000</td>\n",
                            "      <td>0.685007</td>\n",
                            "      <td>0.174265</td>\n",
                            "      <td>0.064189</td>\n",
                            "      <td>0.148405</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>0.606800</td>\n",
                            "      <td>0.671469</td>\n",
                            "      <td>0.250832</td>\n",
                            "      <td>0.090146</td>\n",
                            "      <td>0.212441</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Training complete!\n"
                    ]
                }
            ],
            "source": [
                "# Train\n",
                "print(\"Starting training...\")\n",
                "print(\"=\" * 50)\n",
                "trainer.train()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [25/25 00:37]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluation Results:\n",
                        "========================================\n",
                        "ROUGE-1: 25.08%\n",
                        "ROUGE-2: 9.01%\n",
                        "ROUGE-L: 21.24%\n"
                    ]
                }
            ],
            "source": [
                "# Evaluate\n",
                "eval_results = trainer.evaluate()\n",
                "\n",
                "print(\"Evaluation Results:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ROUGE-1: {eval_results['eval_rouge1']:.2%}\")\n",
                "print(f\"ROUGE-2: {eval_results['eval_rouge2']:.2%}\")\n",
                "print(f\"ROUGE-L: {eval_results['eval_rougeL']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generate Summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Create summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated Summaries:\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Example 1 ---\n",
                        "\n",
                        "DIALOGUE (truncated):\n",
                        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
                        "#Person2#: Yes, sir...\n",
                        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
                        "#Person2#: Yes, sir. Go ahead.\n",
                        "#Person1#: Attention all staff... Effective immediately, all office commu...\n",
                        "\n",
                        "REFERENCE SUMMARY:\n",
                        "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
                        "\n",
                        "GENERATED SUMMARY:\n",
                        "#Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees.\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Example 2 ---\n",
                        "\n",
                        "DIALOGUE (truncated):\n",
                        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
                        "#Person2#: Yes, sir...\n",
                        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
                        "#Person2#: Yes, sir. Go ahead.\n",
                        "#Person1#: Attention all staff... Effective immediately, all office commu...\n",
                        "\n",
                        "REFERENCE SUMMARY:\n",
                        "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
                        "\n",
                        "GENERATED SUMMARY:\n",
                        "#Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees.\n",
                        "------------------------------------------------------------\n",
                        "\n",
                        "--- Example 3 ---\n",
                        "\n",
                        "DIALOGUE (truncated):\n",
                        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
                        "#Person2#: Yes, sir...\n",
                        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
                        "#Person2#: Yes, sir. Go ahead.\n",
                        "#Person1#: Attention all staff... Effective immediately, all office commu...\n",
                        "\n",
                        "REFERENCE SUMMARY:\n",
                        "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
                        "\n",
                        "GENERATED SUMMARY:\n",
                        "#Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees.\n",
                        "------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Test on examples from the test set\n",
                "test_examples = dataset['test'].select(range(3))\n",
                "\n",
                "print(\"Generated Summaries:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, example in enumerate(test_examples):\n",
                "    dialogue = example['dialogue']\n",
                "    reference = example['summary']\n",
                "    \n",
                "    # Generate summary\n",
                "    generated = summarizer(\n",
                "        prefix + dialogue,\n",
                "        max_length=128,\n",
                "        min_length=20,\n",
                "        do_sample=False\n",
                "    )[0]['summary_text']\n",
                "    \n",
                "    print(f\"\\n--- Example {i+1} ---\")\n",
                "    print(f\"\\nDIALOGUE (truncated):\")\n",
                "    print(dialogue[:300] + \"...\")\n",
                "    print(f\"\\nREFERENCE SUMMARY:\")\n",
                "    print(reference)\n",
                "    print(f\"\\nGENERATED SUMMARY:\")\n",
                "    print(generated)\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Custom Dialogue Summarization:\n",
                        "==================================================\n",
                        "\n",
                        "INPUT:\n",
                        "\n",
                        "#Person1#: Hey Sarah, have you finished the project report?\n",
                        "#Person2#: Almost done! Just need to add the conclusion section.\n",
                        "#Person1#: Great! The client meeting is tomorrow at 2 PM.\n",
                        "#Person2#: I'll have it ready by noon so you can review it.\n",
                        "#Person1#: Perfect. Also, can you include the budget projections?\n",
                        "#Person2#: Already added those. I also updated the timeline chart.\n",
                        "#Person1#: You're a lifesaver! Thanks so much.\n",
                        "#Person2#: No problem! See you tomorrow.\n",
                        "\n",
                        "\n",
                        "SUMMARY:\n",
                        "#Person1#'ll have the final report ready by noon so you can review it. She's a lifesaver!\n"
                    ]
                }
            ],
            "source": [
                "# Test on custom dialogue\n",
                "custom_dialogue = \"\"\"\n",
                "#Person1#: Hey Sarah, have you finished the project report?\n",
                "#Person2#: Almost done! Just need to add the conclusion section.\n",
                "#Person1#: Great! The client meeting is tomorrow at 2 PM.\n",
                "#Person2#: I'll have it ready by noon so you can review it.\n",
                "#Person1#: Perfect. Also, can you include the budget projections?\n",
                "#Person2#: Already added those. I also updated the timeline chart.\n",
                "#Person1#: You're a lifesaver! Thanks so much.\n",
                "#Person2#: No problem! See you tomorrow.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(\n",
                "    prefix + custom_dialogue,\n",
                "    max_length=100,\n",
                "    min_length=20,\n",
                "    do_sample=False\n",
                ")[0]['summary_text']\n",
                "\n",
                "print(\"Custom Dialogue Summarization:\")\n",
                "print(\"=\" * 50)\n",
                "print(\"\\nINPUT:\")\n",
                "print(custom_dialogue)\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generation Parameters\n",
                "\n",
                "You can control how summaries are generated:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generation Strategies Comparison:\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Greedy: #Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees before 4 pm.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Beam Search (4): #Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees before 4 pm.\n",
                        "\n",
                        "Sampling (temp=0.7): #Person1# is ready to take a dictation for her. She wants to keep the memo typed up and distributed to all employees before 4 pm.\n"
                    ]
                }
            ],
            "source": [
                "test_dialogue = dataset['test'][0]['dialogue']\n",
                "\n",
                "# Different generation strategies\n",
                "print(\"Generation Strategies Comparison:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Greedy (default)\n",
                "greedy = summarizer(prefix + test_dialogue, max_length=100, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nGreedy: {greedy}\")\n",
                "\n",
                "# Beam search\n",
                "beam = summarizer(prefix + test_dialogue, max_length=100, num_beams=4, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nBeam Search (4): {beam}\")\n",
                "\n",
                "# Sampling\n",
                "sampled = summarizer(prefix + test_dialogue, max_length=100, do_sample=True, top_k=50, temperature=0.7)[0]['summary_text']\n",
                "print(f\"\\nSampling (temp=0.7): {sampled}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generation Parameters Explained\n",
                "\n",
                "| Parameter | Effect |\n",
                "|-----------|--------|\n",
                "| `max_length` | Maximum output tokens |\n",
                "| `min_length` | Minimum output tokens |\n",
                "| `num_beams` | Beam search width (higher = better but slower) |\n",
                "| `do_sample` | Enable random sampling |\n",
                "| `temperature` | Randomness (0=deterministic, 1=default, >1=more random) |\n",
                "| `top_k` | Limit sampling to top K tokens |\n",
                "| `top_p` | Nucleus sampling threshold |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 🎯 Student Challenge\n",
                "\n",
                "### Challenge: Fine-tune for News Summarization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Adapt this notebook for news summarization\n",
                "# 1. Load the CNN/DailyMail dataset: load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
                "# 2. The columns are 'article' (input) and 'highlights' (summary)\n",
                "# 3. Update the preprocess_function\n",
                "# 4. Train and evaluate\n",
                "# 5. Compare ROUGE scores\n",
                "\n",
                "# Hint: CNN/DailyMail articles are longer, you may need max_input_length=1024\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Seq2Seq models** have separate encoder and decoder components\n",
                "2. **T5 uses prefixes** (\"summarize:\", \"translate:\") to specify tasks\n",
                "3. **ROUGE metrics** measure n-gram overlap between generated and reference summaries\n",
                "4. **Generation parameters** control output quality and diversity\n",
                "5. **Beam search** usually produces better results than greedy decoding\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to Module 03: **Model Optimization**\n",
                "- `03_Model_Optimization/01_intro_to_optimization.ipynb`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
