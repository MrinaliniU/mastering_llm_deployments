{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning a Language Model for Healthcare Question Answering\n",
                "\n",
                "**Module 02 | Notebook 4 of 4**\n",
                "\n",
                "In this notebook, we will fine-tune a specialized language model (`TinyLlama`) to answer medical questions using the `MedQuad` dataset.\n",
                "\n",
                "## Learning Objectives\n",
                "1.  Understand the difference between **Context (RAG)** vs **Fine-Tuning**.\n",
                "2.  Learn about **PEFT (Parameter-Efficient Fine-Tuning)** and **LoRA**.\n",
                "3.  Fine-tune a model on a custom dataset.\n",
                "4.  Export the model for local use.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. When to use What? (RAG vs. Fine-Tuning)\n",
                "\n",
                "Before we start, it's important to know *when* to fine-tune.\n",
                "\n",
                "| Feature | **RAG (Retrieval-Augmented Gen)** | **Fine-Tuning** |\n",
                "| :--- | :--- | :--- |\n",
                "| **Analogy** | Giving the model an open textbook during the exam. | Sending the model to medical school for 4 years. |\n",
                "| **Goal** | Add new *knowledge* (facts, data). | Change *behavior*, style, or learn specialized jargon. |\n",
                "| **Pros** | Cheaper, easier to update facts. | Better performance on specific tasks, faster inference (no retrieval). |\n",
                "| **Cons** | Limited context window. | Expensive to train, hard to update facts (requires re-training). |\n",
                "\n",
                "**In this notebook**, we are doing **Fine-Tuning** to teach the model how to *act* like a medical assistant and understand medical terminology, not necessarily to memorize every drug interaction (RAG would be better for that)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate peft trl bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, \n",
                "    AutoTokenizer, \n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Device setup\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "if torch.backends.mps.is_available():\n",
                "    device = \"mps\"  # For Mac users\n",
                "\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Load the Dataset\n",
                "\n",
                "We will use `MedQuad` from generic sources. It contains pairs of `Question` and `Answer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
                "dataset = load_dataset(dataset_name, split=\"train\")\n",
                "\n",
                "# Use a small subset for demonstration (Top 500 examples)\n",
                "dataset = dataset.select(range(500))\n",
                "\n",
                "print(f\"Training on {len(dataset)} examples\")\n",
                "print(\"Sample:\", dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting\n",
                "To train a chat model, we format the data clearly so the model knows what is the input and what is the output.\n",
                "\n",
                "```\n",
                "### Question:\n",
                "{User's Question}\n",
                "\n",
                "### Answer:\n",
                "{Model's Answer}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def formatting_func(example):\n",
                "    text = f\"\"\"### Question:\n",
                "{example['Question']}\n",
                "\n",
                "### Answer:\n",
                "{example['Answer']}\"\"\"\n",
                "    return text\n",
                "\n",
                "print(formatting_func(dataset[0]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Model Setup (with Conditional Quantization)\n",
                "\n",
                "We use **TinyLlama-1.1B**. It's small enough to run on most laptops.\n",
                "\n",
                "### Hardware Note\n",
                "*   **NVIDIA GPU**: We can use **4-bit quantization** to save massive memory.\n",
                "*   **Mac (M1/M2/M3)**: 4-bit quantization (`bitsandbytes`) is not natively supported. We will load the model in `float16` instead. TinyLlama is small (2GB), so this works fine!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "# Determine optimized settings based on hardware\n",
                "if device == \"cuda\":\n",
                "    # Quantization Config (NVIDIA only)\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "    )\n",
                "    model_kwargs = {\"quantization_config\": bnb_config}\n",
                "else:\n",
                "    # Mac/CPU: Load in half-precision (float16) to save RAM\n",
                "    bnb_config = None\n",
                "    model_kwargs = {\"torch_dtype\": torch.float16}\n",
                "\n",
                "print(f\"Loading model with config: {model_kwargs}\")\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None, # MPS/CPU mapping handled manually or by defaults\n",
                "    trust_remote_code=True,\n",
                "    **model_kwargs\n",
                ")\n",
                "\n",
                "# For Mac MPS specifically, we explicit move if needed, but 'auto' usually avoids MPS for some models unless explicit\n",
                "if device == \"mps\":\n",
                "    model.to(\"mps\")\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Setting up LoRA (Low-Rank Adaptation)\n",
                "\n",
                "### The Valid Analogy\n",
                "Imagine you want to customize your car (Pre-trained Model).\n",
                "*   **Full Fine-Tuning**: Rebuilding the entire engine. Powerful, but expensive and slow.\n",
                "*   **LoRA**: Adding a \"Turbocharger\" plugin. You don't touch the engine; you just add a small, focused part that modifies the performance.\n",
                "\n",
                "### Parameters\n",
                "*   **`r` (Rank)**: The size of the \"plugin\". Bigger = smarter but slower. (Common: 8, 16)\n",
                "*   **`target_modules`**: Where to attach the plugin. In a Transformer, we usually attach to the Attention layers (`q_proj`, `v_proj`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if bnb_config: # If using quantization\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16, \n",
                "    lora_alpha=32, \n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = SFTConfig(\n",
                "    output_dir=\"./tinyllama-medical\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=2, # Keep low for standard laptops\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=10,\n",
                "    fp16=True if device != \"cpu\" else False, # Use fp16 for MPS/CUDA\n",
                "    dataset_text_field=\"text\",\n",
                "    max_length=512,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=lora_config,\n",
                "    formatting_func=formatting_func,\n",
                "    args=training_args,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the adapter (the plugin)\n",
                "trainer.model.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "tokenizer.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "print(\"Adapter saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask(question):\n",
                "    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs, \n",
                "        max_new_tokens=100, \n",
                "        do_sample=True, \n",
                "        temperature=0.7\n",
                "    )\n",
                "    \n",
                "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "ask(\"What are the symptoms of a cold?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "### Challenge: Create an Empathetic Mental Health Bot\n",
                "\n",
                "Medical data is factual. But what if we want a bot that is comforting?\n",
                "\n",
                "1.  **Modify the formatting function**: Add a system instruction like \"You are a caring friend.\"\n",
                "2.  **Dataset**: I've provided a tiny list of mental health Q&A below.\n",
                "3.  **Train**: Retrain the model on this new small dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mental Health Mini-Dataset\n",
                "mental_health_data = [\n",
                "    {\"q\": \"I feel sad.\", \"a\": \"I'm sorry to hear that. It's okay to feel down sometimes. Do you want to talk about it?\"},\n",
                "    {\"q\": \"I am anxious.\", \"a\": \"Take a deep breath. Anxiety is tough, but you are not alone. Let's focus on the present moment.\"},\n",
                "    {\"q\": \"Nobody likes me.\", \"a\": \"That must be a painful thought. I care about you, and I'm sure others do too, even if it's hard to see right now.\"}\n",
                "]\n",
                "\n",
                "# TODO: Create a new formatting function for this data\n",
                "# def empathetic_format(example):\n",
                "#     ...\n",
                "\n",
                "# TODO: Fine-tune the model on this new list\n",
                "# Hint: transform the list into a Hugging Face dataset first:\n",
                "# from datasets import Dataset\n",
                "# mh_dataset = Dataset.from_list(mental_health_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "1.  **LoRA** allows us to fine-tune significantly faster by freezing the main model.\n",
                "2.  **Quantization** is great for NVIDIA GPUs, but smaller models (1B) run fine on Mac/CPU in `float16`.\n",
                "3.  **Data Formatting** is critical in teaching the model *how* to speak (e.g., Q&A format)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}