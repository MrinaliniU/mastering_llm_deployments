{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning a Language Model for Healthcare Question Answering\n",
                "\n",
                "**Module 02 | Notebook 4 of 4**\n",
                "\n",
                "In this notebook, we will fine-tune a specialized language model (`TinyLlama`) to answer medical questions using the `MedQuad` dataset.\n",
                "\n",
                "## Learning Objectives\n",
                "1.  Understand the difference between **Context (RAG)** vs **Fine-Tuning**.\n",
                "2.  Learn about **PEFT (Parameter-Efficient Fine-Tuning)** and **LoRA**.\n",
                "3.  Fine-tune a model on a custom dataset.\n",
                "4.  Export the model for local use.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What You'll Build Today\n",
                "\n",
                "By the end of this notebook, you will have:\n",
                "- \u2705 A custom medical question-answering model\n",
                "- \u2705 Understanding of when to use fine-tuning vs RAG\n",
                "- \u2705 Hands-on experience with LoRA (Low-Rank Adaptation)\n",
                "- \u2705 A model that uses only ~2% of the original parameters to learn!\n",
                "\n",
                "**Estimated Time:** 45-60 minutes (depending on hardware)\n",
                "**Prerequisites:** Basic Python, understanding of neural networks\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. When to use What? (RAG vs. Fine-Tuning)\n",
                "\n",
                "Before we start, it's important to know *when* to fine-tune.\n",
                "\n",
                "| Feature | **RAG (Retrieval-Augmented Gen)** | **Fine-Tuning** |\n",
                "| :--- | :--- | :--- |\n",
                "| **Analogy** | Giving the model an open textbook during the exam. | Sending the model to medical school for 4 years. |\n",
                "| **Goal** | Add new *knowledge* (facts, data). | Change *behavior*, style, or learn specialized jargon. |\n",
                "| **Pros** | Cheaper, easier to update facts. | Better performance on specific tasks, faster inference (no retrieval). |\n",
                "| **Cons** | Limited context window. | Expensive to train, hard to update facts (requires re-training). |\n",
                "\n",
                "**In this notebook**, we are doing **Fine-Tuning** to teach the model how to *act* like a medical assistant and understand medical terminology, not necessarily to memorize every drug interaction (RAG would be better for that)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83c\udf0d Real-World Examples\n",
                "\n",
                "**When to use RAG:**\n",
                "- A company chatbot that needs to answer questions about constantly changing product documentation\n",
                "- A legal assistant that references current laws and regulations\n",
                "- A customer service bot with access to your latest knowledge base\n",
                "\n",
                "**When to use Fine-Tuning:**\n",
                "- Teaching a model to write in a specific tone (e.g., Shakespearean English)\n",
                "- Making a model understand medical/legal jargon and respond appropriately\n",
                "- Creating a coding assistant that follows your company's specific style guide\n",
                "- Teaching a model to structure outputs in a particular format (e.g., always JSON)\n",
                "\n",
                "**\ud83d\udca1 Pro Tip:** Many production systems use BOTH! They fine-tune for style/behavior and use RAG for facts.\n",
                "\n",
                "### \ud83d\udcda Think of it this way:\n",
                "\n",
                "**RAG = Open Book Exam**\n",
                "- You (the model) can look up answers in provided documents\n",
                "- You don't need to memorize everything\n",
                "- If the documents are updated, you automatically have new information\n",
                "\n",
                "**Fine-Tuning = Closed Book Exam (After Studying)**\n",
                "- You've internalized the patterns and style\n",
                "- You respond faster (no need to search documents)\n",
                "- But updating your knowledge requires studying again (retraining)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate peft trl bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, \n",
                "    AutoTokenizer, \n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Device setup\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "if torch.backends.mps.is_available():\n",
                "    device = \"mps\"  # For Mac users\n",
                "\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Load the Dataset\n",
                "\n",
                "We will use `MedQuad` from generic sources. It contains pairs of `Question` and `Answer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
                "dataset = load_dataset(dataset_name, split=\"train\")\n",
                "\n",
                "# Use a small subset for demonstration (Top 500 examples)\n",
                "dataset = dataset.select(range(500))\n",
                "\n",
                "print(f\"Training on {len(dataset)} examples\")\n",
                "print(\"Sample:\", dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udcca Understanding Our Dataset\n",
                "\n",
                "Let's explore what we're working with:\n",
                "\n",
                "**MedQuad Dataset:**\n",
                "- Contains medical questions and expert answers\n",
                "- Sourced from trusted health organizations (NIH, CDC, etc.)\n",
                "- Format: {Question, Answer} pairs\n",
                "\n",
                "**Why only 500 examples?**\n",
                "- Full dataset has 16,000+ examples\n",
                "- For learning purposes, 500 is enough to see results quickly\n",
                "- In production, you'd use the full dataset (takes 2-4 hours to train)\n",
                "\n",
                "**Dataset Quality Matters!**\n",
                "- \u2705 Good: Clear questions, accurate answers, consistent formatting\n",
                "- \u274c Bad: Contradictory information, poor grammar, inconsistent style\n",
                "- **Rule of thumb:** 500 high-quality examples > 5,000 mediocre ones\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83c\udfaf Why Data Formatting is Critical\n",
                "\n",
                "The model doesn't \"know\" what's a question vs an answer. We need to teach it!\n",
                "\n",
                "**Bad formatting:**\n",
                "```\n",
                "What are the symptoms of diabetes? Increased thirst, frequent urination...\n",
                "```\n",
                "The model sees one continuous text blob - it doesn't know where the question ends!\n",
                "\n",
                "**Good formatting:**\n",
                "```\n",
                "### Question:\n",
                "What are the symptoms of diabetes?\n",
                "\n",
                "### Answer:\n",
                "Increased thirst, frequent urination...\n",
                "```\n",
                "Now the model learns the pattern:\n",
                "1. Text after \"### Question:\" = what the user asks\n",
                "2. Text after \"### Answer:\" = what I should respond\n",
                "\n",
                "**This is called \"prompt formatting\" - it's one of the most important parts of fine-tuning!**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting\n",
                "To train a chat model, we format the data clearly so the model knows what is the input and what is the output.\n",
                "\n",
                "```\n",
                "### Question:\n",
                "{User's Question}\n",
                "\n",
                "### Answer:\n",
                "{Model's Answer}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def formatting_func(example):\n",
                "    text = f\"\"\"### Question:\n",
                "{example['Question']}\n",
                "\n",
                "### Answer:\n",
                "{example['Answer']}\"\"\"\n",
                "    return text\n",
                "\n",
                "print(formatting_func(dataset[0]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Understanding Quantization (Memory Optimization)\n",
                "\n",
                "Before we load our model, let's understand how we'll fit a billion-parameter model on your laptop!\n",
                "\n",
                "### What is Quantization?\n",
                "\n",
                "**The Problem:**\n",
                "- Modern LLMs are HUGE. TinyLlama (our \"small\" model) has 1.1 BILLION parameters\n",
                "- Each parameter is typically stored as a 32-bit float = 4 bytes\n",
                "- 1.1B \u00d7 4 bytes = 4.4 GB just for the model weights!\n",
                "\n",
                "**The Solution: Quantization**\n",
                "- Store numbers in fewer bits (4-bit or 8-bit instead of 32-bit)\n",
                "- **4-bit quantization:** 1.1B \u00d7 0.5 bytes \u2248 550 MB (8\u00d7 smaller!)\n",
                "\n",
                "### The Trade-off\n",
                "```\n",
                "Higher Precision \u2192 More Memory \u2192 Better Quality (slightly)\n",
                "Lower Precision \u2192 Less Memory \u2192 Faster Training \u2192 Tiny quality loss\n",
                "```\n",
                "**For learning and experimentation, 4-bit is perfect!**\n",
                "\n",
                "### Hardware-Specific Notes:\n",
                "- **NVIDIA GPU:** We use `bitsandbytes` library for 4-bit quantization\n",
                "- **Mac (M1/M2/M3):** We use float16 (16-bit) instead\n",
                "  - Why? The `bitsandbytes` library doesn't support Mac GPUs yet\n",
                "  - Good news: TinyLlama is small enough that float16 works fine!\n",
                "- **CPU Only:** We also use float16, but training will be slower\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Model Setup (with Conditional Quantization)\n",
                "\n",
                "We use **TinyLlama-1.1B**. It's small enough to run on most laptops.\n",
                "\n",
                "### Hardware Note\n",
                "*   **NVIDIA GPU**: We can use **4-bit quantization** to save massive memory.\n",
                "*   **Mac (M1/M2/M3)**: 4-bit quantization (`bitsandbytes`) is not natively supported. We will load the model in `float16` instead. TinyLlama is small (2GB), so this works fine!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "# Determine optimized settings based on hardware\n",
                "if device == \"cuda\":\n",
                "    # Quantization Config (NVIDIA only)\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "    )\n",
                "    model_kwargs = {\"quantization_config\": bnb_config}\n",
                "else:\n",
                "    # Mac/CPU: Load in half-precision (float16) to save RAM\n",
                "    bnb_config = None\n",
                "    model_kwargs = {\"torch_dtype\": torch.float16}\n",
                "\n",
                "print(f\"Loading model with config: {model_kwargs}\")\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None, # MPS/CPU mapping handled manually or by defaults\n",
                "    trust_remote_code=True,\n",
                "    **model_kwargs\n",
                ")\n",
                "\n",
                "# For Mac MPS specifically, we explicit move if needed, but 'auto' usually avoids MPS for some models unless explicit\n",
                "if device == \"mps\":\n",
                "    model.to(\"mps\")\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Setting up LoRA (Low-Rank Adaptation)\n",
                "\n",
                "### The Valid Analogy\n",
                "Imagine you want to customize your car (Pre-trained Model).\n",
                "*   **Full Fine-Tuning**: Rebuilding the entire engine. Powerful, but expensive and slow.\n",
                "*   **LoRA**: Adding a \"Turbocharger\" plugin. You don't touch the engine; you just add a small, focused part that modifies the performance.\n",
                "\n",
                "### Parameters\n",
                "*   **`r` (Rank)**: The size of the \"plugin\". Bigger = smarter but slower. (Common: 8, 16)\n",
                "*   **`target_modules`**: Where to attach the plugin. In a Transformer, we usually attach to the Attention layers (`q_proj`, `v_proj`)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udd2c How LoRA Actually Works (Technical)\n",
                "\n",
                "Let's understand what's happening under the hood:\n",
                "\n",
                "**Traditional Fine-Tuning:**\n",
                "```\n",
                "Original Model: 1.1 Billion parameters\n",
                "Fine-Tuning: Update ALL 1.1 Billion parameters\n",
                "Memory needed: ~20 GB\n",
                "Time: Many hours\n",
                "```\n",
                "**LoRA Fine-Tuning:**\n",
                "```\n",
                "Original Model: 1.1 Billion parameters (FROZEN \u2744\ufe0f)\n",
                "LoRA Adapters: ~2-20 Million parameters (TRAINABLE \ud83d\udd25)\n",
                "Memory needed: ~4 GB\n",
                "Time: Much faster!\n",
                "```\n",
                "**The Math (Simplified):**\n",
                "- In a transformer, we have weight matrices like `W` (e.g., 4096 \u00d7 4096)\n",
                "- LoRA adds two small matrices: `A` (4096 \u00d7 r) and `B` (r \u00d7 4096)\n",
                "- Instead of updating `W`, we learn `A` and `B`\n",
                "- Final output: `W\u00b7x + B\u00b7(A\u00b7x)` \u2248 Updated model behavior\n",
                "\n",
                "**Why does this work?**\n",
                "- The \"important\" changes to a model often live in a lower-dimensional space\n",
                "- `r` (rank) controls this dimension - usually 8 or 16 is enough!\n",
                "\n",
                "### Visualizing Parameter Count\n",
                "```\n",
                "Full Fine-Tuning: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 100% parameters\n",
                "LoRA (r=16):      [\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 1.8% parameters\n",
                "```\n",
                "You're only training ~20 million out of 1.1 billion parameters!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83c\udf9b\ufe0f Understanding LoRA Parameters\n",
                "\n",
                "Let's break down each parameter in our config:\n",
                "\n",
                "**`r=16` (Rank)**\n",
                "- The \"size\" of our adaptation\n",
                "- Higher = More powerful but slower and more memory\n",
                "- Typical values: 8, 16, 32, 64\n",
                "- **Our choice (16):** Good balance for most tasks\n",
                "\n",
                "**`lora_alpha=32` (Scaling Factor)**\n",
                "- Controls how much influence LoRA has\n",
                "- Rule of thumb: `alpha = 2 \u00d7 r`\n",
                "- **Our choice (32):** Standard scaling for r=16\n",
                "\n",
                "**`target_modules` (Where to apply LoRA)**\n",
                "```\n",
                "Transformer Layer:\n",
                "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "\u2502   Attention    \u2502 \u2190 We target these!\n",
                "\u2502 - q_proj     \u2713 \u2502 (Query, Key, Value, Output)\n",
                "\u2502 - k_proj     \u2713 \u2502\n",
                "\u2502 - v_proj     \u2713 \u2502\n",
                "\u2502 - o_proj     \u2713 \u2502\n",
                "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "\u2502  Feed Forward  \u2502 \u2190 We skip these\n",
                "\u2502 - gate_proj    \u2502 (To save memory)\n",
                "\u2502 - up_proj      \u2502\n",
                "\u2502 - down_proj    \u2502\n",
                "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "- **Why attention layers?** They control how the model \"understands\" relationships\n",
                "- **Pro tip:** For maximum quality, target FFN layers too (but uses more memory)\n",
                "\n",
                "**`lora_dropout=0.05`**\n",
                "- Randomly \"turns off\" 5% of LoRA neurons during training\n",
                "- Prevents overfitting (memorizing instead of learning)\n",
                "- **Our choice (0.05):** Conservative, works well for small datasets\n",
                "\n",
                "**`task_type=\"CAUSAL_LM\"`**\n",
                "- Tells LoRA we're doing language generation (not classification)\n",
                "- CAUSAL_LM = predict next word (like GPT)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if bnb_config: # If using quantization\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16, \n",
                "    lora_alpha=32, \n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83c\udfcb\ufe0f Understanding Training Parameters\n",
                "\n",
                "Let's demystify each training parameter:\n",
                "\n",
                "### Core Training Settings\n",
                "\n",
                "**`num_train_epochs=1`**\n",
                "- An \"epoch\" = one complete pass through the entire dataset\n",
                "- 1 epoch on 500 examples = Model sees each Q&A pair once\n",
                "- **Why just 1?** For demo purposes. Production uses 3-5 epochs.\n",
                "\n",
                "**`per_device_train_batch_size=2`**\n",
                "- How many examples to process at once\n",
                "- Larger = faster training but more memory\n",
                "- **Our choice (2):** Conservative for laptops with 8-16 GB RAM\n",
                "- If you have a beefy GPU: try 4 or 8\n",
                "\n",
                "**`gradient_accumulation_steps=4`**\n",
                "- This is clever! We simulate a larger batch size without using more memory\n",
                "- Effective batch size = 2 \u00d7 4 = 8\n",
                "- **How it works:**\n",
                "```\n",
                "Step 1: Process 2 examples, calculate gradients (don't update yet)\n",
                "Step 2: Process 2 more, accumulate gradients\n",
                "Step 3: Process 2 more, accumulate gradients\n",
                "Step 4: Process 2 more, accumulate gradients\n",
                "NOW: Update the model with accumulated gradients from 8 examples\n",
                "```\n",
                "**`learning_rate=2e-4` (0.0002)**\n",
                "- How big of a \"step\" to take when updating parameters\n",
                "- Too high \u2192 Model explodes \ud83d\udca5 or doesn't learn\n",
                "- Too low \u2192 Training takes forever \ud83d\udc0c\n",
                "- **Our choice (2e-4):** Standard for LoRA fine-tuning\n",
                "\n",
                "### Advanced Settings\n",
                "\n",
                "**`bf16=True` (Brain Float 16)**\n",
                "- Use 16-bit precision instead of 32-bit for faster training\n",
                "- \"Brain Float\" = special 16-bit format optimized for AI training\n",
                "- Saves memory and speeds up computation\n",
                "\n",
                "**`max_length=512`**\n",
                "- Maximum tokens (words/subwords) in a training example\n",
                "- Longer sequences = more context but more memory\n",
                "- Medical Q&A rarely exceeds 512 tokens\n",
                "\n",
                "**`packing=False`**\n",
                "- Could we pack multiple short examples into one sequence?\n",
                "- No - we want clean question-answer separation\n",
                "\n",
                "### \ud83d\udcca What to Expect:\n",
                "```\n",
                "Training on 500 examples:\n",
                "- With GPU: ~10-15 minutes\n",
                "- With CPU: ~45-60 minutes\n",
                "- You'll see loss decreasing (good!) - aim for < 1.0\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = SFTConfig(\n",
                "    output_dir=\"./tinyllama-medical\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=2, # Keep low for standard laptops\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=10,\n",
                "    bf16=True,  # Changed from fp16=True\n",
                "    dataset_text_field=\"text\",\n",
                "    max_length=512,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=lora_config,\n",
                "    formatting_func=formatting_func,\n",
                "    args=training_args,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udcc8 Understanding the Training Output\n",
                "\n",
                "While training, you'll see output like this:\n",
                "```\n",
                "| Step | Training Loss |\n",
                "| ---- | ------------- |\n",
                "| 10   | 2.456         |\n",
                "| 20   | 1.892         |\n",
                "| 30   | 1.423         |\n",
                "| 40   | 1.156         |\n",
                "```\n",
                "**What does this mean?**\n",
                "- **Loss** = How \"wrong\" the model's predictions are\n",
                "- Lower loss = Better predictions\n",
                "- Starting loss (~2-3) = Model is guessing randomly\n",
                "- Good final loss (~0.8-1.2) = Model is learning the patterns!\n",
                "\n",
                "**\u26a0\ufe0f Warning Signs:**\n",
                "- Loss increasing \u2192 Learning rate too high\n",
                "- Loss stuck at same value \u2192 Learning rate too low or data too small\n",
                "- Loss drops to nearly 0 \u2192 Model is memorizing (overfitting)\n",
                "\n",
                "**\ud83d\udca1 What's Actually Happening During Training:**\n",
                "```\n",
                "For each Q&A pair:\n",
                "1. Model reads the question\n",
                "2. Tries to predict the answer word-by-word\n",
                "3. Compares prediction to actual answer\n",
                "4. Calculates \"how wrong\" it was (loss)\n",
                "5. Updates the LoRA adapters to do better next time\n",
                "6. Repeat!\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the adapter (the plugin)\n",
                "trainer.model.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "tokenizer.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "print(\"Adapter saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Put model in evaluation mode and disable gradient checkpointing\n",
                "model.eval()\n",
                "model.config.use_cache = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83c\udfb2 Understanding Text Generation Parameters\n",
                "\n",
                "When we ask our model a question, we need to control HOW it generates the answer:\n",
                "\n",
                "**`max_new_tokens=100`**\n",
                "- Maximum number of tokens (words/subwords) to generate\n",
                "- Think of it as \"answer length limit\"\n",
                "- 100 tokens \u2248 75-80 words\n",
                "\n",
                "**`do_sample=True`**\n",
                "- Should the model be creative or deterministic?\n",
                "- **True:** Model picks from top probable words (varied, interesting)\n",
                "- **False:** Always picks THE most probable word (boring, repetitive)\n",
                "\n",
                "**`temperature=0.7`**\n",
                "- Controls randomness (only matters if do_sample=True)\n",
                "- Range: 0.0 to 2.0\n",
                "```\n",
                "Temperature 0.1 \u2192 Very focused, conservative\n",
                "Temperature 0.7 \u2192 Balanced (our choice)\n",
                "Temperature 1.0 \u2192 More creative\n",
                "Temperature 2.0 \u2192 Wild, incoherent\n",
                "```\n",
                "**Visual Example:**\n",
                "```\n",
                "Question: \"What helps a headache?\"\n",
                "Temperature 0.1: \"Take aspirin and rest.\" (boring but safe)\n",
                "Temperature 0.7: \"Try aspirin or ibuprofen. Rest in a dark room may help.\"\n",
                "Temperature 1.5: \"Aspirin, meditation, cucumber slices, purple thoughts...\"\n",
                "```\n",
                "**For Medical Applications:**\n",
                "- Keep temperature low (0.3-0.7) for accuracy\n",
                "- Higher temperature risks hallucinations (making up facts)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask(question):\n",
                "    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs, \n",
                "        max_new_tokens=100, \n",
                "        do_sample=True, \n",
                "        temperature=0.7\n",
                "    )\n",
                "    \n",
                "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "ask(\"What are the symptoms of a cold?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\uddea Test Your Fine-Tuned Model\n",
                "\n",
                "Let's compare the model's performance on different types of questions:\n",
                "\n",
                "**Test 1: Medical Knowledge (From Training)**\n",
                "```python\n",
                "ask(\"What are the symptoms of diabetes?\")\n",
                "# Expected: Should give medically accurate symptoms\n",
                "```\n",
                "**Test 2: General Medical (Not in Training)**\n",
                "```python\n",
                "ask(\"How can I prevent the flu?\")\n",
                "# Expected: Reasonable medical advice, even if not trained on this exact question\n",
                "```\n",
                "**Test 3: Off-Topic Question**\n",
                "```python\n",
                "ask(\"What's the capital of France?\")\n",
                "# Interesting: Model might still answer, or might try to frame as medical\n",
                "```\n",
                "### \ud83d\udcca Evaluating Quality\n",
                "\n",
                "**Good Signs \u2705:**\n",
                "- Answers are medically relevant\n",
                "- Uses appropriate medical terminology\n",
                "- Maintains professional tone\n",
                "- Acknowledges uncertainty when appropriate\n",
                "\n",
                "**Bad Signs \u274c:**\n",
                "- Makes up fake medical facts\n",
                "- Uses inappropriate casual language\n",
                "- Gives dangerous medical advice\n",
                "- Refuses to answer simple questions\n",
                "\n",
                "**Remember:** This model is NOT ready for real medical advice! It's a learning exercise.\n",
                "\n",
                "### \ud83d\udd2c Advanced: Compare to Base Model\n",
                "\n",
                "Want to see the difference fine-tuning made? Load the base model and compare:\n",
                "```python\n",
                "# Load base model (not fine-tuned)\n",
                "base_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
                "# Ask the same question to both models\n",
                "# See the difference!\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83c\udfaf Student Challenge: Build an Empathetic Mental Health Bot\n",
                "\n",
                "### The Goal\n",
                "Medical data is factual and clinical. But what if we want a bot that's warm and comforting?\n",
                "\n",
                "### Why This Matters\n",
                "- Tone and style matter in AI applications\n",
                "- Same model, different fine-tuning = completely different personality\n",
                "- This demonstrates fine-tuning for BEHAVIOR, not just knowledge\n",
                "\n",
                "### Your Mission (Step-by-Step)\n",
                "\n",
                "**STEP 1: Create Your Dataset**\n",
                "```python\n",
                "from datasets import Dataset\n",
                "\n",
                "mental_health_data = [\n",
                "    {\"question\": \"I feel sad.\", \"answer\": \"I'm sorry to hear that. It's okay to feel down sometimes. Do you want to talk about it?\"},\n",
                "    {\"question\": \"I am anxious.\", \"answer\": \"Take a deep breath. Anxiety is tough, but you are not alone. Let's focus on the present moment.\"},\n",
                "    {\"question\": \"Nobody likes me.\", \"answer\": \"That must be a painful thought. I care about you, and I'm sure others do too, even if it's hard to see right now.\"},\n",
                "    {\"question\": \"I can't sleep.\", \"answer\": \"Trouble sleeping is really difficult. Have you tried any relaxation techniques? I'm here to help.\"},\n",
                "    {\"question\": \"I feel overwhelmed.\", \"answer\": \"It's completely understandable to feel that way. Let's break things down together, one step at a time.\"}\n",
                "]\n",
                "\n",
                "mh_dataset = Dataset.from_list(mental_health_data)\n",
                "print(f\"Created dataset with {len(mh_dataset)} examples\")\n",
                "```\n",
                "**STEP 2: Modify the Formatting Function**\n",
                "\n",
                "The key difference: Add a system message to set the tone!\n",
                "```python\n",
                "def empathetic_format(example):\n",
                "    text = f\"\"\"You are a caring, empathetic friend who listens without judgment.\n",
                "\n",
                "### Question:\n",
                "{example['question']}\n",
                "\n",
                "### Answer:\n",
                "{example['answer']}\"\"\"\n",
                "    return text\n",
                "\n",
                "# Test it:\n",
                "print(empathetic_format(mh_dataset[0]))\n",
                "```\n",
                "**STEP 3: Train the Model**\n",
                "```python\n",
                "# Same LoRA config as before\n",
                "\n",
                "empathetic_training_args = SFTConfig(\n",
                "    output_dir=\"./tinyllama-empathetic\",\n",
                "    num_train_epochs=3, # Note: More epochs for small dataset!\n",
                "    per_device_train_batch_size=1, # Smaller because examples are longer\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=5,\n",
                "    bf16=True,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_length=512,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "empathetic_trainer = SFTTrainer(\n",
                "    model=model, # Start from your medical model or reload base\n",
                "    train_dataset=mh_dataset,\n",
                "    peft_config=lora_config,\n",
                "    formatting_func=empathetic_format,\n",
                "    args=empathetic_training_args,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Training empathetic model...\")\n",
                "empathetic_trainer.train()\n",
                "```\n",
                "**STEP 4: Test Your Creation**\n",
                "```python\n",
                "def ask_empathetic(question):\n",
                "    prompt = f\"\"\"You are a caring, empathetic friend who listens without judgment.\n",
                "\n",
                "### Question:\n",
                "{question}\n",
                "\n",
                "### Answer:\n",
                "\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=150, # Longer for empathetic responses\n",
                "        do_sample=True,\n",
                "        temperature=0.8 # Slightly higher for warmth\n",
                "    )\n",
                "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "# Try it!\n",
                "ask_empathetic(\"I'm worried about my exams.\")\n",
                "ask_empathetic(\"I had a bad day.\")\n",
                "```\n",
                "### \ud83e\udd14 Discussion Questions\n",
                "\n",
                "1. **How does the tone differ from the medical model?**\n",
                "   - Medical: factual, clinical\n",
                "   - Empathetic: warm, validating\n",
                "\n",
                "2. **Could we combine both?**\n",
                "   - Yes! Train on both datasets\n",
                "   - Model would be medically accurate AND empathetic\n",
                "\n",
                "3. **What are the risks?**\n",
                "   - AI should not replace professional mental health care\n",
                "   - Could give harmful advice if not carefully supervised\n",
                "   - Important to include disclaimers\n",
                "\n",
                "### \ud83c\udf1f Extension Ideas\n",
                "\n",
                "1. **Add more training data:**\n",
                "   - Find mental health conversation datasets\n",
                "   - Create 100+ examples for better quality\n",
                "\n",
                "2. **Add safety guardrails:**\n",
                "   - Train model to suggest professional help for serious issues\n",
                "   - Add crisis hotline information to responses\n",
                "\n",
                "3. **Combine with RAG:**\n",
                "   - Fine-tune for empathetic tone\n",
                "   - Use RAG to pull from verified mental health resources\n",
                "\n",
                "4. **Multi-turn conversations:**\n",
                "   - Most mental health chats are multi-turn\n",
                "   - Challenge: Modify the format to include conversation history\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udd27 Troubleshooting Common Issues\n",
                "\n",
                "### Issue 1: \"CUDA Out of Memory\"\n",
                "\n",
                "**Problem:** GPU runs out of RAM\n",
                "\n",
                "**Solutions:**\n",
                "```python\n",
                "# Option A: Reduce batch size\n",
                "per_device_train_batch_size=1 # Down from 2\n",
                "\n",
                "# Option B: Reduce LoRA rank\n",
                "r=8 # Down from 16\n",
                "\n",
                "# Option C: Reduce max_length\n",
                "max_length=256 # Down from 512\n",
                "\n",
                "# Option D: Enable gradient checkpointing\n",
                "model.gradient_checkpointing_enable()\n",
                "```\n",
                "### Issue 2: Model Outputs Gibberish\n",
                "\n",
                "**Problem:** Loss decreased but outputs are nonsensical\n",
                "\n",
                "**Likely Causes:**\n",
                "- Learning rate too high\n",
                "- Trained for too many epochs (overfit)\n",
                "- Bad data formatting\n",
                "\n",
                "**Solutions:**\n",
                "```python\n",
                "# Lower learning rate\n",
                "learning_rate=1e-4 # Down from 2e-4\n",
                "\n",
                "# Fewer epochs\n",
                "num_train_epochs=1 # Down from 3\n",
                "\n",
                "# Check your formatting function output manually\n",
                "print(formatting_func(dataset[0]))\n",
                "```\n",
                "### Issue 3: Model Just Repeats the Question\n",
                "\n",
                "**Problem:** Output looks like:\n",
                "```\n",
                "### Question:\n",
                "What is diabetes?\n",
                "### Answer:\n",
                "What is diabetes?\n",
                "```\n",
                "**Cause:** Model hasn't learned where answers should go\n",
                "\n",
                "**Solutions:**\n",
                "- Check formatting function includes \"### Answer:\" marker\n",
                "- Train for more steps\n",
                "- Increase dataset size\n",
                "\n",
                "### Issue 4: Training is Extremely Slow\n",
                "\n",
                "**Speed Benchmarks:**\n",
                "- **GPU (NVIDIA):** 10-15 minutes for 500 examples\n",
                "- **Mac M1/M2:** 30-45 minutes\n",
                "- **CPU Only:** 60-120 minutes\n",
                "\n",
                "**Speed-up Tips:**\n",
                "```python\n",
                "# 1. Reduce number of examples for testing\n",
                "dataset = dataset.select(range(100)) # Just 100 examples\n",
                "\n",
                "# 2. Increase batch size (if memory allows)\n",
                "per_device_train_batch_size=4\n",
                "\n",
                "# 3. Reduce gradient accumulation steps\n",
                "gradient_accumulation_steps=2\n",
                "\n",
                "# 4. Use fewer logging steps\n",
                "logging_steps=50 # Log less frequently\n",
                "```\n",
                "### Issue 5: \"ImportError: bitsandbytes not found\"\n",
                "\n",
                "**On Mac:** This is expected! Use this instead:\n",
                "```python\n",
                "# For Mac, use this loading code:\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model.to(\"mps\") # Move to Mac GPU\n",
                "```\n",
                "### Issue 6: Model Refuses to Follow Fine-Tuning\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "1.  **LoRA** allows us to fine-tune significantly faster by freezing the main model.\n",
                "2.  **Quantization** is great for NVIDIA GPUs, but smaller models (1B) run fine on Mac/CPU in `float16`.\n",
                "3.  **Data Formatting** is critical in teaching the model *how* to speak (e.g., Q&A format)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}