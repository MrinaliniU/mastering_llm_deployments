{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåê Local Model Serving with FastAPI\n",
                "\n",
                "**Module 04 | Notebook 1 of 4**\n",
                "\n",
                "Learn to create production-ready REST APIs for your ML models using FastAPI.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Create a FastAPI application for model serving\n",
                "2. Implement prediction endpoints\n",
                "3. Handle input validation with Pydantic\n",
                "4. Test your API locally\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch fastapi uvicorn pydantic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1Ô∏è‚É£ Why FastAPI?\n",
                "\n",
                "### REST API Serving Pattern\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     HTTP Request      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ   FastAPI   ‚îÇ\n",
                "‚îÇ  (Browser,  ‚îÇ     {\"text\": \"...\"}   ‚îÇ   Server    ‚îÇ\n",
                "‚îÇ   Mobile)   ‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
                "       ‚Üë                                     ‚ñº\n",
                "       ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "       ‚îÇ      HTTP Response           ‚îÇ    Model    ‚îÇ\n",
                "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  Inference  ‚îÇ\n",
                "             {\"label\": \"POSITIVE\"}    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "### FastAPI Advantages\n",
                "\n",
                "| Feature | Benefit |\n",
                "|---------|--------|\n",
                "| **Automatic docs** | Swagger UI out of the box |\n",
                "| **Type hints** | Automatic validation |\n",
                "| **Async support** | High concurrency |\n",
                "| **Fast** | One of the fastest Python frameworks |\n",
                "| **Modern** | Native Python 3.6+ features |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2Ô∏è‚É£ Load the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
                "model.eval()\n",
                "\n",
                "print(f\"Model loaded: {model_name}\")\n",
                "print(f\"Labels: {model.config.id2label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test prediction function\n",
                "def predict(text: str) -> dict:\n",
                "    \"\"\"Run inference on input text.\"\"\"\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=512\n",
                "    ).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "    pred_idx = probs.argmax().item()\n",
                "    \n",
                "    return {\n",
                "        \"label\": model.config.id2label[pred_idx],\n",
                "        \"confidence\": probs[pred_idx].item(),\n",
                "        \"probabilities\": {\n",
                "            model.config.id2label[i]: probs[i].item() \n",
                "            for i in range(len(probs))\n",
                "        }\n",
                "    }\n",
                "\n",
                "# Test\n",
                "result = predict(\"This movie was fantastic!\")\n",
                "print(f\"Test prediction: {result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ Create the FastAPI Application\n",
                "\n",
                "Here's the complete FastAPI application code. In a production setting, you would save this to a file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FastAPI application code\n",
                "app_code = '''\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Dict, List, Optional\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "\n",
                "# Initialize app\n",
                "app = FastAPI(\n",
                "    title=\"Sentiment Analysis API\",\n",
                "    description=\"A REST API for sentiment classification using DistilBERT\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Load model at startup\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "\n",
                "# Request/Response schemas\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to classify\")\n",
                "    \n",
                "    class Config:\n",
                "        json_schema_extra = {\n",
                "            \"example\": {\"text\": \"This movie was absolutely fantastic!\"}\n",
                "        }\n",
                "\n",
                "class PredictionResponse(BaseModel):\n",
                "    label: str\n",
                "    confidence: float\n",
                "    probabilities: Dict[str, float]\n",
                "\n",
                "class BatchRequest(BaseModel):\n",
                "    texts: List[str] = Field(..., max_length=100)\n",
                "\n",
                "class HealthResponse(BaseModel):\n",
                "    status: str\n",
                "    model: str\n",
                "    device: str\n",
                "\n",
                "# Endpoints\n",
                "@app.get(\"/health\", response_model=HealthResponse)\n",
                "def health_check():\n",
                "    \"\"\"Check if the API is running and model is loaded.\"\"\"\n",
                "    return {\n",
                "        \"status\": \"healthy\",\n",
                "        \"model\": model_name,\n",
                "        \"device\": str(device)\n",
                "    }\n",
                "\n",
                "@app.post(\"/predict\", response_model=PredictionResponse)\n",
                "def predict_sentiment(request: PredictionRequest):\n",
                "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
                "    try:\n",
                "        inputs = tokenizer(\n",
                "            request.text,\n",
                "            return_tensors=\"pt\",\n",
                "            truncation=True,\n",
                "            max_length=512\n",
                "        ).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "        \n",
                "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "        pred_idx = probs.argmax().item()\n",
                "        \n",
                "        return {\n",
                "            \"label\": model.config.id2label[pred_idx],\n",
                "            \"confidence\": probs[pred_idx].item(),\n",
                "            \"probabilities\": {\n",
                "                model.config.id2label[i]: probs[i].item() \n",
                "                for i in range(len(probs))\n",
                "            }\n",
                "        }\n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=str(e))\n",
                "\n",
                "@app.post(\"/predict/batch\", response_model=List[PredictionResponse])\n",
                "def predict_batch(request: BatchRequest):\n",
                "    \"\"\"Predict sentiment for multiple texts.\"\"\"\n",
                "    results = []\n",
                "    for text in request.texts:\n",
                "        req = PredictionRequest(text=text)\n",
                "        results.append(predict_sentiment(req))\n",
                "    return results\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    import uvicorn\n",
                "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
                "'''\n",
                "\n",
                "# Display the code\n",
                "print(\"FastAPI Application Code:\")\n",
                "print(\"=\" * 60)\n",
                "print(app_code)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to file\n",
                "with open(\"./app.py\", \"w\") as f:\n",
                "    f.write(app_code)\n",
                "\n",
                "print(\"‚úÖ Application saved to app.py\")\n",
                "print(\"\\nTo run the server:\")\n",
                "print(\"  python app.py\")\n",
                "print(\"  OR\")\n",
                "print(\"  uvicorn app:app --reload --host 0.0.0.0 --port 8000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ Understanding the Application\n",
                "\n",
                "### Request/Response Models (Pydantic)\n",
                "\n",
                "```python\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str = Field(..., min_length=1, max_length=5000)\n",
                "```\n",
                "\n",
                "This provides:\n",
                "- **Automatic validation** (text must be 1-5000 characters)\n",
                "- **Documentation** (shown in Swagger UI)\n",
                "- **Type hints** for IDE support\n",
                "\n",
                "### Endpoints\n",
                "\n",
                "| Endpoint | Method | Description |\n",
                "|----------|--------|-------------|\n",
                "| `/health` | GET | Check API status |\n",
                "| `/predict` | POST | Single text prediction |\n",
                "| `/predict/batch` | POST | Batch predictions |\n",
                "| `/docs` | GET | Swagger UI (automatic) |\n",
                "| `/redoc` | GET | ReDoc UI (automatic) |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ Testing the API\n",
                "\n",
                "Once the server is running, you can test it using `requests`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example client code (run when server is active)\n",
                "client_code = '''\n",
                "import requests\n",
                "\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "# Health check\n",
                "response = requests.get(f\"{BASE_URL}/health\")\n",
                "print(\"Health:\", response.json())\n",
                "\n",
                "# Single prediction\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict\",\n",
                "    json={\"text\": \"This movie was fantastic!\"}\n",
                ")\n",
                "print(\"Prediction:\", response.json())\n",
                "\n",
                "# Batch prediction\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict/batch\",\n",
                "    json={\n",
                "        \"texts\": [\n",
                "            \"I love this product!\",\n",
                "            \"Terrible experience, never again.\",\n",
                "            \"It was okay.\"\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "print(\"Batch:\", response.json())\n",
                "'''\n",
                "\n",
                "print(\"Client Test Code:\")\n",
                "print(\"=\" * 60)\n",
                "print(client_code)\n",
                "\n",
                "# Save client code\n",
                "with open(\"./test_client.py\", \"w\") as f:\n",
                "    f.write(client_code)\n",
                "print(\"\\n‚úÖ Client code saved to test_client.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6Ô∏è‚É£ Production Best Practices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "production_tips = \"\"\"\n",
                "## Production Best Practices\n",
                "\n",
                "### 1. Model Loading\n",
                "- Load model ONCE at startup, not per request\n",
                "- Use `@app.on_event(\"startup\")` for initialization\n",
                "\n",
                "### 2. Error Handling\n",
                "- Use try/except and HTTPException\n",
                "- Return meaningful error messages\n",
                "\n",
                "### 3. Logging\n",
                "```python\n",
                "import logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)\n",
                "```\n",
                "\n",
                "### 4. Rate Limiting\n",
                "```python\n",
                "from fastapi_limiter import FastAPILimiter\n",
                "```\n",
                "\n",
                "### 5. CORS (for web clients)\n",
                "```python\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=[\"*\"],\n",
                "    allow_methods=[\"*\"],\n",
                "    allow_headers=[\"*\"],\n",
                ")\n",
                "```\n",
                "\n",
                "### 6. Async for I/O\n",
                "```python\n",
                "@app.post(\"/predict\")\n",
                "async def predict(request: Request):\n",
                "    # Use async for database/file operations\n",
                "```\n",
                "\n",
                "### 7. Health Checks\n",
                "- Include model status, memory usage, GPU utilization\n",
                "- Kubernetes/Docker can use for readiness probes\n",
                "\"\"\"\n",
                "\n",
                "print(production_tips)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "### Challenge: Add New Endpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Extend the API with these features:\n",
                "\n",
                "# 1. Add a `/tokenize` endpoint that returns token information\n",
                "#    - Input: {\"text\": \"...\"}\n",
                "#    - Output: {\"tokens\": [...], \"token_ids\": [...], \"num_tokens\": N}\n",
                "\n",
                "# 2. Add model info endpoint `/model/info`\n",
                "#    - Output: {\"name\": \"...\", \"parameters\": N, \"vocab_size\": N}\n",
                "\n",
                "# 3. Add request timing middleware\n",
                "#    - Log request duration for each call\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Key Takeaways\n",
                "\n",
                "1. **FastAPI** provides automatic docs, validation, and async support\n",
                "2. **Pydantic models** define request/response schemas with validation\n",
                "3. **Load models once** at startup for efficiency\n",
                "4. **Health endpoints** are essential for production monitoring\n",
                "5. **Batch endpoints** improve throughput for multiple requests\n",
                "\n",
                "---\n",
                "\n",
                "## ‚û°Ô∏è Next Steps\n",
                "\n",
                "Continue to `02_gradio_ui.ipynb` for interactive web interfaces!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
